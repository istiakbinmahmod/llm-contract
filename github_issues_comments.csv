Issue Number,Issue Title,Issue Body,Comment Author,Comment Body
254,### What happened?,"### What happened?

Suppose you want to use `bazel fetch` to prefetch repositories for the node toolchain.

With this WORKSPACE (verbatim from https://github.com/aspect-build/rules_js/releases/tag/v1.6.9):

```
load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"")

http_archive(
    name = ""aspect_rules_js"",
    sha256 = ""dda5fee3926e62c483660b35b25d1577d23f88f11a2775e3555b57289f4edb12"",
    strip_prefix = ""rules_js-1.6.9"",
    url = ""https://github.com/aspect-build/rules_js/archive/refs/tags/v1.6.9.tar.gz"",
)

load(""@aspect_rules_js//js:repositories.bzl"", ""rules_js_dependencies"")

rules_js_dependencies()

load(""@rules_nodejs//nodejs:repositories.bzl"", ""DEFAULT_NODE_VERSION"", ""nodejs_register_toolchains"")

nodejs_register_toolchains(
    name = ""nodejs"",
    node_version = DEFAULT_NODE_VERSION,
)
```

Run `bazel fetch @nodejs_host//...`.

Expected result: the command succeeds.
Actual result: the command fails with this error:

```
➜  rules_nodejs_bug bazel fetch @nodejs_host//...
ERROR: /private/var/tmp/_bazel_john/442663b197eb6942a605345c9a9c84f6/external/nodejs_host/BUILD.bazel:4:6: no such target '@nodejs_darwin_arm64//:run_npm.sh.template': target 'run_npm.sh.template' not declared in package '' (did you mean 'run_npm.template'?) defined by /private/var/tmp/_bazel_john/442663b197eb6942a605345c9a9c84f6/external/nodejs_darwin_arm64/BUILD.bazel and referenced by '@nodejs_host//:run_npm.sh.template'
ERROR: /private/var/tmp/_bazel_john/442663b197eb6942a605345c9a9c84f6/external/nodejs_host/BUILD.bazel:5:6: no such target '@nodejs_darwin_arm64//:run_npm.bat.template': target 'run_npm.bat.template' not declared in package '' (did you mean 'run_npm.template'?) defined by /private/var/tmp/_bazel_john/442663b197eb6942a605345c9a9c84f6/external/nodejs_darwin_arm64/BUILD.bazel and referenced by '@nodejs_host//:run_npm.bat.template'
ERROR: /private/var/tmp/_bazel_john/442663b197eb6942a605345c9a9c84f6/external/nodejs_host/BUILD.bazel:4:6: no such target '@nodejs_darwin_arm64//:run_npm.sh.template': target 'run_npm.sh.template' not declared in package '' (did you mean 'run_npm.template'?) defined by /private/var/tmp/_bazel_john/442663b197eb6942a605345c9a9c84f6/external/nodejs_darwin_arm64/BUILD.bazel and referenced by '@nodejs_host//:run_npm.sh.template'
ERROR: Evaluation of query ""deps(@nodejs_host//...)"" failed: errors were encountered while computing transitive closure
Loading: 2 packages loaded
```

### Version

Development (host) and target OS/architectures:

Output of `bazel --version`:

Version of rules_nodejs, or other relevant rules from your
`WORKSPACE` or `MODULE.bazel` file:

Language(s) and/or frameworks involved:


### How to reproduce

_No response_

### Any other information?

_No response_

_Originally posted by @jfirebaugh in https://github.com/bazel-contrib/rules_nodejs/issues/3599_",jakkal6,"> ### What happened?
> 
> Suppose you want to use `bazel fetch` to prefetch repositories for the node toolchain.
> 
> 
> 
> With this WORKSPACE (verbatim from https://github.com/aspect-build/rules_js/releases/tag/v1.6.9):
> 
> 
> 
> ```
> 
> load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"")
> 
> 
> 
> http_archive(
> 
>     name = ""aspect_rules_js"",
> 
>     sha256 = ""dda5fee3926e62c483660b35b25d1577d23f88f11a2775e3555b57289f4edb12"",
> 
>     strip_prefix = ""rules_js-1.6.9"",
> 
>     url = ""https://github.com/aspect-build/rules_js/archive/refs/tags/v1.6.9.tar.gz"",
> 
> )
> 
> 
> 
> load(""@aspect_rules_js//js:repositories.bzl"", ""rules_js_dependencies"")
> 
> 
> 
> rules_js_dependencies()
> 
> 
> 
> load(""@rules_nodejs//nodejs:repositories.bzl"", ""DEFAULT_NODE_VERSION"", ""nodejs_register_toolchains"")
> 
> 
> 
> nodejs_register_toolchains(
> 
>     name = ""nodejs"",
> 
>     node_version = DEFAULT_NODE_VERSION,
> 
> )
> 
> ```
> 
> 
> 
> Run `bazel fetch @nodejs_host//...`.
> 
> 
> 
> Expected result: the command succeeds.
> 
> Actual result: the command fails with this error:
> 
> 
> 
> ```
> 
> ➜  rules_nodejs_bug bazel fetch @nodejs_host//...
> 
> ERROR: /private/var/tmp/_bazel_john/442663b197eb6942a605345c9a9c84f6/external/nodejs_host/BUILD.bazel:4:6: no such target '@nodejs_darwin_arm64//:run_npm.sh.template': target 'run_npm.sh.template' not declared in package '' (did you mean 'run_npm.template'?) defined by /private/var/tmp/_bazel_john/442663b197eb6942a605345c9a9c84f6/external/nodejs_darwin_arm64/BUILD.bazel and referenced by '@nodejs_host//:run_npm.sh.template'
> 
> ERROR: /private/var/tmp/_bazel_john/442663b197eb6942a605345c9a9c84f6/external/nodejs_host/BUILD.bazel:5:6: no such target '@nodejs_darwin_arm64//:run_npm.bat.template': target 'run_npm.bat.template' not declared in package '' (did you mean 'run_npm.template'?) defined by /private/var/tmp/_bazel_john/442663b197eb6942a605345c9a9c84f6/external/nodejs_darwin_arm64/BUILD.bazel and referenced by '@nodejs_host//:run_npm.bat.template'
> 
> ERROR: /private/var/tmp/_bazel_john/442663b197eb6942a605345c9a9c84f6/external/nodejs_host/BUILD.bazel:4:6: no such target '@nodejs_darwin_arm64//:run_npm.sh.template': target 'run_npm.sh.template' not declared in package '' (did you mean 'run_npm.template'?) defined by /private/var/tmp/_bazel_john/442663b197eb6942a605345c9a9c84f6/external/nodejs_darwin_arm64/BUILD.bazel and referenced by '@nodejs_host//:run_npm.sh.template'
> 
> ERROR: Evaluation of query ""deps(@nodejs_host//...)"" failed: errors were encountered while computing transitive closure
> 
> Loading: 2 packages loaded
> 
> ```
> 
> ### Version
> 
> Development (host) and target OS/architectures:
> 
> 
> 
> Output of `bazel --version`:
> 
> 
> 
> Version of rules_nodejs, or other relevant rules from your
> 
> `WORKSPACE` or `MODULE.bazel` file:
> 
> 
> 
> Language(s) and/or frameworks involved:
> 
> 
> 
> ### How to reproduce
> 
> _No response_
> 
> ### Any other information?
> 
> _No response_
> 
> _Originally posted by @jfirebaugh in https://github.com/bazel-contrib/rules_nodejs/issues/3599_

"
245,Fix dimension error in annotate.,"After reading kv_cache, keys and values should have the dimensions of (bs, cache_len + seqlen, n_local_heads, head_dim), rather than (bs, seqlen, n_local_heads, head_dim).",facebook-github-bot,"Hi @PengWeixuan! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20meta-llama%2Fcodellama%20%23245). Thanks!"
245,Fix dimension error in annotate.,"After reading kv_cache, keys and values should have the dimensions of (bs, cache_len + seqlen, n_local_heads, head_dim), rather than (bs, seqlen, n_local_heads, head_dim).",facebook-github-bot,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!
241,update,,facebook-github-bot,"Hi @UGBOKE! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20meta-llama%2Fcodellama%20%23241). Thanks!"
239,Update download.sh,,facebook-github-bot,"Hi @OssamaMokhtar! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20meta-llama%2Fcodellama%20%23239). Thanks!"
239,Update download.sh,,facebook-github-bot,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!
239,Update download.sh,,jgehring,The URL is unique to each request and only valid for a fixed amount of time.
230,chore(deps): Update dependencies,"Updated:
* Dev dependencies
* Python 3.9 to 3.12 

Changes:
* Deduplicate dev dependencies in CI
* Cache venv in all CI jobs
* Improve setup.py and CI
* Let Dependabot update pip/poetry dependencies",facebook-github-bot,"Hi @reneleonhardt! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20meta-llama%2Fcodellama%20%23230). Thanks!"
230,chore(deps): Update dependencies,"Updated:
* Dev dependencies
* Python 3.9 to 3.12 

Changes:
* Deduplicate dev dependencies in CI
* Cache venv in all CI jobs
* Improve setup.py and CI
* Let Dependabot update pip/poetry dependencies",facebook-github-bot,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!
227,Will there be a Codellama based on Llama 3?,Is there going to be an updated version of Codellama based on Meta's new [LLaMa 3 ](https://github.com/meta-llama/)?,hyperbolic-c,Same question！Looking forward to the model based on llama3!
227,Will there be a Codellama based on Llama 3?,Is there going to be an updated version of Codellama based on Meta's new [LLaMa 3 ](https://github.com/meta-llama/)?,bgruszka,It would be great to have it based on llama 3 😄 
227,Will there be a Codellama based on Llama 3?,Is there going to be an updated version of Codellama based on Meta's new [LLaMa 3 ](https://github.com/meta-llama/)?,bmaciag,Same question.
227,Will there be a Codellama based on Llama 3?,Is there going to be an updated version of Codellama based on Meta's new [LLaMa 3 ](https://github.com/meta-llama/)?,TheLapinMalin,"Apparently Llama 3 has already been trained on a lot more code than Llama 2.

So, do we need a full blown Codellama 3 model, or do you think a FIM fine-tune of Llama 3 would be sufficient?"
227,Will there be a Codellama based on Llama 3?,Is there going to be an updated version of Codellama based on Meta's new [LLaMa 3 ](https://github.com/meta-llama/)?,PhilKes,"> Apparently Llama 3 has already been trained on a lot more code than Llama 2.
> 
> So, do we need a full blown Codellama 3 model, or do you think a FIM fine-tune of Llama 3 would be sufficient?

Would love to see a FIM fine-tune of Llama 3, I dont have any insights on how the training process differed from Llama 2.
Is there anyone working on a FIM fine-tune? Haven't seen one yet on HF"
227,Will there be a Codellama based on Llama 3?,Is there going to be an updated version of Codellama based on Meta's new [LLaMa 3 ](https://github.com/meta-llama/)?,reneleonhardt,"> Would love to see a FIM fine-tune of Llama 3, I dont have any insights on how the training process differed from Llama 2.
> Is there anyone working on a FIM fine-tune? Haven't seen one yet on HF

Supposedly [StarCoder2](https://huggingface.co/bigcode/starcoder2-15b) should be perfect for FIM 😅"
227,Will there be a Codellama based on Llama 3?,Is there going to be an updated version of Codellama based on Meta's new [LLaMa 3 ](https://github.com/meta-llama/)?,hschaeufler,Would also love to see a Code Llama Version based on LLama 3. Llama 3 70b [seems to perform](https://mistral.ai/news/codestral/) better on coding-Tasks than Code Llama 70b.
227,Will there be a Codellama based on Llama 3?,Is there going to be an updated version of Codellama based on Meta's new [LLaMa 3 ](https://github.com/meta-llama/)?,DevopsDood,3.1:70b will be :fire: 
227,Will there be a Codellama based on Llama 3?,Is there going to be an updated version of Codellama based on Meta's new [LLaMa 3 ](https://github.com/meta-llama/)?,ShahNewazKhan,+1
227,Will there be a Codellama based on Llama 3?,Is there going to be an updated version of Codellama based on Meta's new [LLaMa 3 ](https://github.com/meta-llama/)?,ogbozoyan,"+1
there'e even fresher model which impress"
223,download.sh,,facebook-github-bot,"Hi @Korruption! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20meta-llama%2Fcodellama%20%23223). Thanks!"
221,Update download.sh,,facebook-github-bot,"Hi @theaskews08! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20meta-llama%2Fcodellama%20%23221). Thanks!"
218,Update download.sh,my code from meta LLama,facebook-github-bot,"Hi @Tetin83! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20meta-llama%2Fcodellama%20%23218). Thanks!"
215,Address family not supported by protocol Error,"On running samples I am getting this error. I want to generate code context/documentation in simple language when provided a code in java. For that is codellama better or llama? 

```
myenv) [10:52]:[mehparmar@py029:codellama-main]$ torchrun --nproc_per_node 1 example_infilling.py \
>     --ckpt_dir CodeLlama-7b/ \
>     --tokenizer_path CodeLlama-7b/tokenizer.model \
>     --max_seq_len 192 --max_batch_size 4
[W socket.cpp:464] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""example_infilling.py"", line 79, in <module>
    fire.Fire(main)
  File ""/home/mehparmar/.conda/envs/myenv/lib/python3.8/site-packages/fire/core.py"", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/mehparmar/.conda/envs/myenv/lib/python3.8/site-packages/fire/core.py"", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/mehparmar/.conda/envs/myenv/lib/python3.8/site-packages/fire/core.py"", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""example_infilling.py"", line 18, in main
    generator = Llama.build(
  File ""/vol/etl_jupyterdata1/home/github/public/Sreeramm/codellama-main/llama/generation.py"", line 97, in build
    assert len(checkpoints) > 0, f""no checkpoint files found in {ckpt_dir}""
AssertionError: no checkpoint files found in CodeLlama-7b/
[2024-03-16 10:54:20,433] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 75378) of binary: /home/mehparmar/.conda/envs/myenv/bin/python
Traceback (most recent call last):
  File ""/home/mehparmar/.conda/envs/myenv/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/mehparmar/.conda/envs/myenv/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 347, in wrapper
    return f(*args, **kwargs)
  File ""/home/mehparmar/.conda/envs/myenv/lib/python3.8/site-packages/torch/distributed/run.py"", line 812, in main
    run(args)
  File ""/home/mehparmar/.conda/envs/myenv/lib/python3.8/site-packages/torch/distributed/run.py"", line 803, in run
    elastic_launch(
  File ""/home/mehparmar/.conda/envs/myenv/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/mehparmar/.conda/envs/myenv/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example_infilling.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-16_10:54:20
  host      : py029.lvs.abc.com
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 75378)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
```  ",KC888-cpu,"hi, i face this error too, did you solve this problem now? could you please share some ideas. thank you so much!"
212,Pre-Requisite package correction and programming.,Created scripts and updated readme for corrected pre-requisite packages.,facebook-github-bot,"Hi @mantanz! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%23212). Thanks!"
212,Pre-Requisite package correction and programming.,Created scripts and updated readme for corrected pre-requisite packages.,mantanz,CLA Agreement signed. Thanks for the opportunity
212,Pre-Requisite package correction and programming.,Created scripts and updated readme for corrected pre-requisite packages.,facebook-github-bot,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!
211,CodeLlama 中文text2sql效果如何？,,jgehring,"According to Google Translate, the English title is: ""How effective is CodeLlama Chinese text2sql?""

I don't speak Chinese and don't know of a corresponding benchmark so I can't make a statement about these capabilities. If you're asking about a specific text-to-sql tuned model I'd recommended getting in touch with the creators of these fine-tuned versions."
209,Question on specifying File Path for FIM prompt?,"Does CodeLlama have a token for including File Path in a prompt, just like Starcoder's `<filename>`?",jgehring,"Hi @icnahom, our models don't support special tokens or syntax for delimiting individual files in prompts out of the box."
208,70B model memory issue,"Hello,

I have 2 GPU of 24 GB RTX 4090 GPU. 

I want to fine-tune the 70b model but it throws a **cuda out of memory exceptions** even though I have used Lora and BitsAndBytesConfig.

Let me know if I'm overlooking this or please give me suggestions.

Thanks.",jgehring,"I can't really tell whether the configuration you provided is supposed to fit into your GPUs. If it doesn't work even with a batch size of 1, it's an indication that your hardware setup may not be sufficient. Try bringing this up to the maintainers or community of the tools you're using to fine-tune the model."
207,Loss calculation always 0,"Hello,

I'm trying to fine-tune the 34B model but during fine-tuning, I always get a loss 0. While I was able to fine-tune 7B and 13B models but not 34B.

Let me know if I'm overlooking this or please give me suggestions.

Thanks.",jgehring,"Hi @sanipanwala, we don't provide support for fine-tuning in this repository. Which tools are you using for this? Are you sure they support the 34B model well? The exact same setting works for 7B and 13B? In any case, a loss of 0 at the start of training is a good indication that something's going wrong."
207,Loss calculation always 0,"Hello,

I'm trying to fine-tune the 34B model but during fine-tuning, I always get a loss 0. While I was able to fine-tune 7B and 13B models but not 34B.

Let me know if I'm overlooking this or please give me suggestions.

Thanks.",sanipanwala,"@jgehring I mean I'm using ""codellama/CodeLlama-34b-hf"" model and running a normal Python script and yes same configuration works with 7B and 13B.

Thanks."
207,Loss calculation always 0,"Hello,

I'm trying to fine-tune the 34B model but during fine-tuning, I always get a loss 0. While I was able to fine-tune 7B and 13B models but not 34B.

Let me know if I'm overlooking this or please give me suggestions.

Thanks.",sssszh,"@sanipanwala 
Hi, have you solved this problem yet? 

I found the same problem when trying to peft fine-tune CodeLLama-7B (using LlamaForSequenceClassification), the Loss is always 0 during the fine-tuning.

Thanks！"
207,Loss calculation always 0,"Hello,

I'm trying to fine-tune the 34B model but during fine-tuning, I always get a loss 0. While I was able to fine-tune 7B and 13B models but not 34B.

Let me know if I'm overlooking this or please give me suggestions.

Thanks.",sanipanwala,"Hi @sssszh ,

No, I haven't found any solution yet.

Thanks,
Sani"
206,I'd like to know whether to use eos or bos during Code Llama pre-training,"I am curious about the form of the dataset for Code Llama pre-training. I want to know whether eos or bos was used during the pre-training process. For example, the data format is {code}{EOS} or {BOS}{code}, which format is used for Code Llama pre-training?",tangbo-sh,"I have same question.  When pre-training llama2(code llama), which one is used as the delimiter between samples: eos or bos?"
206,I'd like to know whether to use eos or bos during Code Llama pre-training,"I am curious about the form of the dataset for Code Llama pre-training. I want to know whether eos or bos was used during the pre-training process. For example, the data format is {code}{EOS} or {BOS}{code}, which format is used for Code Llama pre-training?",jgehring,"For training, we add both BOS and EOS tokens."
206,I'd like to know whether to use eos or bos during Code Llama pre-training,"I am curious about the form of the dataset for Code Llama pre-training. I want to know whether eos or bos was used during the pre-training process. For example, the data format is {code}{EOS} or {BOS}{code}, which format is used for Code Llama pre-training?",tangbo-sh,"@jgehring I understand, thanks for your reply."
205,Where is the attribute `past_key_values`,"I hope to use `past_key_values` for inferring longer texts, but even though I have set `use_cache=True`, I still cannot find it in the `generated_tokens`'s attributes.
```python
generated_tokens = model.generate(
                **model_input, 
                max_new_tokens=1600,
                use_cache=True
                )
```
",jgehring,"Hi @XinyuShe, I'm not sure what you're referring to since the repo doesn't feature a `generate()` function like the one you're describing."
204,CodeLlama went into infinite cycle (of communication),"I did not really measure, whether it's infinite (I did hit Ctrl+C before infinity), but it suddenly started to repeat itself:

tvali@PC366:~$ ollama run codellama:7b
>>> Is LDM a Deep Learning model?

LDM (Learning-based Decision Making) is a type of decision making model 
that uses machine learning algorithms to learn from data and make 
decisions. It is based on the concept of learning and improving over time^C

>>> Is Latent Diffusion Model a Deep Learning Model?

Latent Diffusion Model (LDM) is a type of deep learning model that uses a 
probabilistic approach to generate images. It is based on the concept of 
diffusion models, which are statistical models that describe how data 
points move over time. In the context of LDM, the diffusion process is 
used to model the generation of images, and the model learns to generate 
new images by iteratively refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is^C

>>> Send a message (/? for help)

-------------------

It's Ubuntu 22.04, and as I don't know what more could you need, I ran all those commands:

>>> /show info
Model details:
Family              llama
Parameter Size      7B
Quantization Level  Q4_0

>>> /show modelfile
# Modelfile generated by ""ollama show""
# To build a new Modelfile based on this one, replace the FROM line with:
# FROM codellama:7b

FROM /usr/share/ollama/.ollama/models/blobs/sha256:3a43f93b78ec50f7c4e4dc8bd1cb3fff5a900e7d574c51a6f7495e48486e0dac
TEMPLATE """"""[INST] <<SYS>>{{ .System }}<</SYS>>

{{ .Prompt }} [/INST]
""""""
PARAMETER rope_frequency_base 1e+06
PARAMETER stop ""[INST]""
PARAMETER stop ""[/INST]""
PARAMETER stop ""<<SYS>>""
PARAMETER stop ""<</SYS>>""
>>> /show parameters
Model defined parameters:
stop                           ""[INST]""
stop                           ""[/INST]""
stop                           ""<<SYS>>""
stop                           ""<</SYS>>""
rope_frequency_base            1e+06
>>> Send a message (/? for help)
",jgehring,I can't provide support for the quantized versions of Code Llama. Can you double-check that you get similar behavior with inference code from this repo and a model retrieved via `download.sh`?
204,CodeLlama went into infinite cycle (of communication),"I did not really measure, whether it's infinite (I did hit Ctrl+C before infinity), but it suddenly started to repeat itself:

tvali@PC366:~$ ollama run codellama:7b
>>> Is LDM a Deep Learning model?

LDM (Learning-based Decision Making) is a type of decision making model 
that uses machine learning algorithms to learn from data and make 
decisions. It is based on the concept of learning and improving over time^C

>>> Is Latent Diffusion Model a Deep Learning Model?

Latent Diffusion Model (LDM) is a type of deep learning model that uses a 
probabilistic approach to generate images. It is based on the concept of 
diffusion models, which are statistical models that describe how data 
points move over time. In the context of LDM, the diffusion process is 
used to model the generation of images, and the model learns to generate 
new images by iteratively refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is^C

>>> Send a message (/? for help)

-------------------

It's Ubuntu 22.04, and as I don't know what more could you need, I ran all those commands:

>>> /show info
Model details:
Family              llama
Parameter Size      7B
Quantization Level  Q4_0

>>> /show modelfile
# Modelfile generated by ""ollama show""
# To build a new Modelfile based on this one, replace the FROM line with:
# FROM codellama:7b

FROM /usr/share/ollama/.ollama/models/blobs/sha256:3a43f93b78ec50f7c4e4dc8bd1cb3fff5a900e7d574c51a6f7495e48486e0dac
TEMPLATE """"""[INST] <<SYS>>{{ .System }}<</SYS>>

{{ .Prompt }} [/INST]
""""""
PARAMETER rope_frequency_base 1e+06
PARAMETER stop ""[INST]""
PARAMETER stop ""[/INST]""
PARAMETER stop ""<<SYS>>""
PARAMETER stop ""<</SYS>>""
>>> /show parameters
Model defined parameters:
stop                           ""[INST]""
stop                           ""[/INST]""
stop                           ""<<SYS>>""
stop                           ""<</SYS>>""
rope_frequency_base            1e+06
>>> Send a message (/? for help)
",tambetvali,"Sorry I have 5GB left of hard drive, because I am without internet at home and I have to download things ..cannot you simply ask the same question? I don't know how to reproduce, with AI, as well - does it answer the same way every time? I noticed another bug as well, it does not always differentiate between ""me"" and ""you"".

Later time, I want to go deeper with CodeLlama - I learn some AI programming and I have downloaded some videos about programming CodeLlama and creating it from scratch as well. I was some time away from programming and now I notice how evolved is AI -  I have to learn a lot right now :)

"
204,CodeLlama went into infinite cycle (of communication),"I did not really measure, whether it's infinite (I did hit Ctrl+C before infinity), but it suddenly started to repeat itself:

tvali@PC366:~$ ollama run codellama:7b
>>> Is LDM a Deep Learning model?

LDM (Learning-based Decision Making) is a type of decision making model 
that uses machine learning algorithms to learn from data and make 
decisions. It is based on the concept of learning and improving over time^C

>>> Is Latent Diffusion Model a Deep Learning Model?

Latent Diffusion Model (LDM) is a type of deep learning model that uses a 
probabilistic approach to generate images. It is based on the concept of 
diffusion models, which are statistical models that describe how data 
points move over time. In the context of LDM, the diffusion process is 
used to model the generation of images, and the model learns to generate 
new images by iteratively refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is^C

>>> Send a message (/? for help)

-------------------

It's Ubuntu 22.04, and as I don't know what more could you need, I ran all those commands:

>>> /show info
Model details:
Family              llama
Parameter Size      7B
Quantization Level  Q4_0

>>> /show modelfile
# Modelfile generated by ""ollama show""
# To build a new Modelfile based on this one, replace the FROM line with:
# FROM codellama:7b

FROM /usr/share/ollama/.ollama/models/blobs/sha256:3a43f93b78ec50f7c4e4dc8bd1cb3fff5a900e7d574c51a6f7495e48486e0dac
TEMPLATE """"""[INST] <<SYS>>{{ .System }}<</SYS>>

{{ .Prompt }} [/INST]
""""""
PARAMETER rope_frequency_base 1e+06
PARAMETER stop ""[INST]""
PARAMETER stop ""[/INST]""
PARAMETER stop ""<<SYS>>""
PARAMETER stop ""<</SYS>>""
>>> /show parameters
Model defined parameters:
stop                           ""[INST]""
stop                           ""[/INST]""
stop                           ""<<SYS>>""
stop                           ""<</SYS>>""
rope_frequency_base            1e+06
>>> Send a message (/? for help)
",jgehring,"Can you check whether ollama's `codellama:7b` is built from the ""Code Llama"" or the ""Code Llama Instruct"" model? If it's the former then it won't know out of the box when to stop. For uses-cases like question answering, you would either need to use few-shot prompting or use one of the Instruct models."
204,CodeLlama went into infinite cycle (of communication),"I did not really measure, whether it's infinite (I did hit Ctrl+C before infinity), but it suddenly started to repeat itself:

tvali@PC366:~$ ollama run codellama:7b
>>> Is LDM a Deep Learning model?

LDM (Learning-based Decision Making) is a type of decision making model 
that uses machine learning algorithms to learn from data and make 
decisions. It is based on the concept of learning and improving over time^C

>>> Is Latent Diffusion Model a Deep Learning Model?

Latent Diffusion Model (LDM) is a type of deep learning model that uses a 
probabilistic approach to generate images. It is based on the concept of 
diffusion models, which are statistical models that describe how data 
points move over time. In the context of LDM, the diffusion process is 
used to model the generation of images, and the model learns to generate 
new images by iteratively refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is^C

>>> Send a message (/? for help)

-------------------

It's Ubuntu 22.04, and as I don't know what more could you need, I ran all those commands:

>>> /show info
Model details:
Family              llama
Parameter Size      7B
Quantization Level  Q4_0

>>> /show modelfile
# Modelfile generated by ""ollama show""
# To build a new Modelfile based on this one, replace the FROM line with:
# FROM codellama:7b

FROM /usr/share/ollama/.ollama/models/blobs/sha256:3a43f93b78ec50f7c4e4dc8bd1cb3fff5a900e7d574c51a6f7495e48486e0dac
TEMPLATE """"""[INST] <<SYS>>{{ .System }}<</SYS>>

{{ .Prompt }} [/INST]
""""""
PARAMETER rope_frequency_base 1e+06
PARAMETER stop ""[INST]""
PARAMETER stop ""[/INST]""
PARAMETER stop ""<<SYS>>""
PARAMETER stop ""<</SYS>>""
>>> /show parameters
Model defined parameters:
stop                           ""[INST]""
stop                           ""[/INST]""
stop                           ""<<SYS>>""
stop                           ""<</SYS>>""
rope_frequency_base            1e+06
>>> Send a message (/? for help)
",tambetvali,"How to check it?

I followed these installation instructions:
- Install, in VSC, the Continue plugin (it might not matter, but for information)
- Install Ollama, probably as instructed on their site - https://ollama.com/download ""install with one command""
- VSC, after I chose Codellama:7b, instructed to run the command ""ollama run codellama:7b""
- I ran this command and it downloaded some kind of model, which started to work
- This issue is from another run of this same command, ""ollama run codellama:7b"", which downloads the model if it's not there and runs it in any case.

So, following this, it has to be model described here:
https://ollama.com/library/codellama:7b

ID
8fdf8f752f6e
Model family
llama
Parameters
7B
Quantization
4-bit
Last pushed
4 months ago

It also has this information (https://ollama.com/library/codellama):

API

curl -X POST http://localhost:11434/api/generate -d '{
  ""model"": ""codellama"",
  ""prompt"": ""Write me a function that outputs the fibonacci sequence""
}'

""model"": ""codellama"" probably means that it's not an instruct model (repeating the words like a monkey, as I really don't know, what's the difference - but it's probably as good as an AI answer)."
204,CodeLlama went into infinite cycle (of communication),"I did not really measure, whether it's infinite (I did hit Ctrl+C before infinity), but it suddenly started to repeat itself:

tvali@PC366:~$ ollama run codellama:7b
>>> Is LDM a Deep Learning model?

LDM (Learning-based Decision Making) is a type of decision making model 
that uses machine learning algorithms to learn from data and make 
decisions. It is based on the concept of learning and improving over time^C

>>> Is Latent Diffusion Model a Deep Learning Model?

Latent Diffusion Model (LDM) is a type of deep learning model that uses a 
probabilistic approach to generate images. It is based on the concept of 
diffusion models, which are statistical models that describe how data 
points move over time. In the context of LDM, the diffusion process is 
used to model the generation of images, and the model learns to generate 
new images by iteratively refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is^C

>>> Send a message (/? for help)

-------------------

It's Ubuntu 22.04, and as I don't know what more could you need, I ran all those commands:

>>> /show info
Model details:
Family              llama
Parameter Size      7B
Quantization Level  Q4_0

>>> /show modelfile
# Modelfile generated by ""ollama show""
# To build a new Modelfile based on this one, replace the FROM line with:
# FROM codellama:7b

FROM /usr/share/ollama/.ollama/models/blobs/sha256:3a43f93b78ec50f7c4e4dc8bd1cb3fff5a900e7d574c51a6f7495e48486e0dac
TEMPLATE """"""[INST] <<SYS>>{{ .System }}<</SYS>>

{{ .Prompt }} [/INST]
""""""
PARAMETER rope_frequency_base 1e+06
PARAMETER stop ""[INST]""
PARAMETER stop ""[/INST]""
PARAMETER stop ""<<SYS>>""
PARAMETER stop ""<</SYS>>""
>>> /show parameters
Model defined parameters:
stop                           ""[INST]""
stop                           ""[/INST]""
stop                           ""<<SYS>>""
stop                           ""<</SYS>>""
rope_frequency_base            1e+06
>>> Send a message (/? for help)
",jgehring,"Thanks! Yeah that seems to be the base model which is trained purely to continue the sequence you prompt it with. For any Q&A-style or chat scenarios, try the 7b-instruct model."
204,CodeLlama went into infinite cycle (of communication),"I did not really measure, whether it's infinite (I did hit Ctrl+C before infinity), but it suddenly started to repeat itself:

tvali@PC366:~$ ollama run codellama:7b
>>> Is LDM a Deep Learning model?

LDM (Learning-based Decision Making) is a type of decision making model 
that uses machine learning algorithms to learn from data and make 
decisions. It is based on the concept of learning and improving over time^C

>>> Is Latent Diffusion Model a Deep Learning Model?

Latent Diffusion Model (LDM) is a type of deep learning model that uses a 
probabilistic approach to generate images. It is based on the concept of 
diffusion models, which are statistical models that describe how data 
points move over time. In the context of LDM, the diffusion process is 
used to model the generation of images, and the model learns to generate 
new images by iteratively refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is^C

>>> Send a message (/? for help)

-------------------

It's Ubuntu 22.04, and as I don't know what more could you need, I ran all those commands:

>>> /show info
Model details:
Family              llama
Parameter Size      7B
Quantization Level  Q4_0

>>> /show modelfile
# Modelfile generated by ""ollama show""
# To build a new Modelfile based on this one, replace the FROM line with:
# FROM codellama:7b

FROM /usr/share/ollama/.ollama/models/blobs/sha256:3a43f93b78ec50f7c4e4dc8bd1cb3fff5a900e7d574c51a6f7495e48486e0dac
TEMPLATE """"""[INST] <<SYS>>{{ .System }}<</SYS>>

{{ .Prompt }} [/INST]
""""""
PARAMETER rope_frequency_base 1e+06
PARAMETER stop ""[INST]""
PARAMETER stop ""[/INST]""
PARAMETER stop ""<<SYS>>""
PARAMETER stop ""<</SYS>>""
>>> /show parameters
Model defined parameters:
stop                           ""[INST]""
stop                           ""[/INST]""
stop                           ""<<SYS>>""
stop                           ""<</SYS>>""
rope_frequency_base            1e+06
>>> Send a message (/? for help)
",jgehring,"You can also check the examples in `example_completion.py`, for which the base models will output sensible completions:
https://github.com/facebookresearch/codellama/blob/1af62e1f43db1fa5140fa43cb828465a603a48f3/example_completion.py#L27-L39"
204,CodeLlama went into infinite cycle (of communication),"I did not really measure, whether it's infinite (I did hit Ctrl+C before infinity), but it suddenly started to repeat itself:

tvali@PC366:~$ ollama run codellama:7b
>>> Is LDM a Deep Learning model?

LDM (Learning-based Decision Making) is a type of decision making model 
that uses machine learning algorithms to learn from data and make 
decisions. It is based on the concept of learning and improving over time^C

>>> Is Latent Diffusion Model a Deep Learning Model?

Latent Diffusion Model (LDM) is a type of deep learning model that uses a 
probabilistic approach to generate images. It is based on the concept of 
diffusion models, which are statistical models that describe how data 
points move over time. In the context of LDM, the diffusion process is 
used to model the generation of images, and the model learns to generate 
new images by iteratively refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is a deep learning model that uses a probabilistic approach to 
generate images. It is based on the concept of diffusion models, which are
statistical models that describe how data points move over time. In the 
context of LDM, the diffusion process is used to model the generation of 
images, and the model learns to generate new images by iteratively 
refining its predictions.

LDM is a type of Generative Adversarial Network (GAN) that uses a 
probabilistic approach to generate images. It is based on the concept of 
learning and improving over time, using a process called iterative 
refinement. The model learns to generate new images by iteratively 
refining its predictions, and it can be used for various tasks such as 
image generation, image-to-image translation, and image synthesis.

LDM is^C

>>> Send a message (/? for help)

-------------------

It's Ubuntu 22.04, and as I don't know what more could you need, I ran all those commands:

>>> /show info
Model details:
Family              llama
Parameter Size      7B
Quantization Level  Q4_0

>>> /show modelfile
# Modelfile generated by ""ollama show""
# To build a new Modelfile based on this one, replace the FROM line with:
# FROM codellama:7b

FROM /usr/share/ollama/.ollama/models/blobs/sha256:3a43f93b78ec50f7c4e4dc8bd1cb3fff5a900e7d574c51a6f7495e48486e0dac
TEMPLATE """"""[INST] <<SYS>>{{ .System }}<</SYS>>

{{ .Prompt }} [/INST]
""""""
PARAMETER rope_frequency_base 1e+06
PARAMETER stop ""[INST]""
PARAMETER stop ""[/INST]""
PARAMETER stop ""<<SYS>>""
PARAMETER stop ""<</SYS>>""
>>> /show parameters
Model defined parameters:
stop                           ""[INST]""
stop                           ""[/INST]""
stop                           ""<<SYS>>""
stop                           ""<</SYS>>""
rope_frequency_base            1e+06
>>> Send a message (/? for help)
",jgehring,I'll close this for now; feel free to re-open if you're still having issues with this.
203,Unable to run example_completion.py on CodeLlama-7b,"Hi,
I have a single GPU on my system and I am using CodeLlama-7b to test my environment.
I am running into the following error when I run the sample.

```
$ torchrun --nproc_per_node 1 example_completion.py    \
 --ckpt_dir CodeLlama-7b    \
 --tokenizer_path CodeLlama-7b/tokenizer.model    \
 --max_seq_len 128 --max_batch_size 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/home/aditya/rb16/Code/llama-ft/codellama/example_completion.py"", line 53, in <module>
    fire.Fire(main)
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/rb16/Code/llama-ft/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""/home/aditya/rb16/Code/llama-ft/codellama/llama/generation.py"", line 102, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1026, in load
    return _load(opened_zipfile,
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1438, in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1408, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1373, in load_tensor
    storage = zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)._typed_storage()._untyped_storage
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error
[2024-02-17 13:26:43,422] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3852309) of binary: /home/aditya/anaconda3/bin/python
Traceback (most recent call last):
  File ""/home/aditya/anaconda3/bin/torchrun"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.2.0', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 347, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py"", line 812, in main
    run(args)
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py"", line 803, in run
    elastic_launch(
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-17_13:26:43
  host      : stormbreaker
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3852309)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html

```

```
$ ls -ltr ./CodeLlama-7b
total 13169098
-rw-rw-r-- 1 aditya aditya      500058 Aug 21 14:32 tokenizer.model
-rw-rw-r-- 1 aditya aditya         163 Aug 21 14:32 params.json
-rw-rw-r-- 1 aditya aditya 13477187307 Aug 21 14:32 consolidated.00.pth
-rw-rw-r-- 1 aditya aditya         150 Aug 21 14:32 checklist.chk
```

```
$ echo $CUDA_VISIBLE_DEVICES
0
```

The conda env
```
channels:
  - pytorch
  - nvidia
dependencies:
  - numpy
  - pandas
  - pytorch-cuda=12.1
  - pytorch
  - torchvision
  - torchaudio
variables:
  CUDA_PATH: /usr/local/cuda-12.1
```
",jgehring,"Hi @aditya4d1, to rule out corrupted files (which the error message seems to point to), can you run `md5sum -c checklist.chk` in the CodeLlama-7b directory?"
203,Unable to run example_completion.py on CodeLlama-7b,"Hi,
I have a single GPU on my system and I am using CodeLlama-7b to test my environment.
I am running into the following error when I run the sample.

```
$ torchrun --nproc_per_node 1 example_completion.py    \
 --ckpt_dir CodeLlama-7b    \
 --tokenizer_path CodeLlama-7b/tokenizer.model    \
 --max_seq_len 128 --max_batch_size 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/home/aditya/rb16/Code/llama-ft/codellama/example_completion.py"", line 53, in <module>
    fire.Fire(main)
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/rb16/Code/llama-ft/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""/home/aditya/rb16/Code/llama-ft/codellama/llama/generation.py"", line 102, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1026, in load
    return _load(opened_zipfile,
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1438, in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1408, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1373, in load_tensor
    storage = zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)._typed_storage()._untyped_storage
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error
[2024-02-17 13:26:43,422] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3852309) of binary: /home/aditya/anaconda3/bin/python
Traceback (most recent call last):
  File ""/home/aditya/anaconda3/bin/torchrun"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.2.0', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 347, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py"", line 812, in main
    run(args)
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py"", line 803, in run
    elastic_launch(
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-17_13:26:43
  host      : stormbreaker
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3852309)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html

```

```
$ ls -ltr ./CodeLlama-7b
total 13169098
-rw-rw-r-- 1 aditya aditya      500058 Aug 21 14:32 tokenizer.model
-rw-rw-r-- 1 aditya aditya         163 Aug 21 14:32 params.json
-rw-rw-r-- 1 aditya aditya 13477187307 Aug 21 14:32 consolidated.00.pth
-rw-rw-r-- 1 aditya aditya         150 Aug 21 14:32 checklist.chk
```

```
$ echo $CUDA_VISIBLE_DEVICES
0
```

The conda env
```
channels:
  - pytorch
  - nvidia
dependencies:
  - numpy
  - pandas
  - pytorch-cuda=12.1
  - pytorch
  - torchvision
  - torchaudio
variables:
  CUDA_PATH: /usr/local/cuda-12.1
```
",aditya4d,"@jgehring 
```
md5sum: consolidated.00.pth: Input/output error
consolidated.00.pth: FAILED open or read
params.json: OK
tokenizer.model: OK
md5sum: WARNING: 1 listed file could not be read
```

should i re-download the weights?

Update:
Re-downloaded the weights. Ran into checksum error again
```
Checking checksums
consolidated.00.pth: FAILED
params.json: OK
tokenizer.model: OK
md5sum: WARNING: 1 computed checksum did NOT match
```"
203,Unable to run example_completion.py on CodeLlama-7b,"Hi,
I have a single GPU on my system and I am using CodeLlama-7b to test my environment.
I am running into the following error when I run the sample.

```
$ torchrun --nproc_per_node 1 example_completion.py    \
 --ckpt_dir CodeLlama-7b    \
 --tokenizer_path CodeLlama-7b/tokenizer.model    \
 --max_seq_len 128 --max_batch_size 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/home/aditya/rb16/Code/llama-ft/codellama/example_completion.py"", line 53, in <module>
    fire.Fire(main)
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/rb16/Code/llama-ft/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""/home/aditya/rb16/Code/llama-ft/codellama/llama/generation.py"", line 102, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1026, in load
    return _load(opened_zipfile,
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1438, in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1408, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1373, in load_tensor
    storage = zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)._typed_storage()._untyped_storage
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error
[2024-02-17 13:26:43,422] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3852309) of binary: /home/aditya/anaconda3/bin/python
Traceback (most recent call last):
  File ""/home/aditya/anaconda3/bin/torchrun"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.2.0', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 347, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py"", line 812, in main
    run(args)
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py"", line 803, in run
    elastic_launch(
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-17_13:26:43
  host      : stormbreaker
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3852309)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html

```

```
$ ls -ltr ./CodeLlama-7b
total 13169098
-rw-rw-r-- 1 aditya aditya      500058 Aug 21 14:32 tokenizer.model
-rw-rw-r-- 1 aditya aditya         163 Aug 21 14:32 params.json
-rw-rw-r-- 1 aditya aditya 13477187307 Aug 21 14:32 consolidated.00.pth
-rw-rw-r-- 1 aditya aditya         150 Aug 21 14:32 checklist.chk
```

```
$ echo $CUDA_VISIBLE_DEVICES
0
```

The conda env
```
channels:
  - pytorch
  - nvidia
dependencies:
  - numpy
  - pandas
  - pytorch-cuda=12.1
  - pytorch
  - torchvision
  - torchaudio
variables:
  CUDA_PATH: /usr/local/cuda-12.1
```
",irmaerviana1993,[https://download2.llamameta. net/*?Policy= eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV 9oYXNoIjoic2t3emhrb25pbnNmdTVi MHNubXJqa2ZyIiwiUmVzb3VyY2UiOi JodHRwczpcL1wvZG93bmxvYWQyLmxs YW1hbWV0YS5uZXRcLyoiLCJDb25kaX Rpb24iOnsiRGF0ZUxlc3NUaGFuIjp7 IkFXUzpFcG9jaFRpbWUiOjE3MDg4ND UwMTV9fX1dfQ__&Signature= Nxra9pLsscgQLsZoZBSEiNvG2Y9JUb RRum34HfejV26KWsjEcIk%7En% 7EwIeuI8Rj0hNXToqbZ% 7EwLZNAXiTrZRzhVFsq3xhyUvAolTm RaaN9P9VBTRPKE- Ui1bPh8bp25C8y5AyzQal3MH2jsixV jQGoWjbNpBCgu% 7EDp5zDMBROtZz5JWDV2rFv- YQBAg5EKY-7bJ3tTJuwUr- sOdMFdjHz0V% 7Ef3EPl6IX0inTCL97uFm0w5taacpQ tbshySH3ZN1Ls0Q8mLVpD%7E1M% 7ElBOt23bKiZVNgFHOCorkGFeJLqe4 5ncFAIUlf8p42kvw2V7QeKqpyEHXd6 vHsPSoHjVauDpDpw__&Key-Pair- Id=K15QRJLYKIFSLZ&Download - ID Permintaan=1456084395000243](https://download2.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoic2t3emhrb25pbnNmdTViMHNubXJqa2ZyIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3MDg4NDUwMTV9fX1dfQ__&Signature=Nxra9pLsscgQLsZoZBSEiNvG2Y9JUbRRum34HfejV26KWsjEcIk%7En%7EwIeuI8Rj0hNXToqbZ%7EwLZNAXiTrZRzhVFsq3xhyUvAolTmRaaN9P9VBTRPKE-Ui1bPh8bp25C8y5AyzQal3MH2jsixVjQGoWjbNpBCgu%7EDp5zDMBROtZz5JWDV2rFv-YQBAg5EKY-7bJ3tTJuwUr-sOdMFdjHz0V%7Ef3EPl6IX0inTCL97uFm0w5taacpQtbshySH3ZN1Ls0Q8mLVpD%7E1M%7ElBOt23bKiZVNgFHOCorkGFeJLqe45ncFAIUlf8p42kvw2V7QeKqpyEHXd6vHsPSoHjVauDpDpw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1456084395000243)
203,Unable to run example_completion.py on CodeLlama-7b,"Hi,
I have a single GPU on my system and I am using CodeLlama-7b to test my environment.
I am running into the following error when I run the sample.

```
$ torchrun --nproc_per_node 1 example_completion.py    \
 --ckpt_dir CodeLlama-7b    \
 --tokenizer_path CodeLlama-7b/tokenizer.model    \
 --max_seq_len 128 --max_batch_size 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/home/aditya/rb16/Code/llama-ft/codellama/example_completion.py"", line 53, in <module>
    fire.Fire(main)
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/rb16/Code/llama-ft/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""/home/aditya/rb16/Code/llama-ft/codellama/llama/generation.py"", line 102, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1026, in load
    return _load(opened_zipfile,
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1438, in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1408, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1373, in load_tensor
    storage = zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)._typed_storage()._untyped_storage
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error
[2024-02-17 13:26:43,422] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3852309) of binary: /home/aditya/anaconda3/bin/python
Traceback (most recent call last):
  File ""/home/aditya/anaconda3/bin/torchrun"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.2.0', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 347, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py"", line 812, in main
    run(args)
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py"", line 803, in run
    elastic_launch(
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-17_13:26:43
  host      : stormbreaker
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3852309)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html

```

```
$ ls -ltr ./CodeLlama-7b
total 13169098
-rw-rw-r-- 1 aditya aditya      500058 Aug 21 14:32 tokenizer.model
-rw-rw-r-- 1 aditya aditya         163 Aug 21 14:32 params.json
-rw-rw-r-- 1 aditya aditya 13477187307 Aug 21 14:32 consolidated.00.pth
-rw-rw-r-- 1 aditya aditya         150 Aug 21 14:32 checklist.chk
```

```
$ echo $CUDA_VISIBLE_DEVICES
0
```

The conda env
```
channels:
  - pytorch
  - nvidia
dependencies:
  - numpy
  - pandas
  - pytorch-cuda=12.1
  - pytorch
  - torchvision
  - torchaudio
variables:
  CUDA_PATH: /usr/local/cuda-12.1
```
",aditya4d,ping @jgehring 
202,fine-tuning CodeLlama-34b loss,"Hello,

I'm fine-tuning using the **CodeLlama-34b** as a base model. During training My loss always shows me 0 with all the datasets.

Would someone be able to help me with this?

Thanks.",jgehring,Same question as #207?
201,Does codellama 13B/34B/70B support function calling and Lora fine tuning on multi-run chat with function calling?,"Does codellama 13B/34B/70B support function calling and Lora fine tuning on multi-run chat with function calling?
Are there any instruction pages about this? Thanks a lot.",jgehring,"Hi @zlfeng83, Code Llama models have not be specifically trained or evaluated for function calling. However, I would think that it might be worth experimenting with it and maybe try to fine-tune it for these use-cases.

This repository does not support LoRA (or any fine-tuning for that matter), but there are several open-source projects offering this functionality.

Regarding multi-run chat: all instruction models support multi-turn prompts (i.e., user -> assistant -> user ->  assistant -> ... -> user) out of the box, while the other models can either be few-shot prompted or fine-tuned to support multi-turn chat given sufficient example data."
200,CodeLlama-34b Fine-Tune Evaluation,"Hello,

I have done training and saved the model, and adapter config file on the local disk.

When I load the model from the local disk again to generate the output I get the below error. 

Anyone can help me with this issue?

`File ""PythonV2.py"", line 11, in <module>
    model = AutoPeftModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/python3.12/site-packages/peft/auto.py"", line 127, in from_pretrained
    return cls._target_peft_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/python3.12/site-packages/peft/peft_model.py"", line 354, in from_pretrained
    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)
  File ""/python3.12/site-packages/peft/peft_model.py"", line 695, in load_adapter
    adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/python3.12/site-packages/peft/utils/save_and_load.py"", line 313, in load_peft_weights
    adapters_weights = safe_load_file(filename, device=device)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/python3.12/site-packages/safetensors/torch.py"", line 308, in load_file
    with safe_open(filename, framework=""pt"", device=device) as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
safetensors_rust.SafetensorError: Error while deserializing header: InvalidHeaderDeserialization
`

Thanks.",jgehring,"Hi @sanipanwala, I'd post this question in Github issues or other forums related to the fine-tuning and/or inference library you're using (supposedly https://github.com/huggingface/transformers?). The codellama repository offers a reference implementation for inference only."
199,MAYCHU AC,,facebook-github-bot,"Hi @MAYCHUI! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%23199). Thanks!"
199,MAYCHU AC,,MAYCHUI,"Vào 9:35, Th 5, 1 thg 2, 2024 Facebook Community Bot <
***@***.***> đã viết:

> Hi @MAYCHUI <https://github.com/MAYCHUI>!
>
> Thank you for your pull request and welcome to our community.
> Action Required
>
> In order to merge *any pull request* (code, docs, etc.), we *require*
> contributors to sign our *Contributor License Agreement*, and we don't
> seem to have one on file for you.
> Process
>
> In order for us to review and merge your suggested changes, please sign at
> https://github.com/MAYCHUI/README <https://code.facebook.com/cla>. *If
> you are contributing on behalf of someone else (eg your employer)*, the
> individual CLA may not be sufficient and your employer may need to sign the
> corporate CLA.
>
> Once the CLA is signed, our tooling will perform checks and validations.
> Afterwards, the *pull request will be tagged* with CLA signed. The
> tagging process may take up to 1 hour after signing. Please give it that
> time before contacting us about it.
>
> If you have received this in error or have any questions, please contact
> us at https://github.com/MAYCHUI/README
> ***@***.***?subject=CLA%20for%20facebookresearch%2Fcodellama%20%23199>.
> Thanks!
>
> —
> Reply to this email directly, https://github.com/MAYCHUI/README
> <https://github.com/facebookresearch/codellama/pull/199#issuecomment-1920384048>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/BFOFK6LNH66UY4FRXZPHWO3YRL5OTAVCNFSM6AAAAABCUDOZSSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSMRQGM4DIMBUHA>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
"
199,MAYCHU AC,,MAYCHUI,"[![.github/workflows/python-package.yml](
https://github.com/MAYCHUI/codellama/actions/workflows/python-package.yml/badge.svg)](https://github.com/MAYCHUI/codellama/actions/workflows/python-package.yml
)

Vào 9:40, Th 5, 1 thg 2, 2024 Huan Tran ***@***.***> đã viết:

>
>
> Vào 9:35, Th 5, 1 thg 2, 2024 Facebook Community Bot <
> ***@***.***> đã viết:
>
>> Hi @MAYCHUI <https://github.com/MAYCHUI>!
>>
>> Thank you for your pull request and welcome to our community.
>> Action Required
>>
>> In order to merge *any pull request* (code, docs, etc.), we *require*
>> contributors to sign our *Contributor License Agreement*, and we don't
>> seem to have one on file for you.
>> Process
>>
>> In order for us to review and merge your suggested changes, please sign
>> at https://github.com/MAYCHUI/README <https://code.facebook.com/cla>. *If
>> you are contributing on behalf of someone else (eg your employer)*, the
>> individual CLA may not be sufficient and your employer may need to sign the
>> corporate CLA.
>>
>> Once the CLA is signed, our tooling will perform checks and validations.
>> Afterwards, the *pull request will be tagged* with CLA signed. The
>> tagging process may take up to 1 hour after signing. Please give it that
>> time before contacting us about it.
>>
>> If you have received this in error or have any questions, please contact
>> us at https://github.com/MAYCHUI/README
>> ***@***.***?subject=CLA%20for%20facebookresearch%2Fcodellama%20%23199>.
>> Thanks!
>>
>> —
>> Reply to this email directly, https://github.com/MAYCHUI/README
>> <https://github.com/facebookresearch/codellama/pull/199#issuecomment-1920384048>,
>> or unsubscribe
>> <https://github.com/notifications/unsubscribe-auth/BFOFK6LNH66UY4FRXZPHWO3YRL5OTAVCNFSM6AAAAABCUDOZSSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSMRQGM4DIMBUHA>
>> .
>> You are receiving this because you were mentioned.Message ID:
>> ***@***.***>
>>
>
"
199,MAYCHU AC,,MAYCHUI,[![.github/workflows/python-package.yml](https://github.com/MAYCHUI/codellama/actions/workflows/python-package.yml/badge.svg)](https://github.com/MAYCHUI/codellama/actions/workflows/python-package.yml)
197,Code Llama 13b download failed: no properly formatted checksum lines found,"I ran the bash script from the command console and received the following message and the download was aborted:

Checking checksums
md5sum: checklist.chk: no properly formatted checksum lines found

Any ideas?

",ankitrs7,Got same issue.
197,Code Llama 13b download failed: no properly formatted checksum lines found,"I ran the bash script from the command console and received the following message and the download was aborted:

Checking checksums
md5sum: checklist.chk: no properly formatted checksum lines found

Any ideas?

",jgehring,Can you double-check the contents of the `checksum.chk` in the directory that was downloaded?
197,Code Llama 13b download failed: no properly formatted checksum lines found,"I ran the bash script from the command console and received the following message and the download was aborted:

Checking checksums
md5sum: checklist.chk: no properly formatted checksum lines found

Any ideas?

",kraso,"> Can you double-check the contents of the `checksum.chk` in the directory that was downloaded?

[checklist.zip](https://github.com/facebookresearch/codellama/files/14150812/checklist.zip)

Hello, here is a compressed copy of the file.

"
197,Code Llama 13b download failed: no properly formatted checksum lines found,"I ran the bash script from the command console and received the following message and the download was aborted:

Checking checksums
md5sum: checklist.chk: no properly formatted checksum lines found

Any ideas?

",jgehring,"Thanks for sharing; your checklist file seems to be an HTML page for signing into a Google account. Just to double-check, you ran `bash download.sh` and pasted the link you received in the e-mail from Meta, titled ""Get started with Code Llama""? And this operation produced this checksum.chk file? What do the other files in the directory where you have the checksum file look like (name and size)?"
197,Code Llama 13b download failed: no properly formatted checksum lines found,"I ran the bash script from the command console and received the following message and the download was aborted:

Checking checksums
md5sum: checklist.chk: no properly formatted checksum lines found

Any ideas?

",kraso,"> Thanks for sharing; your checklist file seems to be an HTML page for signing into a Google account. Just to double-check, you ran `bash download.sh` and pasted the link you received in the e-mail from Meta, titled ""Get started with Code Llama""? And this operation produced this checksum.chk file? What do the other files in the directory where you have the checksum file look like (name and size)?

No, I didn't have time to paste the link that I received in the email because no interface appeared to provide that information. Unless you have to supply the information to the command as an argument, I don't know how to pass the email URL to it.

These are the files into the CodeLlama-13b directory:

-rw-r--r-- 1  172576 feb  3 09:56 checklist.chk
-rw-r--r-- 1  172177 feb  3 09:56 consolidated.00.pth
-rw-r--r-- 1  172167 feb  3 09:56 consolidated.01.pth
-rw-r--r-- 1  172152 feb  3 09:56 params.json
-rw-r--r-- 1  172013 feb  3 09:56 tokenizer.model
"
197,Code Llama 13b download failed: no properly formatted checksum lines found,"I ran the bash script from the command console and received the following message and the download was aborted:

Checking checksums
md5sum: checklist.chk: no properly formatted checksum lines found

Any ideas?

",allelive,"> > Grazie per la condivisione; il file della tua lista di controllo sembra essere una pagina HTML per l'accesso a un account Google. Giusto per ricontrollare, hai eseguito `bash download.sh`e incollato il collegamento ricevuto nell'e-mail da Meta, intitolato ""Inizia con Code Llama""? E questa operazione ha prodotto questo file checksum.chk? Che aspetto hanno gli altri file nella directory in cui hai il file di checksum (nome e dimensione)?
> 
> No, non ho avuto il tempo di incollare il collegamento che ho ricevuto nell'e-mail perché nessuna interfaccia sembrava fornire tali informazioni. A meno che tu non debba fornire le informazioni al comando come argomento, non so come passargli l'URL dell'e-mail.
> 
> Questi sono i file nella directory CodeLlama-13b:
> 
> -rw-r--r-- 1 172576 3 feb 09:56 checklist.chk -rw-r--r-- 1 172177 3 feb 09:56 consolidato.00.pth -rw-r--r-- 1 172167 3 feb 09:56 consolidato.01.pth -rw-r--r-- 1 172152 3 feb 09:56 params.json -rw-r--r-- 1 172013 3 feb 09:56 tokenizer.model

pip install -r requirements.txt
./download.sh quando dai questo comando ti chiede di inserire il link ricevuto in email. se da problemi controlla gli spazi che non devono esserci nel link oppure trascrivilo manualmente se non parte il download e solo problema del link che quando tu lo copi e poi incolli ti modifica il link altrimenti elimini la email e richiedi un link nuovo ma se richiedi il link nuovo vai nella directory codellama e cancella le versioni dei modelli che hai tentato di scaricare e riprovi con il nuovo link controllando che non ci siano spazi in pratica deve essere tutto blu il link spero di averer spiegato bene io cosi ho risolto buon fine settimana.. con la cli lo puoi usare nel terminale e in pratica uhaoooo 
"
197,Code Llama 13b download failed: no properly formatted checksum lines found,"I ran the bash script from the command console and received the following message and the download was aborted:

Checking checksums
md5sum: checklist.chk: no properly formatted checksum lines found

Any ideas?

",kraso,"I'm very sorry, I had read the instructions very wrong and when I was asked for the email url, I simply indicated my email account instead of the URL.

I have deleted the folder with all the previous configurations and I have downloaded the installation file again from the following link:

[https://github.com/facebookresearch/codellama/archive/refs/heads/main.zip](https://github.com/facebookresearch/codellama/archive/refs/heads/main.zip)

I have unzipped and launched the bash script with the following command:

`bash download.sh`

It asks me for the URL and I enter it. It asks me which model I want to install and, in my case, I enter 13b. Press enter and the download begins. Finally, it seems that the download of the files and their verification has been correct and everything has gone well.

Now I have the installation done but I don't know the steps to follow to be able to interact with the model. I am going to continue reading the instructions and try to follow them to the letter.

I have learned that it is important to read things twice before making rash decisions.

Thank you very much for your time."
196,Incomplete Download of 13B and higher parameters model,"While downloading 13B, the model size is around 12 GB and it is saying that 

consolidated.00.pth -> OK
consolidated.01.pth -> FAILED

I am following all the steps as mentioned but nothing ain't working.

Ideally model size should be around 34 GB but it is coming only 12 GB",allelive,"non riesco a capire il tuo problema. comunque ti indico tutti i passaggi: (ho usato python3.12 e continuo con l'esempio con python3.12 credo che serva almeno python3.10

git clone https://github.com/facebookresearch/codellama.git
cd codellama
python3.12 -m ven env 
source env/bin/activate
pip3.12 install -r requirements.txt
./download.sh
qui ti chiede di inserire il link ricevuto tramite email verifica che nell'url non ci siano spazi deve essere una stringa unica dovresti vederla blu quando e tutta unita, se non funziona cosi, vai nella cartella nella home di codellama e cancelli le cartelle 7b, 13b e altre che vedi che hai provato a scaricare parliamo delle versioni dei modelli... una volta cancellato elimini le email vecchie e richiedi un altro link vedrai che dopo funzionera.. dopo qualche tentativo se non ci si riesce ti aiutano inviandoti il link non corrotto e ripristinato dgli spazi creato quando si copia e incolla problemi di formao penso e dopo ti scaricherà e potrai utilizzarlo "
194,elmah.io,"            - tên: elmah.io Tạo hành động triển khai
  # Bạn có thể ghim vào cam kết chính xác hoặc phiên bản.
  # sử dụng: elmahio/github-create-deployment-action@132611db9161ecebb1b07db6510d4ea5e0d2d415
  sử dụng: elmahio/github-create-deployment-action@v1
  với:
    # Khóa API có quyền tạo triển khai.
    Mã API:
    # Số phiên bản của việc triển khai này. Giá trị của phiên bản có thể là chuỗi tuân thủ SemVer hoặc bất kỳ cú pháp nào khác mà bạn đang sử dụng làm sơ đồ đánh số phiên bản của mình.
    phiên bản:
    # Mô tả tùy chọn về việc triển khai này. Có thể đánh dấu hoặc xóa văn bản.
    Mô tả (không bắt buộc
    # Tên của người chịu trách nhiệm tạo triển khai này. Đây có thể là tên được lấy từ máy chủ triển khai của bạn.
    Tên người dùng: # tùy chọn
    # Email của người chịu trách nhiệm tạo triển khai này. Đây có thể là email được lấy từ máy chủ triển khai của bạn.
    email người dùng: # tùy chọn
    # Theo mặc định, việc triển khai được đính kèm tất cả nhật ký của tổ chức. Nếu bạn muốn triển khai chỉ đính kèm vào một nhật ký duy nhất, hãy đặt mã này thành ID của nhật ký đó.
    logId: # tùy chọn",facebook-github-bot,"Hi @MAYCHUI! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%23194). Thanks!"
194,elmah.io,"            - tên: elmah.io Tạo hành động triển khai
  # Bạn có thể ghim vào cam kết chính xác hoặc phiên bản.
  # sử dụng: elmahio/github-create-deployment-action@132611db9161ecebb1b07db6510d4ea5e0d2d415
  sử dụng: elmahio/github-create-deployment-action@v1
  với:
    # Khóa API có quyền tạo triển khai.
    Mã API:
    # Số phiên bản của việc triển khai này. Giá trị của phiên bản có thể là chuỗi tuân thủ SemVer hoặc bất kỳ cú pháp nào khác mà bạn đang sử dụng làm sơ đồ đánh số phiên bản của mình.
    phiên bản:
    # Mô tả tùy chọn về việc triển khai này. Có thể đánh dấu hoặc xóa văn bản.
    Mô tả (không bắt buộc
    # Tên của người chịu trách nhiệm tạo triển khai này. Đây có thể là tên được lấy từ máy chủ triển khai của bạn.
    Tên người dùng: # tùy chọn
    # Email của người chịu trách nhiệm tạo triển khai này. Đây có thể là email được lấy từ máy chủ triển khai của bạn.
    email người dùng: # tùy chọn
    # Theo mặc định, việc triển khai được đính kèm tất cả nhật ký của tổ chức. Nếu bạn muốn triển khai chỉ đính kèm vào một nhật ký duy nhất, hãy đặt mã này thành ID của nhật ký đó.
    logId: # tùy chọn",jgehring,The PR description is not clear to me and we are not planning to add Github workflows to this repository.
193,Add link to transformers-format repos,,facebook-github-bot,"Hi @osanseviero! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%23193). Thanks!"
193,Add link to transformers-format repos,,facebook-github-bot,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!
192,Checksum failures for 70B Instruct (and Python) ,"I am getting a checksum failures for 70B Instruct and Python:

CodeLlama-70b-Instruct:
Checking checksums
Warning: Hash size is wrong, maybe you want to use 'sha1sum' or 'ripemd160sum'
consolidated.00.pth: FAILED
consolidated.01.pth: OK
consolidated.02.pth: OK
consolidated.03.pth: OK
consolidated.04.pth: OK
consolidated.05.pth: OK
consolidated.06.pth: OK
consolidated.07.pth: OK
params.json: FAILED
tokenizer.model: OK
md5sum: WARNING: 2 of 10 computed checksums did NOT match

CodeLlama-70b-Python:
Checking checksums
consolidated.00.pth: OK
consolidated.01.pth: OK
consolidated.02.pth: OK
consolidated.03.pth: OK
consolidated.04.pth: OK
consolidated.05.pth: OK
consolidated.06.pth: OK
consolidated.07.pth: OK
params.json: FAILED
tokenizer.model: OK
",fourpartswater,The params errors seem to be the same as #188 
192,Checksum failures for 70B Instruct (and Python) ,"I am getting a checksum failures for 70B Instruct and Python:

CodeLlama-70b-Instruct:
Checking checksums
Warning: Hash size is wrong, maybe you want to use 'sha1sum' or 'ripemd160sum'
consolidated.00.pth: FAILED
consolidated.01.pth: OK
consolidated.02.pth: OK
consolidated.03.pth: OK
consolidated.04.pth: OK
consolidated.05.pth: OK
consolidated.06.pth: OK
consolidated.07.pth: OK
params.json: FAILED
tokenizer.model: OK
md5sum: WARNING: 2 of 10 computed checksums did NOT match

CodeLlama-70b-Python:
Checking checksums
consolidated.00.pth: OK
consolidated.01.pth: OK
consolidated.02.pth: OK
consolidated.03.pth: OK
consolidated.04.pth: OK
consolidated.05.pth: OK
consolidated.06.pth: OK
consolidated.07.pth: OK
params.json: FAILED
tokenizer.model: OK
",fourpartswater,I tried downloading CodeLlama-70b-Instruct consolidated.00.pth multiple times with the same error.
192,Checksum failures for 70B Instruct (and Python) ,"I am getting a checksum failures for 70B Instruct and Python:

CodeLlama-70b-Instruct:
Checking checksums
Warning: Hash size is wrong, maybe you want to use 'sha1sum' or 'ripemd160sum'
consolidated.00.pth: FAILED
consolidated.01.pth: OK
consolidated.02.pth: OK
consolidated.03.pth: OK
consolidated.04.pth: OK
consolidated.05.pth: OK
consolidated.06.pth: OK
consolidated.07.pth: OK
params.json: FAILED
tokenizer.model: OK
md5sum: WARNING: 2 of 10 computed checksums did NOT match

CodeLlama-70b-Python:
Checking checksums
consolidated.00.pth: OK
consolidated.01.pth: OK
consolidated.02.pth: OK
consolidated.03.pth: OK
consolidated.04.pth: OK
consolidated.05.pth: OK
consolidated.06.pth: OK
consolidated.07.pth: OK
params.json: FAILED
tokenizer.model: OK
",jgehring,"Hi @fourpartswater, thanks for the notice! Indeed, there was an error in the checksum files for CodeLlama-70b-Python and CodeLlama-70b-Instruct. If you download these models again you should get the updated versions. For reference (and to maybe save you some download time), you can check that each `params.json` file (i.e., for CodeLlama-70b-Python and CodeLlama-70b-Instruct) has an MD5 checksum of `184c6afa048cf53e3f8755904556b2cb`."
191,line 1: payload:allShortcutsEnabled:false: command not found,"when I run the download.sh script in bash I get this error:
line 1: payload:allShortcutsEnabled:false: command not found

How do I fix this?

Thank you.",AKA-Axanar,"BTW, I get the same error in an Ubuntu VM and on Windows."
191,line 1: payload:allShortcutsEnabled:false: command not found,"when I run the download.sh script in bash I get this error:
line 1: payload:allShortcutsEnabled:false: command not found

How do I fix this?

Thank you.",jgehring,"That doesn't sound right, can you check whether your copy of `download.sh` starts with the following lines?

```bash
#!/bin/bash

# Copyright (c) Meta Platforms, Inc. and affiliates.
# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.

read -p ""Enter the URL from email: "" PRESIGNED_URL
```"
191,line 1: payload:allShortcutsEnabled:false: command not found,"when I run the download.sh script in bash I get this error:
line 1: payload:allShortcutsEnabled:false: command not found

How do I fix this?

Thank you.",AKA-Axanar,"It does not.  it starts like this and is all in one long single line:
{""payload"":{""allShortcutsEnabled"":false,""fileTree"":{"""":{""items"":[{""name"":"".circleci"",""path"":"".circleci"",""contentType"":""directory""},{""name"":""llama"",""path"":""llama"",""contentType"":""directory""},

Thank you.
"
191,line 1: payload:allShortcutsEnabled:false: command not found,"when I run the download.sh script in bash I get this error:
line 1: payload:allShortcutsEnabled:false: command not found

How do I fix this?

Thank you.",jgehring,"Hum, not sure what went wrong here, but that's certainly not the correct script. Can you try cloning this whole repository with `git clone https://github.com/facebookresearch/codellama` and then run download.sh from the resulting directory (codellama)?"
191,line 1: payload:allShortcutsEnabled:false: command not found,"when I run the download.sh script in bash I get this error:
line 1: payload:allShortcutsEnabled:false: command not found

How do I fix this?

Thank you.",AKA-Axanar,I downloaded just the sh file from github and it's now downloading fine in an Ubuntu VM.  Thank you for your help.
189,"Annoying, Non-User-Friendly Download Script","Summary: The current script for downloading models, while supposedly secure, leaves much to be desired in terms of user experience. It's a classic case of over-engineering, where the effort to enhance security and versatility results in a cumbersome and less intuitive process.

Problems Identified:

Lack of Clarity: Users are often puzzled about the format for selecting models to download, leading to wasted attempts and frustration.
No Interactive Guidance: The script lacks interactive elements, such as pauses or clear instructions, making it daunting for less experienced users.
Python Environment Checks Missing: The script does not verify the presence of required Python environments beforehand, leading to last-minute errors and confusion.
Inconvenient Post-Download Process: Users are left with the task of manually managing downloaded files, adding unnecessary post-download work.
Proposed Solutions:

Enhance Clarity: Revise the script to include explicit instructions about the format for model selection.
Improve Interactivity: Integrate user prompts and pause functions for a more guided experience.
Environment Validation: Implement checks for necessary Python environments before initiating downloads.
Automate Cleanup: Optionally, add a feature to automatically delete files post-verification, reducing user workload.
Conclusion:
The aim is to transform this digital equivalent of a ""needle in a haystack"" search into a more straightforward, less infuriating experience. Because let's face it, the current script is like a puzzle where you're unsure if the pieces are missing or you just can't see the bigger picture.

pyenv git and md5, verify selected downloads, verify where to download. download. verify. cleanup and complete. 
may as well do it all and make it secure. 



Joke: echo ""Congratulations, you've successfully navigated the labyrinth of our 'user-friendly' download script. Just think of it as a digital scavenger hunt, but instead of treasure, you get model files. Enjoy deciphering them! 🤷‍♂️""
",phamkinhquoc2002,facts
189,"Annoying, Non-User-Friendly Download Script","Summary: The current script for downloading models, while supposedly secure, leaves much to be desired in terms of user experience. It's a classic case of over-engineering, where the effort to enhance security and versatility results in a cumbersome and less intuitive process.

Problems Identified:

Lack of Clarity: Users are often puzzled about the format for selecting models to download, leading to wasted attempts and frustration.
No Interactive Guidance: The script lacks interactive elements, such as pauses or clear instructions, making it daunting for less experienced users.
Python Environment Checks Missing: The script does not verify the presence of required Python environments beforehand, leading to last-minute errors and confusion.
Inconvenient Post-Download Process: Users are left with the task of manually managing downloaded files, adding unnecessary post-download work.
Proposed Solutions:

Enhance Clarity: Revise the script to include explicit instructions about the format for model selection.
Improve Interactivity: Integrate user prompts and pause functions for a more guided experience.
Environment Validation: Implement checks for necessary Python environments before initiating downloads.
Automate Cleanup: Optionally, add a feature to automatically delete files post-verification, reducing user workload.
Conclusion:
The aim is to transform this digital equivalent of a ""needle in a haystack"" search into a more straightforward, less infuriating experience. Because let's face it, the current script is like a puzzle where you're unsure if the pieces are missing or you just can't see the bigger picture.

pyenv git and md5, verify selected downloads, verify where to download. download. verify. cleanup and complete. 
may as well do it all and make it secure. 



Joke: echo ""Congratulations, you've successfully navigated the labyrinth of our 'user-friendly' download script. Just think of it as a digital scavenger hunt, but instead of treasure, you get model files. Enjoy deciphering them! 🤷‍♂️""
",phamkinhquoc2002,"I have troubble downloading as im using windows, and the wsl confusing me"
189,"Annoying, Non-User-Friendly Download Script","Summary: The current script for downloading models, while supposedly secure, leaves much to be desired in terms of user experience. It's a classic case of over-engineering, where the effort to enhance security and versatility results in a cumbersome and less intuitive process.

Problems Identified:

Lack of Clarity: Users are often puzzled about the format for selecting models to download, leading to wasted attempts and frustration.
No Interactive Guidance: The script lacks interactive elements, such as pauses or clear instructions, making it daunting for less experienced users.
Python Environment Checks Missing: The script does not verify the presence of required Python environments beforehand, leading to last-minute errors and confusion.
Inconvenient Post-Download Process: Users are left with the task of manually managing downloaded files, adding unnecessary post-download work.
Proposed Solutions:

Enhance Clarity: Revise the script to include explicit instructions about the format for model selection.
Improve Interactivity: Integrate user prompts and pause functions for a more guided experience.
Environment Validation: Implement checks for necessary Python environments before initiating downloads.
Automate Cleanup: Optionally, add a feature to automatically delete files post-verification, reducing user workload.
Conclusion:
The aim is to transform this digital equivalent of a ""needle in a haystack"" search into a more straightforward, less infuriating experience. Because let's face it, the current script is like a puzzle where you're unsure if the pieces are missing or you just can't see the bigger picture.

pyenv git and md5, verify selected downloads, verify where to download. download. verify. cleanup and complete. 
may as well do it all and make it secure. 



Joke: echo ""Congratulations, you've successfully navigated the labyrinth of our 'user-friendly' download script. Just think of it as a digital scavenger hunt, but instead of treasure, you get model files. Enjoy deciphering them! 🤷‍♂️""
",unphased,"If whatever model files you're after were put on huggingface, you can grab them very easily and reliably by having `git-lfs` installed on your machine and `git clone https://hf.co/<repo path>`."
189,"Annoying, Non-User-Friendly Download Script","Summary: The current script for downloading models, while supposedly secure, leaves much to be desired in terms of user experience. It's a classic case of over-engineering, where the effort to enhance security and versatility results in a cumbersome and less intuitive process.

Problems Identified:

Lack of Clarity: Users are often puzzled about the format for selecting models to download, leading to wasted attempts and frustration.
No Interactive Guidance: The script lacks interactive elements, such as pauses or clear instructions, making it daunting for less experienced users.
Python Environment Checks Missing: The script does not verify the presence of required Python environments beforehand, leading to last-minute errors and confusion.
Inconvenient Post-Download Process: Users are left with the task of manually managing downloaded files, adding unnecessary post-download work.
Proposed Solutions:

Enhance Clarity: Revise the script to include explicit instructions about the format for model selection.
Improve Interactivity: Integrate user prompts and pause functions for a more guided experience.
Environment Validation: Implement checks for necessary Python environments before initiating downloads.
Automate Cleanup: Optionally, add a feature to automatically delete files post-verification, reducing user workload.
Conclusion:
The aim is to transform this digital equivalent of a ""needle in a haystack"" search into a more straightforward, less infuriating experience. Because let's face it, the current script is like a puzzle where you're unsure if the pieces are missing or you just can't see the bigger picture.

pyenv git and md5, verify selected downloads, verify where to download. download. verify. cleanup and complete. 
may as well do it all and make it secure. 



Joke: echo ""Congratulations, you've successfully navigated the labyrinth of our 'user-friendly' download script. Just think of it as a digital scavenger hunt, but instead of treasure, you get model files. Enjoy deciphering them! 🤷‍♂️""
",Tom-Neverwinter,"> If whatever model files you're after were put on huggingface, you can grab them very easily and reliably by having `git-lfs` installed on your machine and `git clone https://hf.co/<repo path>`.

yeah, it would be much easier. agree to the terms and then download however you want even using jdownloader to check integrity and move to desired location. instead we have the ""EA launcher"" of AI"
189,"Annoying, Non-User-Friendly Download Script","Summary: The current script for downloading models, while supposedly secure, leaves much to be desired in terms of user experience. It's a classic case of over-engineering, where the effort to enhance security and versatility results in a cumbersome and less intuitive process.

Problems Identified:

Lack of Clarity: Users are often puzzled about the format for selecting models to download, leading to wasted attempts and frustration.
No Interactive Guidance: The script lacks interactive elements, such as pauses or clear instructions, making it daunting for less experienced users.
Python Environment Checks Missing: The script does not verify the presence of required Python environments beforehand, leading to last-minute errors and confusion.
Inconvenient Post-Download Process: Users are left with the task of manually managing downloaded files, adding unnecessary post-download work.
Proposed Solutions:

Enhance Clarity: Revise the script to include explicit instructions about the format for model selection.
Improve Interactivity: Integrate user prompts and pause functions for a more guided experience.
Environment Validation: Implement checks for necessary Python environments before initiating downloads.
Automate Cleanup: Optionally, add a feature to automatically delete files post-verification, reducing user workload.
Conclusion:
The aim is to transform this digital equivalent of a ""needle in a haystack"" search into a more straightforward, less infuriating experience. Because let's face it, the current script is like a puzzle where you're unsure if the pieces are missing or you just can't see the bigger picture.

pyenv git and md5, verify selected downloads, verify where to download. download. verify. cleanup and complete. 
may as well do it all and make it secure. 



Joke: echo ""Congratulations, you've successfully navigated the labyrinth of our 'user-friendly' download script. Just think of it as a digital scavenger hunt, but instead of treasure, you get model files. Enjoy deciphering them! 🤷‍♂️""
",coloradokb,"I'm very interested in this, however, just like the details in issue 176 (https://github.com/facebookresearch/codellama/issues/176) I am also getting forbidden, bad schema, 416 Requested Range Not Satisfiable. I've gone through the accepting license form page and link(s) I get are the same and in past 4 hours of this post. Oh well, will try again later in the week."
189,"Annoying, Non-User-Friendly Download Script","Summary: The current script for downloading models, while supposedly secure, leaves much to be desired in terms of user experience. It's a classic case of over-engineering, where the effort to enhance security and versatility results in a cumbersome and less intuitive process.

Problems Identified:

Lack of Clarity: Users are often puzzled about the format for selecting models to download, leading to wasted attempts and frustration.
No Interactive Guidance: The script lacks interactive elements, such as pauses or clear instructions, making it daunting for less experienced users.
Python Environment Checks Missing: The script does not verify the presence of required Python environments beforehand, leading to last-minute errors and confusion.
Inconvenient Post-Download Process: Users are left with the task of manually managing downloaded files, adding unnecessary post-download work.
Proposed Solutions:

Enhance Clarity: Revise the script to include explicit instructions about the format for model selection.
Improve Interactivity: Integrate user prompts and pause functions for a more guided experience.
Environment Validation: Implement checks for necessary Python environments before initiating downloads.
Automate Cleanup: Optionally, add a feature to automatically delete files post-verification, reducing user workload.
Conclusion:
The aim is to transform this digital equivalent of a ""needle in a haystack"" search into a more straightforward, less infuriating experience. Because let's face it, the current script is like a puzzle where you're unsure if the pieces are missing or you just can't see the bigger picture.

pyenv git and md5, verify selected downloads, verify where to download. download. verify. cleanup and complete. 
may as well do it all and make it secure. 



Joke: echo ""Congratulations, you've successfully navigated the labyrinth of our 'user-friendly' download script. Just think of it as a digital scavenger hunt, but instead of treasure, you get model files. Enjoy deciphering them! 🤷‍♂️""
",Tom-Neverwinter,"The Bloke seems to have quantized the model already, so no reason to waste time on this anymore. 

https://huggingface.co/TheBloke"
188,Potentially Incorrect Configs,"The new CodeLlama70b Instruct model seem to have incorrect settings for `rope_theta` and `max_position_embeddings`.  

CodeLlama 34b Values:

``` 
""rope_theta"": 1000000
""max_position_embeddings"": 16384
```

CodeLlama 70b Values:

``` 
""rope_theta"": 10000
""max_position_embeddings"": 2048
```

This seems to be the case for the HF safetensors version as well as the version downloaded via `download.sh`.

https://huggingface.co/codellama/CodeLlama-70b-Instruct-hf/blob/main/config.json


Are these config values correct or should we be able to use the 34b values?",jgehring,"Hi @andrewgross, indeed `rope_theta` for 70b-Instruct and 70b-Python is 10,000. `max_positioning_embedding` on HuggingFace will be fixed, the correct value is 4096 -- thanks for letting us know!"
188,Potentially Incorrect Configs,"The new CodeLlama70b Instruct model seem to have incorrect settings for `rope_theta` and `max_position_embeddings`.  

CodeLlama 34b Values:

``` 
""rope_theta"": 1000000
""max_position_embeddings"": 16384
```

CodeLlama 70b Values:

``` 
""rope_theta"": 10000
""max_position_embeddings"": 2048
```

This seems to be the case for the HF safetensors version as well as the version downloaded via `download.sh`.

https://huggingface.co/codellama/CodeLlama-70b-Instruct-hf/blob/main/config.json


Are these config values correct or should we be able to use the 34b values?",andrewgross,"Great, thanks for the info."
188,Potentially Incorrect Configs,"The new CodeLlama70b Instruct model seem to have incorrect settings for `rope_theta` and `max_position_embeddings`.  

CodeLlama 34b Values:

``` 
""rope_theta"": 1000000
""max_position_embeddings"": 16384
```

CodeLlama 70b Values:

``` 
""rope_theta"": 10000
""max_position_embeddings"": 2048
```

This seems to be the case for the HF safetensors version as well as the version downloaded via `download.sh`.

https://huggingface.co/codellama/CodeLlama-70b-Instruct-hf/blob/main/config.json


Are these config values correct or should we be able to use the 34b values?",LavaPlanetLLM,"Sorry for the question @jgehring, but if the correct value is 4096, is this still capable of going up to 16k without a lot of issues, or will it show a lot of degradation past 8k?

Thanks for the models in any case!"
188,Potentially Incorrect Configs,"The new CodeLlama70b Instruct model seem to have incorrect settings for `rope_theta` and `max_position_embeddings`.  

CodeLlama 34b Values:

``` 
""rope_theta"": 1000000
""max_position_embeddings"": 16384
```

CodeLlama 70b Values:

``` 
""rope_theta"": 10000
""max_position_embeddings"": 2048
```

This seems to be the case for the HF safetensors version as well as the version downloaded via `download.sh`.

https://huggingface.co/codellama/CodeLlama-70b-Instruct-hf/blob/main/config.json


Are these config values correct or should we be able to use the 34b values?",michaelroyzen,"Is the `max_position_embeddings` still 16384 for the regular codellama-70b? And I'm curious why it is lower for the Python and Instruct variants @jgehring, would love some clarity."
188,Potentially Incorrect Configs,"The new CodeLlama70b Instruct model seem to have incorrect settings for `rope_theta` and `max_position_embeddings`.  

CodeLlama 34b Values:

``` 
""rope_theta"": 1000000
""max_position_embeddings"": 16384
```

CodeLlama 70b Values:

``` 
""rope_theta"": 10000
""max_position_embeddings"": 2048
```

This seems to be the case for the HF safetensors version as well as the version downloaded via `download.sh`.

https://huggingface.co/codellama/CodeLlama-70b-Instruct-hf/blob/main/config.json


Are these config values correct or should we be able to use the 34b values?",syhw,It's 16k only for the base (pretrained) Code Llama 70B @michaelroyzen 
188,Potentially Incorrect Configs,"The new CodeLlama70b Instruct model seem to have incorrect settings for `rope_theta` and `max_position_embeddings`.  

CodeLlama 34b Values:

``` 
""rope_theta"": 1000000
""max_position_embeddings"": 16384
```

CodeLlama 70b Values:

``` 
""rope_theta"": 10000
""max_position_embeddings"": 2048
```

This seems to be the case for the HF safetensors version as well as the version downloaded via `download.sh`.

https://huggingface.co/codellama/CodeLlama-70b-Instruct-hf/blob/main/config.json


Are these config values correct or should we be able to use the 34b values?",nazarov-yuriy,"But why validation of params.json in CodeLlama-70b-Python with ""rope_theta"": 10000 fails
```
Checking checksums
consolidated.00.pth: OK
consolidated.01.pth: OK
consolidated.02.pth: OK
consolidated.03.pth: OK
consolidated.04.pth: OK
consolidated.05.pth: OK
consolidated.06.pth: OK
consolidated.07.pth: OK
params.json: FAILED
tokenizer.model: OK
md5sum: WARNING: 1 computed checksum did NOT match
```
And CodeLlama-70b with ""rope_theta"": 1000000 passes
```
Checking checksums
consolidated.00.pth: OK
consolidated.01.pth: OK
consolidated.02.pth: OK
consolidated.03.pth: OK
consolidated.04.pth: OK
consolidated.05.pth: OK
consolidated.06.pth: OK
consolidated.07.pth: OK
params.json: OK
tokenizer.model: OK
```
?"
188,Potentially Incorrect Configs,"The new CodeLlama70b Instruct model seem to have incorrect settings for `rope_theta` and `max_position_embeddings`.  

CodeLlama 34b Values:

``` 
""rope_theta"": 1000000
""max_position_embeddings"": 16384
```

CodeLlama 70b Values:

``` 
""rope_theta"": 10000
""max_position_embeddings"": 2048
```

This seems to be the case for the HF safetensors version as well as the version downloaded via `download.sh`.

https://huggingface.co/codellama/CodeLlama-70b-Instruct-hf/blob/main/config.json


Are these config values correct or should we be able to use the 34b values?",nazarov-yuriy,"@jgehring could you share the file expected by checklist.chk(with md5 equal to 2e6de9333f10527d5976d32d8bcddd05)
```
$ grep params.json CodeLlama-70b*/checklist.chk
CodeLlama-70b-Instruct/checklist.chk:2e6de9333f10527d5976d32d8bcddd05  params.json
CodeLlama-70b-Python/checklist.chk:2e6de9333f10527d5976d32d8bcddd05  params.json
CodeLlama-70b/checklist.chk:a4d42626f8b801baf33c4e9a27fc52e7  params.json
```
?"
188,Potentially Incorrect Configs,"The new CodeLlama70b Instruct model seem to have incorrect settings for `rope_theta` and `max_position_embeddings`.  

CodeLlama 34b Values:

``` 
""rope_theta"": 1000000
""max_position_embeddings"": 16384
```

CodeLlama 70b Values:

``` 
""rope_theta"": 10000
""max_position_embeddings"": 2048
```

This seems to be the case for the HF safetensors version as well as the version downloaded via `download.sh`.

https://huggingface.co/codellama/CodeLlama-70b-Instruct-hf/blob/main/config.json


Are these config values correct or should we be able to use the 34b values?",jgehring,"Good catch @nazarov-yuriy, this is an error in the `checklist.chk` file which we initially provided. This has now been fixed and new downloads will retrieve the corrected checksum file. If you want to verify the `params.json` you have on disk for CodeLlama-70b-Instruct and CodeLlama-70b-Python, its MD5 checksum is `184c6afa048cf53e3f8755904556b2cb`."
186,Where are the docs for the `llama` API?,"I have difficulty finding docs for the `llama` API. Like, where is the meaning of the parameters defined (some like temperature and top_p I can guess but others...), how is `generator` to be used etc.",jgehring,"Hi @antonkratz, please take a look at example_completion.py, example_infilling.py, and example_instructions.py for some usage examples. Are there specific parameters you're wondering about?"
186,Where are the docs for the `llama` API?,"I have difficulty finding docs for the `llama` API. Like, where is the meaning of the parameters defined (some like temperature and top_p I can guess but others...), how is `generator` to be used etc.",antonkratz,"I want to use CodeLlama-7b-Instruct interactively, i.e. I want to be able to have a back-and-forth style conversation about the generated code.

I already managed to install Code Llama and run it on my own infrastructure, i.e. I can run:
`torchrun --nproc_per_node 1 example_instructions.py     --ckpt_dir CodeLlama-7b-Instruct/     --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4`

But how to get from here to an interaction? Does each call to `generator.chat_completion` ""reset"" the state? Or how can I write my script it so that the next prompt ""stays in the same conversation""? My terminology is maybe a bit off here.

A related issue is that for this, it is critical to be able to work interactively, but I do not know how to achieve that because I apparently must run stuff via `torchrun`?!

P.S.: Also, how can I query `generator` such that I get the tokens back one after the other, for interactive display, i.e. I want to display while it is generating.

P.P.S.: @jgehring I just realize you are one of the authors of the Code Llama paper, congratulations to this wonderful work!"
186,Where are the docs for the `llama` API?,"I have difficulty finding docs for the `llama` API. Like, where is the meaning of the parameters defined (some like temperature and top_p I can guess but others...), how is `generator` to be used etc.",jgehring,"For including CodeLlama in real applications I would recommend building on top of other open-source inference engines. The repo here serves as a reference implementation, whereas other projects such as [transformers](https://huggingface.co/docs/transformers/main/model_doc/code_llama) or [ollama](https://ollama.ai/) provide a better offering in terms of bells and whistles and/or inference speed. I suggest you check out a few inference engines for Llama models; I'm sure you'll find something that fits your requirements. 

For completeness, let me briefly answer your concrete questions, thought:
- If you want to perform an ongoing dialog, you'd have to keep track of individual turns and provide `chat_completion()` with the full history at each call. 
- torchrun doesn't prevent you from interacting with stdin/stdout; it's a thin wrapper that helps setting up the torch distributed machinery. I think for the 7B model you could also get away without torchrun, but for the bigger models you'll need multiple GPUs (and hence processes), and writing interactive applications gets a bit unwieldy.
- Streaming generation is not implemented in this repo, but [here's the loop](https://github.com/facebookresearch/codellama/blob/e66609cfbd73503ef25e597fd82c59084836155d/llama/generation.py#L163) that could be repurposed to yield individual tokens."
185,"RuntimeError: ""addmm_impl_cpu_"" not implemented for 'Half'","**I am using wsl 2 with ubuntu-22.04, this is the gpu  information**
![image](https://github.com/facebookresearch/codellama/assets/1501241/fd964023-2df8-43ae-8d7e-96f7fc359f7d)

**when i run ""sudo lshw -C display""**
![image](https://github.com/facebookresearch/codellama/assets/1501241/a244d7c9-4f5c-4dc4-8b24-403efa7644be)

**I install torch using this command**
pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url 
 https://download.pytorch.org/whl/cu113

**when i run command** 
""torchrun --nproc_per_node 1 example_instructions.py \
    --ckpt_dir CodeLlama-7b-Instruct/ \
    --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \
    --max_seq_len 512 --max_batch_size 4"", 

it has **error**: RuntimeError: ""addmm_impl_cpu_"" not implemented for 'Half'.

**These are full log:**
```

:/mnt/c/Users/john.john/codelama/weight/codellama-main$ torchrun --nproc_per_node 1 example_instructions.py \
kpt_dir>     --ckpt_dir CodeLlama-7b-Instruct/ \
>     --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \
>     --max_seq_len 512 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Loaded in 38.10 seconds
Traceback (most recent call last):
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/example_instructions.py"", line 68, in <module>
    fire.Fire(main)
  File ""/home/john/.local/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/john/.local/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/john/.local/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/example_instructions.py"", line 51, in main
    results = generator.chat_completion(
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/generation.py"", line 351, in chat_completion
    generation_tokens, generation_logprobs = self.generate(
  File ""/home/john/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py"", line 27, in decorate_context
    return func(*args, **kwargs)
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/generation.py"", line 164, in generate
    logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)
  File ""/home/john/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py"", line 27, in decorate_context
    return func(*args, **kwargs)
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/model.py"", line 300, in forward
    h = layer(h, start_pos, freqs_cis, (mask.to(device) if mask is not None else mask))
  File ""/home/john/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/model.py"", line 252, in forward
    h = x + self.attention.forward(
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/model.py"", line 165, in forward
    xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
  File ""/home/john/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/john/.local/lib/python3.10/site-packages/fairscale/nn/model_parallel/layers.py"", line 290, in forward
    output_parallel = F.linear(input_parallel, self.weight, self.bias)
RuntimeError: ""addmm_impl_cpu_"" not implemented for 'Half'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 433) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/home/john/.local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper
    return f(*args, **kwargs)
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/run.py"", line 761, in main
    run(args)
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/run.py"", line 752, in run
    elastic_launch(
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:

example_instructions.py FAILED

Failures:
  <NO_OTHER_FAILURES>

Root Cause (first observed failure):
[0]:
  time      : 2024-01-24_16:51:25
  host      : company
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 433)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html

```",akashdhruv,"I am getting the same error, but in CPU mode. It looks like your model is running on CPU as well."
185,"RuntimeError: ""addmm_impl_cpu_"" not implemented for 'Half'","**I am using wsl 2 with ubuntu-22.04, this is the gpu  information**
![image](https://github.com/facebookresearch/codellama/assets/1501241/fd964023-2df8-43ae-8d7e-96f7fc359f7d)

**when i run ""sudo lshw -C display""**
![image](https://github.com/facebookresearch/codellama/assets/1501241/a244d7c9-4f5c-4dc4-8b24-403efa7644be)

**I install torch using this command**
pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url 
 https://download.pytorch.org/whl/cu113

**when i run command** 
""torchrun --nproc_per_node 1 example_instructions.py \
    --ckpt_dir CodeLlama-7b-Instruct/ \
    --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \
    --max_seq_len 512 --max_batch_size 4"", 

it has **error**: RuntimeError: ""addmm_impl_cpu_"" not implemented for 'Half'.

**These are full log:**
```

:/mnt/c/Users/john.john/codelama/weight/codellama-main$ torchrun --nproc_per_node 1 example_instructions.py \
kpt_dir>     --ckpt_dir CodeLlama-7b-Instruct/ \
>     --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \
>     --max_seq_len 512 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Loaded in 38.10 seconds
Traceback (most recent call last):
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/example_instructions.py"", line 68, in <module>
    fire.Fire(main)
  File ""/home/john/.local/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/john/.local/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/john/.local/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/example_instructions.py"", line 51, in main
    results = generator.chat_completion(
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/generation.py"", line 351, in chat_completion
    generation_tokens, generation_logprobs = self.generate(
  File ""/home/john/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py"", line 27, in decorate_context
    return func(*args, **kwargs)
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/generation.py"", line 164, in generate
    logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)
  File ""/home/john/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py"", line 27, in decorate_context
    return func(*args, **kwargs)
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/model.py"", line 300, in forward
    h = layer(h, start_pos, freqs_cis, (mask.to(device) if mask is not None else mask))
  File ""/home/john/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/model.py"", line 252, in forward
    h = x + self.attention.forward(
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/model.py"", line 165, in forward
    xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
  File ""/home/john/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/john/.local/lib/python3.10/site-packages/fairscale/nn/model_parallel/layers.py"", line 290, in forward
    output_parallel = F.linear(input_parallel, self.weight, self.bias)
RuntimeError: ""addmm_impl_cpu_"" not implemented for 'Half'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 433) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/home/john/.local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper
    return f(*args, **kwargs)
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/run.py"", line 761, in main
    run(args)
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/run.py"", line 752, in run
    elastic_launch(
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:

example_instructions.py FAILED

Failures:
  <NO_OTHER_FAILURES>

Root Cause (first observed failure):
[0]:
  time      : 2024-01-24_16:51:25
  host      : company
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 433)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html

```",fhlkm,"hi @akashdhruv ,
My pc has nvidia GPU, please check above screenshot.

It is suppose to run on GPU. do you know why it only runs on CPU?"
185,"RuntimeError: ""addmm_impl_cpu_"" not implemented for 'Half'","**I am using wsl 2 with ubuntu-22.04, this is the gpu  information**
![image](https://github.com/facebookresearch/codellama/assets/1501241/fd964023-2df8-43ae-8d7e-96f7fc359f7d)

**when i run ""sudo lshw -C display""**
![image](https://github.com/facebookresearch/codellama/assets/1501241/a244d7c9-4f5c-4dc4-8b24-403efa7644be)

**I install torch using this command**
pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url 
 https://download.pytorch.org/whl/cu113

**when i run command** 
""torchrun --nproc_per_node 1 example_instructions.py \
    --ckpt_dir CodeLlama-7b-Instruct/ \
    --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \
    --max_seq_len 512 --max_batch_size 4"", 

it has **error**: RuntimeError: ""addmm_impl_cpu_"" not implemented for 'Half'.

**These are full log:**
```

:/mnt/c/Users/john.john/codelama/weight/codellama-main$ torchrun --nproc_per_node 1 example_instructions.py \
kpt_dir>     --ckpt_dir CodeLlama-7b-Instruct/ \
>     --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \
>     --max_seq_len 512 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Loaded in 38.10 seconds
Traceback (most recent call last):
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/example_instructions.py"", line 68, in <module>
    fire.Fire(main)
  File ""/home/john/.local/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/john/.local/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/john/.local/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/example_instructions.py"", line 51, in main
    results = generator.chat_completion(
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/generation.py"", line 351, in chat_completion
    generation_tokens, generation_logprobs = self.generate(
  File ""/home/john/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py"", line 27, in decorate_context
    return func(*args, **kwargs)
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/generation.py"", line 164, in generate
    logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)
  File ""/home/john/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py"", line 27, in decorate_context
    return func(*args, **kwargs)
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/model.py"", line 300, in forward
    h = layer(h, start_pos, freqs_cis, (mask.to(device) if mask is not None else mask))
  File ""/home/john/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/model.py"", line 252, in forward
    h = x + self.attention.forward(
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/model.py"", line 165, in forward
    xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
  File ""/home/john/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/john/.local/lib/python3.10/site-packages/fairscale/nn/model_parallel/layers.py"", line 290, in forward
    output_parallel = F.linear(input_parallel, self.weight, self.bias)
RuntimeError: ""addmm_impl_cpu_"" not implemented for 'Half'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 433) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/home/john/.local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper
    return f(*args, **kwargs)
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/run.py"", line 761, in main
    run(args)
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/run.py"", line 752, in run
    elastic_launch(
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:

example_instructions.py FAILED

Failures:
  <NO_OTHER_FAILURES>

Root Cause (first observed failure):
[0]:
  time      : 2024-01-24_16:51:25
  host      : company
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 433)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html

```",akashdhruv,"> hi @akashdhruv , My pc has nvidia GPU, please check above screenshot.
> 
> It is suppose to run on GPU. do you know why it only runs on CPU?

I think you need to look into your system and torchrun configuration to figure out why GPU is not being identified. Is your PyTorch installed with GPU support? If yes maybe try,

`export CUDA_VISIBLE_DEVICES=""0""` "
184,Fixed and enhanced!,,facebook-github-bot,"Hi @likhonsible! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%23184). Thanks!"
184,Fixed and enhanced!,,facebook-github-bot,"Hi @likhonsible! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20meta-llama%2Fcodellama%20%23184). Thanks!"
184,Fixed and enhanced!,,facebook-github-bot,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!
184,Fixed and enhanced!,,jgehring,"Sorry, we will leave the README as-is."
183,"Meta AI response ""bug"" ","Any time I ask certain questions like ""what is your favorite color"" the AI will respond with ""ah! Ah! Ah! Ah!""
But like 50 ""ah!""s
![Screenshot_20240117-212142~2](https://github.com/facebookresearch/codellama/assets/156983217/13c2aa7d-17b8-4fb3-821e-b464018e7f39)
",jgehring,"Sorry, this is outside of the scope of this project."
182,Knowledge Cutoff Date?,"We are aware that the knowledge cutoff date for Llama-2 series models are Sep 2022 according to their model card.

What's the knowledge cutoff date for CodeLlama? Neither the model card and the paper mention anything about this.",justine-gehring,"On their paper it says:

> All experiments reported here and the released models have been trained and fine-tuned using the same data as
Llama 2 (Touvron et al., 2023b) with different weights (see Section 2 and Table 1). Code Llama - Instruct
uses additional instruction fine-tuning data.

So I'm assuming September 2022?"
182,Knowledge Cutoff Date?,"We are aware that the knowledge cutoff date for Llama-2 series models are Sep 2022 according to their model card.

What's the knowledge cutoff date for CodeLlama? Neither the model card and the paper mention anything about this.",liyucheng09,"The cutoff date is crucial for programing tasks.
We would have no idea whether CodeLlama handles torch2.0, unless we know its cutoff."
180,Update download.sh,,facebook-github-bot,"Hi @Lovertilee! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%23180). Thanks!"
177,I am curious about the form of the infilling dataset for training.,"Hello, I am studying machine learning and I have a question.

I'm not good at English, so I'm sorry for asking questions through a translator.

I am training codellama using the infilling dataset I created and SFTTrainer, and I am curious whether the infilling dataset is in the correct shape.

The current json data set format is

{""context"" : ""\<PRE\>{prefix}\<SUF\>{suffix}\<MID\>{answer}\<EOT\>""}

It is composed of a shape.

Also, what I'm curious about is whether the loss output when trained with the dataset is correct.

When using a general Trainer, I understand that loss is calculated by comparing the correct answer inferred by the model using the input labels.

Then, the dataset I am currently training with SFTrainer does not have labels.
There is only one ""context"", so I'm curious about what part of the loss is calculated from.

I would really appreciate it if you could leave any reply.",ParksangwookA,"If the format of the data set mentioned above is incorrect,

{""input_ids"" : ""\<PRE\>{prefix}\<SUF\>{suffix}\<MID\>"",
 ""label"" : ""{answer}""
}

Is this the correct form?

thank you"
174,I'd like to know if this is the right type of dataset for the model using the infilling function.,"Thanks for your excellent work.
I want to make a model that uses the infilling function, but the type of dataset I use right now is  
[PRE]{prefix} [SUF]{suffix}[MID]{example['answer']} [EOT]
It is in form and based on [FILL], the front prefix and the back of the prefix were divided by suffix. When [FILL]  << the data to be entered here is {example['answer']}, it is taught by attaching it after [MID>] is it the correct method to learn with this type of dataset?",gunwooleee,I'm curious too.
175,Questions about learning more than 4096 sequences in codellama,"hello

I had a question and wanted to get some ideas, so I registered an issue.

Now, I'm sorry that I don't have much knowledge as it's been a while since I studied LLM.

I don't speak English, so I wrote it through a translator, so the words may sound strange.

Currently, I try to fine-tuning infilling capability from one node to four GPUs of the codellama 7b or 13b models using deepspeed, but if the sequence length of the dataset to be trained exceeds 4096 supported by codellama, whether it is not suitable for learning infilling, or is there any other way or good ideas to learn
I want to know.

thank you",jgehring,"Hi @ParkRio, longer sequences can be used for fine-tuning with or without infilling. Code Llama supports sequences of up to 100k tokens. If you’re able to do multiple fine-tuning runs it may be a good idea to test for multiple lengths and pick the one that works best for you. It’s a tradeoff between resources required for inference (longer context requires more memory and takes longer), the ability to recall information in the context (Code Llama as-is works very well up to 16k tokens but fine-tuning could further expand on that), and what your application requires."
175,Questions about learning more than 4096 sequences in codellama,"hello

I had a question and wanted to get some ideas, so I registered an issue.

Now, I'm sorry that I don't have much knowledge as it's been a while since I studied LLM.

I don't speak English, so I wrote it through a translator, so the words may sound strange.

Currently, I try to fine-tuning infilling capability from one node to four GPUs of the codellama 7b or 13b models using deepspeed, but if the sequence length of the dataset to be trained exceeds 4096 supported by codellama, whether it is not suitable for learning infilling, or is there any other way or good ideas to learn
I want to know.

thank you",ParkRio,"@jgehring 
Thank you very much for your kind explanation!!"
172,Single-line Infilling Results reproduction,"Hello, 

I am trying to reproduce the infilling results on HumanEval (Table 14 CodeLLAMA 7B SPM, pass@1=83%). I am using the single-line benchmark from https://github.com/openai/human-eval-infilling. I use the below code to generate the samples.

```
from human_eval_infilling.data import write_jsonl, read_problems
from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM,
    CodeLlamaTokenizer,
)
import torch


model_name = ""codellama/CodeLlama-7b-hf""
load_in_8bit = ""False""
device_map = ""auto""
max_gen_len = 128
problems = read_problems(benchmark_name=""single-line"")
model = AutoModelForCausalLM.from_pretrained(
        model_name,
        load_in_8bit=load_in_8bit,
        device_map=device_map,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16
    )
tokenizer = CodeLlamaTokenizer.from_pretrained(model_name, trust_remote_code=True, suffix_first=True)

def generate_one_completion(pre,suf):
    prompt = pre+""<FILL_ME>""+suf
    input_ids = tokenizer(prompt, suffix_first=True, return_tensors=""pt"")[""input_ids""].to('cuda')
    generation_tokens = model.generate(
            input_ids,
            max_new_tokens=max_gen_len,
            temperature=0.2
        )
    outputs = tokenizer.batch_decode(generation_tokens[:, input_ids.shape[1]:], skip_special_tokens = True)[0]
    return outputs

num_samples_per_task = 1
samples = [
    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][""prompt""], problems[task_id][""suffix""]))
    for task_id in tqdm(problems)
    for _ in range(num_samples_per_task)
]
print(len(samples))
write_jsonl(""samples_base_pretrained_codellama.jsonl"", samples)
```

Next I run the following for computing pass@1. I obtain pass@1= 0.73281 which is much smaller than the reported results.

`
evaluate_infilling_functional_correctness samples_base_pretrained_codellama.jsonl --benchmark_name=single-line
`
 
Can you please help with the following:
- Are the benchmarks and prompts for evaluation correct? 
- Is there any post-processing required on the generated codes? (e.g. code sanitation)
- Are there any hyperparameter recommended? (e.g. temperature, decoding strategy?)


",timxx,"I use the [instruct model](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf) , only got `{'pass@1': 0.05227492739593417}` :(

But I use the raw `<PRE>` `<SUF>` `<MID>` tokens, as my test it works fine than `<FILL_ME>`"
172,Single-line Infilling Results reproduction,"Hello, 

I am trying to reproduce the infilling results on HumanEval (Table 14 CodeLLAMA 7B SPM, pass@1=83%). I am using the single-line benchmark from https://github.com/openai/human-eval-infilling. I use the below code to generate the samples.

```
from human_eval_infilling.data import write_jsonl, read_problems
from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM,
    CodeLlamaTokenizer,
)
import torch


model_name = ""codellama/CodeLlama-7b-hf""
load_in_8bit = ""False""
device_map = ""auto""
max_gen_len = 128
problems = read_problems(benchmark_name=""single-line"")
model = AutoModelForCausalLM.from_pretrained(
        model_name,
        load_in_8bit=load_in_8bit,
        device_map=device_map,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16
    )
tokenizer = CodeLlamaTokenizer.from_pretrained(model_name, trust_remote_code=True, suffix_first=True)

def generate_one_completion(pre,suf):
    prompt = pre+""<FILL_ME>""+suf
    input_ids = tokenizer(prompt, suffix_first=True, return_tensors=""pt"")[""input_ids""].to('cuda')
    generation_tokens = model.generate(
            input_ids,
            max_new_tokens=max_gen_len,
            temperature=0.2
        )
    outputs = tokenizer.batch_decode(generation_tokens[:, input_ids.shape[1]:], skip_special_tokens = True)[0]
    return outputs

num_samples_per_task = 1
samples = [
    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][""prompt""], problems[task_id][""suffix""]))
    for task_id in tqdm(problems)
    for _ in range(num_samples_per_task)
]
print(len(samples))
write_jsonl(""samples_base_pretrained_codellama.jsonl"", samples)
```

Next I run the following for computing pass@1. I obtain pass@1= 0.73281 which is much smaller than the reported results.

`
evaluate_infilling_functional_correctness samples_base_pretrained_codellama.jsonl --benchmark_name=single-line
`
 
Can you please help with the following:
- Are the benchmarks and prompts for evaluation correct? 
- Is there any post-processing required on the generated codes? (e.g. code sanitation)
- Are there any hyperparameter recommended? (e.g. temperature, decoding strategy?)


",stgzr,"Same issues. Cannot reproduce the infilling results as paper reported, a bit lower. Any ideas?"
172,Single-line Infilling Results reproduction,"Hello, 

I am trying to reproduce the infilling results on HumanEval (Table 14 CodeLLAMA 7B SPM, pass@1=83%). I am using the single-line benchmark from https://github.com/openai/human-eval-infilling. I use the below code to generate the samples.

```
from human_eval_infilling.data import write_jsonl, read_problems
from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM,
    CodeLlamaTokenizer,
)
import torch


model_name = ""codellama/CodeLlama-7b-hf""
load_in_8bit = ""False""
device_map = ""auto""
max_gen_len = 128
problems = read_problems(benchmark_name=""single-line"")
model = AutoModelForCausalLM.from_pretrained(
        model_name,
        load_in_8bit=load_in_8bit,
        device_map=device_map,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16
    )
tokenizer = CodeLlamaTokenizer.from_pretrained(model_name, trust_remote_code=True, suffix_first=True)

def generate_one_completion(pre,suf):
    prompt = pre+""<FILL_ME>""+suf
    input_ids = tokenizer(prompt, suffix_first=True, return_tensors=""pt"")[""input_ids""].to('cuda')
    generation_tokens = model.generate(
            input_ids,
            max_new_tokens=max_gen_len,
            temperature=0.2
        )
    outputs = tokenizer.batch_decode(generation_tokens[:, input_ids.shape[1]:], skip_special_tokens = True)[0]
    return outputs

num_samples_per_task = 1
samples = [
    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][""prompt""], problems[task_id][""suffix""]))
    for task_id in tqdm(problems)
    for _ in range(num_samples_per_task)
]
print(len(samples))
write_jsonl(""samples_base_pretrained_codellama.jsonl"", samples)
```

Next I run the following for computing pass@1. I obtain pass@1= 0.73281 which is much smaller than the reported results.

`
evaluate_infilling_functional_correctness samples_base_pretrained_codellama.jsonl --benchmark_name=single-line
`
 
Can you please help with the following:
- Are the benchmarks and prompts for evaluation correct? 
- Is there any post-processing required on the generated codes? (e.g. code sanitation)
- Are there any hyperparameter recommended? (e.g. temperature, decoding strategy?)


",faabian,"Dear @shivamag125 , @timxx  and @stgzr, thanks for reporting!

@timxx : The instruction models are not intended to be used for infilling, please use the pretrained models.

@shivamag125 and @stgzr : The hyperparameters (greedy decoding i.e. temperature=0) are reported in the paper (Table 14). Note that you need to compare to the models with LCFT in the table since pretrained models without LCFT have not been released. Moreover, a frequent problem for infilling models is knowing where to stop. Our code cuts the generation after the first linebreak in the single line infilling task."
172,Single-line Infilling Results reproduction,"Hello, 

I am trying to reproduce the infilling results on HumanEval (Table 14 CodeLLAMA 7B SPM, pass@1=83%). I am using the single-line benchmark from https://github.com/openai/human-eval-infilling. I use the below code to generate the samples.

```
from human_eval_infilling.data import write_jsonl, read_problems
from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM,
    CodeLlamaTokenizer,
)
import torch


model_name = ""codellama/CodeLlama-7b-hf""
load_in_8bit = ""False""
device_map = ""auto""
max_gen_len = 128
problems = read_problems(benchmark_name=""single-line"")
model = AutoModelForCausalLM.from_pretrained(
        model_name,
        load_in_8bit=load_in_8bit,
        device_map=device_map,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16
    )
tokenizer = CodeLlamaTokenizer.from_pretrained(model_name, trust_remote_code=True, suffix_first=True)

def generate_one_completion(pre,suf):
    prompt = pre+""<FILL_ME>""+suf
    input_ids = tokenizer(prompt, suffix_first=True, return_tensors=""pt"")[""input_ids""].to('cuda')
    generation_tokens = model.generate(
            input_ids,
            max_new_tokens=max_gen_len,
            temperature=0.2
        )
    outputs = tokenizer.batch_decode(generation_tokens[:, input_ids.shape[1]:], skip_special_tokens = True)[0]
    return outputs

num_samples_per_task = 1
samples = [
    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][""prompt""], problems[task_id][""suffix""]))
    for task_id in tqdm(problems)
    for _ in range(num_samples_per_task)
]
print(len(samples))
write_jsonl(""samples_base_pretrained_codellama.jsonl"", samples)
```

Next I run the following for computing pass@1. I obtain pass@1= 0.73281 which is much smaller than the reported results.

`
evaluate_infilling_functional_correctness samples_base_pretrained_codellama.jsonl --benchmark_name=single-line
`
 
Can you please help with the following:
- Are the benchmarks and prompts for evaluation correct? 
- Is there any post-processing required on the generated codes? (e.g. code sanitation)
- Are there any hyperparameter recommended? (e.g. temperature, decoding strategy?)


",stgzr,"> Dear @shivamag125 , @timxx and @stgzr, thanks for reporting!
> 
> @timxx : The instruction models are not intended to be used for infilling, please use the pretrained models.
> 
> @shivamag125 and @stgzr : The hyperparameters (greedy decoding i.e. temperature=0) are reported in the paper (Table 14). Note that you need to compare to the models with LCFT in the table since pretrained models without LCFT have not been released. Moreover, a frequent problem for infilling models is knowing where to stop. Our code cuts the generation after the first linebreak in the single line infilling task.

Thank you for the detailed reply. I will try to check my implementation.
Another question: when to stop generation in the multi-line and random-span tasks, using \<EOT\>? "
172,Single-line Infilling Results reproduction,"Hello, 

I am trying to reproduce the infilling results on HumanEval (Table 14 CodeLLAMA 7B SPM, pass@1=83%). I am using the single-line benchmark from https://github.com/openai/human-eval-infilling. I use the below code to generate the samples.

```
from human_eval_infilling.data import write_jsonl, read_problems
from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM,
    CodeLlamaTokenizer,
)
import torch


model_name = ""codellama/CodeLlama-7b-hf""
load_in_8bit = ""False""
device_map = ""auto""
max_gen_len = 128
problems = read_problems(benchmark_name=""single-line"")
model = AutoModelForCausalLM.from_pretrained(
        model_name,
        load_in_8bit=load_in_8bit,
        device_map=device_map,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16
    )
tokenizer = CodeLlamaTokenizer.from_pretrained(model_name, trust_remote_code=True, suffix_first=True)

def generate_one_completion(pre,suf):
    prompt = pre+""<FILL_ME>""+suf
    input_ids = tokenizer(prompt, suffix_first=True, return_tensors=""pt"")[""input_ids""].to('cuda')
    generation_tokens = model.generate(
            input_ids,
            max_new_tokens=max_gen_len,
            temperature=0.2
        )
    outputs = tokenizer.batch_decode(generation_tokens[:, input_ids.shape[1]:], skip_special_tokens = True)[0]
    return outputs

num_samples_per_task = 1
samples = [
    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][""prompt""], problems[task_id][""suffix""]))
    for task_id in tqdm(problems)
    for _ in range(num_samples_per_task)
]
print(len(samples))
write_jsonl(""samples_base_pretrained_codellama.jsonl"", samples)
```

Next I run the following for computing pass@1. I obtain pass@1= 0.73281 which is much smaller than the reported results.

`
evaluate_infilling_functional_correctness samples_base_pretrained_codellama.jsonl --benchmark_name=single-line
`
 
Can you please help with the following:
- Are the benchmarks and prompts for evaluation correct? 
- Is there any post-processing required on the generated codes? (e.g. code sanitation)
- Are there any hyperparameter recommended? (e.g. temperature, decoding strategy?)


",shivamag125,Thanks! Using a stopping condition like \n reproduces the numbers. 
172,Single-line Infilling Results reproduction,"Hello, 

I am trying to reproduce the infilling results on HumanEval (Table 14 CodeLLAMA 7B SPM, pass@1=83%). I am using the single-line benchmark from https://github.com/openai/human-eval-infilling. I use the below code to generate the samples.

```
from human_eval_infilling.data import write_jsonl, read_problems
from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM,
    CodeLlamaTokenizer,
)
import torch


model_name = ""codellama/CodeLlama-7b-hf""
load_in_8bit = ""False""
device_map = ""auto""
max_gen_len = 128
problems = read_problems(benchmark_name=""single-line"")
model = AutoModelForCausalLM.from_pretrained(
        model_name,
        load_in_8bit=load_in_8bit,
        device_map=device_map,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16
    )
tokenizer = CodeLlamaTokenizer.from_pretrained(model_name, trust_remote_code=True, suffix_first=True)

def generate_one_completion(pre,suf):
    prompt = pre+""<FILL_ME>""+suf
    input_ids = tokenizer(prompt, suffix_first=True, return_tensors=""pt"")[""input_ids""].to('cuda')
    generation_tokens = model.generate(
            input_ids,
            max_new_tokens=max_gen_len,
            temperature=0.2
        )
    outputs = tokenizer.batch_decode(generation_tokens[:, input_ids.shape[1]:], skip_special_tokens = True)[0]
    return outputs

num_samples_per_task = 1
samples = [
    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][""prompt""], problems[task_id][""suffix""]))
    for task_id in tqdm(problems)
    for _ in range(num_samples_per_task)
]
print(len(samples))
write_jsonl(""samples_base_pretrained_codellama.jsonl"", samples)
```

Next I run the following for computing pass@1. I obtain pass@1= 0.73281 which is much smaller than the reported results.

`
evaluate_infilling_functional_correctness samples_base_pretrained_codellama.jsonl --benchmark_name=single-line
`
 
Can you please help with the following:
- Are the benchmarks and prompts for evaluation correct? 
- Is there any post-processing required on the generated codes? (e.g. code sanitation)
- Are there any hyperparameter recommended? (e.g. temperature, decoding strategy?)


",faabian,"> > Dear @shivamag125 , @timxx and @stgzr, thanks for reporting!
> > @timxx : The instruction models are not intended to be used for infilling, please use the pretrained models.
> > @shivamag125 and @stgzr : The hyperparameters (greedy decoding i.e. temperature=0) are reported in the paper (Table 14). Note that you need to compare to the models with LCFT in the table since pretrained models without LCFT have not been released. Moreover, a frequent problem for infilling models is knowing where to stop. Our code cuts the generation after the first linebreak in the single line infilling task.
> 
> Thank you for the detailed reply. I will try to check my implementation. Another question: when to stop generation in the multi-line and random-span tasks, using <EOT>?

For multiline, there exist other stopping heuristics (see TruncationParameters here https://github.com/Eric-Wallace/codex/blob/main/infill_evaluation.py), but IIRC both https://github.com/bigcode-project/bigcode-evaluation-harness and our internal code use only EOT as stop symbol in multiline."
171,ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9),"## Problem Description
After completing setup for CodeLlama, from the [README.md](https://github.com/facebookresearch/codellama), when I attempt to run any of the examples, with the specified commands:

```
torchrun --nproc_per_node 1 example_completion.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 128 --max_batch_size 4
```
OR
```
torchrun --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 192 --max_batch_size 4
```
OR
```
torchrun --nproc_per_node 1 example_instructions.py --ckpt_dir CodeLlama-7b-Instruct/ --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model --max_seq_len 512 --max_batch_size 4
```

I get the output with the error below:

### Output

```
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 0 (pid: 31383) of binary: /home/abc/miniconda3/envs/llama_env/bin/python
Traceback (most recent call last):
  File ""/home/abc/miniconda3/envs/llama_env/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/abc/miniconda3/envs/llama_env/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/abc/miniconda3/envs/llama_env/lib/python3.10/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/abc/miniconda3/envs/llama_env/lib/python3.10/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/abc/miniconda3/envs/llama_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/abc/miniconda3/envs/llama_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
======================================================
example_completion.py FAILED
------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-12-10_13:12:17
  host      : ABC-PC.
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 31383)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 31383
======================================================
```

## Runtime Environment
- Model: [`CodeLlama-7b`, `CodeLlama-7b-Instruct`, `CodeLlama-7b-Python`]
- Using via huggingface?: [no]
- OS: [Linux/Ubuntu (via WSL2), Windows]
- GPU VRAM: 4GB
- Number of GPUs: 1
- GPU Make: [Nvidia]
- GPU Version: NVIDIA GeForce GTX 1650

**Additional context**
I am trying to run the models on Ubuntu through WSL 2, I tried setting the batch size to 6 (`--max_batch_size 6`) as was mentioned in [llama #706](https://github.com/facebookresearch/llama/issues/706) but this did not help.
",x807x,I met the same issue. I find that I ran out of RAM via `htop`. Try `.wslconfig` to enable more RAM.
170,What is the max length could the codellama-2-7B generate?,"I was doing an inference work using codellama-2-7B.

Here is my code:
```
inputs_ids = tokenizer(prompt, return_tensors=""pt"").input_ids.to(self.device)
generate_ids=model.generate(inputs_ids,max_new_tokens=1024,num_return_sequences=1,pad_token_id=tokenizer.eos_token_id)
output = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)
```
I want to know the maximum that `max_new_token` can be set to?",humza-sami,This base model can produce total 4096 tokens. You can set `max_new_token` to 4096. This 4096 tokens are including number of tokens of the prompts as well.
170,What is the max length could the codellama-2-7B generate?,"I was doing an inference work using codellama-2-7B.

Here is my code:
```
inputs_ids = tokenizer(prompt, return_tensors=""pt"").input_ids.to(self.device)
generate_ids=model.generate(inputs_ids,max_new_tokens=1024,num_return_sequences=1,pad_token_id=tokenizer.eos_token_id)
output = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)
```
I want to know the maximum that `max_new_token` can be set to?",jgehring,"Hi @Uestc-Young, please note that since generation is auto-regressive, the maximum length for generation is the maximum sequence length supported minus the length of the prompt. There is a `max_seq_len` argument that you can specify when you build the model, and you can set this parameter to up to 100000 (but, depending on your GPU, you may run into memory issues and hence go with a lower value)."
168,Bugfixing error thrown when logprobs=True,"This PR introduces a fix for the error: ""TypeError: 'int' object is not iterable"" that appears when one invokes to text_completion or chat_completion  method with argument logprobs=True. For example, 

```
    results = generator.chat_completion(
        instructions,  
        max_gen_len=max_gen_len,
        temperature=temperature,
        top_p=top_p,
        logprobs=True ## Exposes the wrong behaviour
    )
```
or 

```
    results = generator.text_completion(
        prompts,
        max_gen_len=max_gen_len,
        temperature=temperature,
        top_p=top_p,
        logprobs=True ## Exposes the wrong behaviour

    )
```

The changes done by this PR fixes this problem.",jgehring,This is fixed in today's update.
167,Update download.sh,I want to chat with you,facebook-github-bot,"Hi @Mmmred! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%23167). Thanks!"
167,Update download.sh,I want to chat with you,facebook-github-bot,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!
167,Update download.sh,I want to chat with you,Mmmred,I want to chat with you 
167,Update download.sh,I want to chat with you,Mmmred,"Where is the code

في الجمعة، ٢٤ نوفمبر ٢٠٢٣, ١١:٢٨ م Facebook Community Bot <
***@***.***> كتب:

> Thank you for signing our Contributor License Agreement. We can now accept
> your code for this (and any) Meta Open Source project. Thanks!
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/facebookresearch/codellama/pull/167#issuecomment-1826065657>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/BD62CKLMIW6USPF5EODL2IDYGD7P3AVCNFSM6AAAAAA7ZPNQNWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQMRWGA3DKNRVG4>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
"
167,Update download.sh,I want to chat with you,Mmmred,Hiii there it's very nice to chat you 
166,Cannot run HF example codes on all three codeLlama-Python-hf models,"I understand this might be a huggingface-related problem but I cannot find the answer anywhere so I come to ask for help.

On huggingface there is a example code for codellama model:

>>>from transformers import LlamaForCausalLM, CodeLlamaTokenizer

>>>tokenizer = CodeLlamaTokenizer.from_pretrained(""codellama/CodeLlama-7b-hf"")
>>>model = LlamaForCausalLM.from_pretrained(""codellama/CodeLlama-7b-hf"")
>>>PROMPT = '''def remove_non_ascii(s: str) -> str:
    """""" <FILL_ME>
    return result
'''
>>>input_ids = tokenizer(PROMPT, return_tensors=""pt"")[""input_ids""]
>>>generated_ids = model.generate(input_ids, max_new_tokens=128)

>>>filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]
>>>print(PROMPT.replace(""<FILL_ME>"", filling))

And the output is like:

```
def remove_non_ascii(s: str) -> str:
    """""" Remove non-ASCII characters from a string.

    Args:
        s: The string to remove non-ASCII characters from.

    Returns:
        The string with non-ASCII characters removed.
    """"""
    result = """"
    for c in s:
        if ord(c) < 128:
            result += c
    return result
```

However, this works fine with all the original codellama model and codellama instruct models. But all three codellama-Python models will show tons of ""Assertion `srcIndex < srcSelectDimSize` failed"" errors and fail to complete the running.
The second strange thing is that, if I delete the ' <FILL_ME> ' part in the PROMPT when I am using codellama-Python model, then the error won't show , however there will still be no output.

So my questions are:
1. Why will these ""Assertion `srcIndex < srcSelectDimSize` failed"" errors happen on codellama-Python, as well as the no-output problems after I deleting the <FILL_ME> in the PROMPT? From my point of view, codeLlama-python is just modified on more Python tasks, and it should not be fundamentally different with original codellama and codellama-instruct.

2. Why the readme of Huggingface page says Codellama-Python cannot do infilling? Why modification on Python tasks will make the model cannot do infilling? Is the problem in my question 1 related to this lack of infilling of codellama-Python?

Thank you so much for your precious time.
",humza-sami,"As far as I know, `codellama-Python` is not for infilling. Please refer to its documentation.
![image](https://github.com/facebookresearch/codellama/assets/63999516/5937eab9-271f-4abe-b18e-85ccb2388492)

This model is not finetuned on infilling dataset. It is finetuned on only next token prediction dataset. "
165,Running example_completion.py with CodeLLama-34b locally returns an assertion error.,"```
torchrun --nproc_per_node 1 example_completion.py     --ckpt_dir CodeLlama-34b/     --tokenizer_path CodeLlama-34b/tokenizer.model     --max_seq_len 128 --max_batch_size 4 --nodes 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/home/steven/.local/lib/python3.8/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/steven/.local/lib/python3.8/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/steven/.local/lib/python3.8/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/home/steven/github/stephenwithav/codellama/llama/generation.py"", line 97, in build
    assert model_parallel_size == len(
AssertionError: Loading a checkpoint for MP=4 but world size is 1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2308980) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/home/steven/.local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/run.py"", line 762, in main
    run(args)
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/run.py"", line 753, in run
    elastic_launch(
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-11-15_15:48:25
  host      : work
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2308980)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",humza-sami,What is your system specs ?
165,Running example_completion.py with CodeLLama-34b locally returns an assertion error.,"```
torchrun --nproc_per_node 1 example_completion.py     --ckpt_dir CodeLlama-34b/     --tokenizer_path CodeLlama-34b/tokenizer.model     --max_seq_len 128 --max_batch_size 4 --nodes 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/home/steven/.local/lib/python3.8/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/steven/.local/lib/python3.8/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/steven/.local/lib/python3.8/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/home/steven/github/stephenwithav/codellama/llama/generation.py"", line 97, in build
    assert model_parallel_size == len(
AssertionError: Loading a checkpoint for MP=4 but world size is 1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2308980) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/home/steven/.local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/run.py"", line 762, in main
    run(args)
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/run.py"", line 753, in run
    elastic_launch(
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-11-15_15:48:25
  host      : work
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2308980)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",stephenwithav,"12 i7 cores, 64GB RAM."
165,Running example_completion.py with CodeLLama-34b locally returns an assertion error.,"```
torchrun --nproc_per_node 1 example_completion.py     --ckpt_dir CodeLlama-34b/     --tokenizer_path CodeLlama-34b/tokenizer.model     --max_seq_len 128 --max_batch_size 4 --nodes 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/home/steven/.local/lib/python3.8/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/steven/.local/lib/python3.8/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/steven/.local/lib/python3.8/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/home/steven/github/stephenwithav/codellama/llama/generation.py"", line 97, in build
    assert model_parallel_size == len(
AssertionError: Loading a checkpoint for MP=4 but world size is 1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2308980) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/home/steven/.local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/run.py"", line 762, in main
    run(args)
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/run.py"", line 753, in run
    elastic_launch(
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-11-15_15:48:25
  host      : work
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2308980)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",humza-sami,"Can you share code how you are running the code ?
Also nvidia-smi output as well

On Fri, 1 Dec 2023, 1:49 am Steven Edwards, ***@***.***>
wrote:

> 12 i7 cores, 64GB RAM.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/facebookresearch/codellama/issues/165#issuecomment-1834540604>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/APII4HB3IRSMPVVD6WLIUV3YHDWPPAVCNFSM6AAAAAA7NDQYCSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQMZUGU2DANRQGQ>
> .
> You are receiving this because you commented.Message ID:
> ***@***.***>
>
"
165,Running example_completion.py with CodeLLama-34b locally returns an assertion error.,"```
torchrun --nproc_per_node 1 example_completion.py     --ckpt_dir CodeLlama-34b/     --tokenizer_path CodeLlama-34b/tokenizer.model     --max_seq_len 128 --max_batch_size 4 --nodes 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/home/steven/.local/lib/python3.8/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/steven/.local/lib/python3.8/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/steven/.local/lib/python3.8/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/home/steven/github/stephenwithav/codellama/llama/generation.py"", line 97, in build
    assert model_parallel_size == len(
AssertionError: Loading a checkpoint for MP=4 but world size is 1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2308980) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/home/steven/.local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/run.py"", line 762, in main
    run(args)
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/run.py"", line 753, in run
    elastic_launch(
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-11-15_15:48:25
  host      : work
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2308980)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",stephenwithav,"I reinstalled 7b-Instruct, 13b-Instruct, and 13b today and everything worked fine.  I must have made an error previously.  Thanks for your assistance!"
164,Is download.sh providing the correct tokenizer.model files?,"When I try to run a model ..

```
torchrun example_js.py \
    --ckpt_dir CodeLlama-13b-Instruct \
    --tokenizer_path CodeLlama-13b-Instruct/tokenizer.model \
    --max_seq_len 1024 --max_batch_size 4 --nproc_per_node 2
```
example_js is the same as the provide example_completion, but with different prompts

... I get this error:
```
RuntimeError: Error(s) in loading state_dict for Transformer:
	size mismatch for tok_embeddings.weight: copying a param with shape torch.Size([32016, 2560]) from checkpoint, the shape in current model is torch.Size([32000, 5120]).
	size mismatch for layers.0.attention.wq.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).
	size mismatch for layers.0.attention.wk.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).
	size mismatch for layers.0.attention.wv.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).
	size mismatch for layers.0.attention.wo.weight: copying a param with shape torch.Size([5120, 2560]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).
...
```

This code works perfectly fine if I use the 7b model and tokenizer

Investigating a bit further, I noticed this:
```
md5sum CodeLlama-7b-Instruct/tokenizer.model
9e597e72392fd4005529a33f2bf708ba  CodeLlama-7b-Instruct/tokenizer.model
md5sum CodeLlama-13b-Instruct/tokenizer.model
9e597e72392fd4005529a33f2bf708ba  CodeLlama-13b-Instruct/tokenizer.model
md5sum CodeLlama-34b-Instruct/tokenizer.model
eeec4125e9c7560836b4873b6f8e3025  CodeLlama-34b-Instruct/tokenizer.model
```

the tokenizer for 7b and 13b are identical? That seems unlikely. 

I also attempted these variants of torchrun just to see what happens
```
torchrun --ckpt_dir CodeLlama-13b-Instruct --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model 
torchrun --ckpt_dir CodeLlama-34b-Instruct --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model --nproc_per_node 4
```
- These produced the same errors, but with different numbers

On another node, the --nproc_per_node value is provided to the commands just in case (as the docs say it's needed), but in practice I find it has no effect. I was forced to modify the code that builds the model like so:
```
generator = Llama.build(
        ckpt_dir=ckpt_dir,
        tokenizer_path=tokenizer_path,
        max_seq_len=max_seq_len,
        max_batch_size=max_batch_size,

       # Added this, value is 2 for 13b and 4 for 34b
        model_parallel_size=2,
    )
```

I'm on an M1 Macbook Pro with 64 GB of ram",JoshuaChou2018,"I encountered the same problem, CodeLlama-7b-Instruct works, but CodeLlama-13b-Instruct and CodeLlama-34b-Instruct failed. I manually set model_parallel_size=3 for 13b and 4 for 34b, still get size mismatch error."
164,Is download.sh providing the correct tokenizer.model files?,"When I try to run a model ..

```
torchrun example_js.py \
    --ckpt_dir CodeLlama-13b-Instruct \
    --tokenizer_path CodeLlama-13b-Instruct/tokenizer.model \
    --max_seq_len 1024 --max_batch_size 4 --nproc_per_node 2
```
example_js is the same as the provide example_completion, but with different prompts

... I get this error:
```
RuntimeError: Error(s) in loading state_dict for Transformer:
	size mismatch for tok_embeddings.weight: copying a param with shape torch.Size([32016, 2560]) from checkpoint, the shape in current model is torch.Size([32000, 5120]).
	size mismatch for layers.0.attention.wq.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).
	size mismatch for layers.0.attention.wk.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).
	size mismatch for layers.0.attention.wv.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).
	size mismatch for layers.0.attention.wo.weight: copying a param with shape torch.Size([5120, 2560]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).
...
```

This code works perfectly fine if I use the 7b model and tokenizer

Investigating a bit further, I noticed this:
```
md5sum CodeLlama-7b-Instruct/tokenizer.model
9e597e72392fd4005529a33f2bf708ba  CodeLlama-7b-Instruct/tokenizer.model
md5sum CodeLlama-13b-Instruct/tokenizer.model
9e597e72392fd4005529a33f2bf708ba  CodeLlama-13b-Instruct/tokenizer.model
md5sum CodeLlama-34b-Instruct/tokenizer.model
eeec4125e9c7560836b4873b6f8e3025  CodeLlama-34b-Instruct/tokenizer.model
```

the tokenizer for 7b and 13b are identical? That seems unlikely. 

I also attempted these variants of torchrun just to see what happens
```
torchrun --ckpt_dir CodeLlama-13b-Instruct --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model 
torchrun --ckpt_dir CodeLlama-34b-Instruct --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model --nproc_per_node 4
```
- These produced the same errors, but with different numbers

On another node, the --nproc_per_node value is provided to the commands just in case (as the docs say it's needed), but in practice I find it has no effect. I was forced to modify the code that builds the model like so:
```
generator = Llama.build(
        ckpt_dir=ckpt_dir,
        tokenizer_path=tokenizer_path,
        max_seq_len=max_seq_len,
        max_batch_size=max_batch_size,

       # Added this, value is 2 for 13b and 4 for 34b
        model_parallel_size=2,
    )
```

I'm on an M1 Macbook Pro with 64 GB of ram",jgehring,"Sorry for replying so late, but just to clarify, the 34b model uses a different tokenizer as it was not trained with fill-in-the-middle capabilities.

For the commands you provided, the `--nproc_per_node` needs to be passed to `torchrun` but by appending it to the rest of the command it will be pass to `example_js.py` instead. The current version of the code will warn you about any model parallel mismatches at runtime. This command works for me:

```
torchrun --nproc_per_node=2 example_instructions.py \
    --ckpt_dir CodeLlama-13b-Instruct \
    --tokenizer_path CodeLlama-13b-Instruct/tokenizer.model 
```"
163,"Download URL from the email gives me Llama code, not Llama 2","I get an email with a download URL, but when I run a download URL, I get these options:
**Enter the list of models to download without spaces (7b,13b,34b,7b-Python,13b-Python,34b-Python,7b-Instruct,13b-Instruct,34b-Instruct), or press Enter for all:** 

I do not see these models as per email:
Llama-2-7b
Llama-2-7b-chat
Llama-2-13b
Llama-2-13b-chat
Llama-2-70b
Llama-2-70b-chat

Am I doing anything wrong?
",dveb8886,"These look like they correspond to the chat variant of llama 2 (this repo: https://github.com/facebookresearch/llama)

When you requested access to llama, you had the opportunity to select which model you want, the current repo you filed the issue for corresponds to the code variant.

The code and chat variants arrived in different emails for me, so check your email if you requested both"
161,Issue with loading codellama models outside codellama directory,"When I try to run python script, I get this error :
```
ypeError                                 Traceback (most recent call last)
Cell In[6], line 10
      6 max_batch_size = 4
      7 max_gen_len: Optional[int] = None
---> 10 generator = Llama.build(
     11         ckpt_dir=ckpt_dir,
     12         tokenizer_path=tokenizer_path,
     13         max_seq_len=max_seq_len,
     14         max_batch_size=max_batch_size,
     15     )

File ~/llama/llama/generation.py:111, in Llama.build(ckpt_dir, tokenizer_path, max_seq_len, max_batch_size, model_parallel_size, seed)
    108 with open(Path(ckpt_dir) / ""params.json"", ""r"") as f:
    109     params = json.loads(f.read())
--> 111 model_args: ModelArgs = ModelArgs(
    112     max_seq_len=max_seq_len,
    113     max_batch_size=max_batch_size,
    114     **params,
    115 )
    116 tokenizer = Tokenizer(model_path=tokenizer_path)
    117 model_args.vocab_size = tokenizer.n_words

TypeError: __init__() got an unexpected keyword argument 'rope_theta'
```",sukeesh,"+1 I am also seeing this error
"
161,Issue with loading codellama models outside codellama directory,"When I try to run python script, I get this error :
```
ypeError                                 Traceback (most recent call last)
Cell In[6], line 10
      6 max_batch_size = 4
      7 max_gen_len: Optional[int] = None
---> 10 generator = Llama.build(
     11         ckpt_dir=ckpt_dir,
     12         tokenizer_path=tokenizer_path,
     13         max_seq_len=max_seq_len,
     14         max_batch_size=max_batch_size,
     15     )

File ~/llama/llama/generation.py:111, in Llama.build(ckpt_dir, tokenizer_path, max_seq_len, max_batch_size, model_parallel_size, seed)
    108 with open(Path(ckpt_dir) / ""params.json"", ""r"") as f:
    109     params = json.loads(f.read())
--> 111 model_args: ModelArgs = ModelArgs(
    112     max_seq_len=max_seq_len,
    113     max_batch_size=max_batch_size,
    114     **params,
    115 )
    116 tokenizer = Tokenizer(model_path=tokenizer_path)
    117 model_args.vocab_size = tokenizer.n_words

TypeError: __init__() got an unexpected keyword argument 'rope_theta'
```",niginajon,+1
161,Issue with loading codellama models outside codellama directory,"When I try to run python script, I get this error :
```
ypeError                                 Traceback (most recent call last)
Cell In[6], line 10
      6 max_batch_size = 4
      7 max_gen_len: Optional[int] = None
---> 10 generator = Llama.build(
     11         ckpt_dir=ckpt_dir,
     12         tokenizer_path=tokenizer_path,
     13         max_seq_len=max_seq_len,
     14         max_batch_size=max_batch_size,
     15     )

File ~/llama/llama/generation.py:111, in Llama.build(ckpt_dir, tokenizer_path, max_seq_len, max_batch_size, model_parallel_size, seed)
    108 with open(Path(ckpt_dir) / ""params.json"", ""r"") as f:
    109     params = json.loads(f.read())
--> 111 model_args: ModelArgs = ModelArgs(
    112     max_seq_len=max_seq_len,
    113     max_batch_size=max_batch_size,
    114     **params,
    115 )
    116 tokenizer = Tokenizer(model_path=tokenizer_path)
    117 model_args.vocab_size = tokenizer.n_words

TypeError: __init__() got an unexpected keyword argument 'rope_theta'
```",dayuyang1999,+ 1
161,Issue with loading codellama models outside codellama directory,"When I try to run python script, I get this error :
```
ypeError                                 Traceback (most recent call last)
Cell In[6], line 10
      6 max_batch_size = 4
      7 max_gen_len: Optional[int] = None
---> 10 generator = Llama.build(
     11         ckpt_dir=ckpt_dir,
     12         tokenizer_path=tokenizer_path,
     13         max_seq_len=max_seq_len,
     14         max_batch_size=max_batch_size,
     15     )

File ~/llama/llama/generation.py:111, in Llama.build(ckpt_dir, tokenizer_path, max_seq_len, max_batch_size, model_parallel_size, seed)
    108 with open(Path(ckpt_dir) / ""params.json"", ""r"") as f:
    109     params = json.loads(f.read())
--> 111 model_args: ModelArgs = ModelArgs(
    112     max_seq_len=max_seq_len,
    113     max_batch_size=max_batch_size,
    114     **params,
    115 )
    116 tokenizer = Tokenizer(model_path=tokenizer_path)
    117 model_args.vocab_size = tokenizer.n_words

TypeError: __init__() got an unexpected keyword argument 'rope_theta'
```",mayifeidaisy,"same problem here, have you solved it?"
160,How to use with VS Code?,"I thinks we need:
- VS Code plugins
- Model loading methods
- API Server to communicate between plugins and model backend

Is here we have mature solution?",AlessandroZavoli,"Look for ""Llama Coder"" in VS Code extensions"
158,How to add tokens to tokenizer?,"I want to add some tokens like `[BOST]` to the tokenizer so that it does not split these.

How can I achieve this? Any suggestions are welcome.

Huggingface provides functions like `add_tokens` but I want to make other changes in the source, so I don't want to use HF.",humza-sami,"```python
tokenizer.add_special_tokens([""[BOST]""])"
157,What kind of prompt was used in HumanEval evaluation?,"Thanks for your excellent work. I find there is no prompt for HumanEval completion task was released, could you give me some introduction?",humza-sami,"Its same prompt which are for llama-2.

`<s>[INST] User message [/INST] Response </s>`"
157,What kind of prompt was used in HumanEval evaluation?,"Thanks for your excellent work. I find there is no prompt for HumanEval completion task was released, could you give me some introduction?",jgehring,"See #104 for some discussion on this, specifically [this comment](https://github.com/facebookresearch/codellama/issues/104#issuecomment-1779575189). Feel free to reopen this if you have further questions."
156,Gets stuck running example_completion.py ,"I ran `example_completion.py`

```
python -m torch.distributed.run --nproc_per_node 1 example_completion.py \
    --ckpt_dir CodeLlama-7b/ \
    --tokenizer_path CodeLlama-7b/tokenizer.model \
    --max_seq_len 128 --max_batch_size 4
```

and immediately I see 

```
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
```

And nothing happens for a very long time. I thought the code completion should be printed.",zxj-c,Just realized what the problem was: not enough GPU memory. Now it worked!
155,How to let codellama or codellama-python stop?,"When I use codellama or codellama-python to finish the continuation of a prompt, a lot of '\n' are outputed in the end until it reaches the max_gen_len. Is there any way to let it stop early except limiting the max_gen_len which is not convenient?",yangzhj53,"I read the code and I found a trick to solve it. After encoding, the id of '\n' is 13. The program will generate a batch of token_id each time, and I just need to check if 13 appears in all positions for two times."
155,How to let codellama or codellama-python stop?,"When I use codellama or codellama-python to finish the continuation of a prompt, a lot of '\n' are outputed in the end until it reaches the max_gen_len. Is there any way to let it stop early except limiting the max_gen_len which is not convenient?",abs-xyz,Maybe use the [`stop_token`](https://github.com/facebookresearch/codellama/blob/30083473e9c4c46b2e4eb69e1c7572b1098ffa82/llama/generation.py#L140) in `generate()`?
155,How to let codellama or codellama-python stop?,"When I use codellama or codellama-python to finish the continuation of a prompt, a lot of '\n' are outputed in the end until it reaches the max_gen_len. Is there any way to let it stop early except limiting the max_gen_len which is not convenient?",humza-sami,Use  `</s>`  as end of string token in the generation
152,Run code llama from Hugging Face locally with GPU,"
I have trying to host the Code Llama from Hugging Face locally and trying to run it. It runs soley on CPU and it is not utilizing GPU available in the machine despite having Nvidia Drivers and Cuda toolkit.

from transformers import AutoTokenizer
import transformers

model = ""codellama/CodeLlama-7b-hf""

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    torch_dtype=None,
    device_map = ""cuda:0""
)

prompt = ""Write python code to reverse a string""

sequences = pipeline(
    prompt,
    do_sample=True,
    top_k=10,
    temperature=0.1,
    top_p=0.95,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    max_length=200,
)
for seq in sequences:
    print(f""Result: {seq['generated_text']}"")

The code above runs the LLM locally but in case we use cuda for the device, it gives the following error


File ""C:\Users\winuser3\Desktop\GENAI-App\venv\lib\site-packages\transformers\modeling_utils.py"", line 3333, in from_pretrained
    ) = cls._load_pretrained_model(
  File ""C:\Users\winuser3\Desktop\GENAI-App\venv\lib\site-packages\transformers\modeling_utils.py"", line 3723, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File ""C:\Users\winuser3\Desktop\GENAI-App\venv\lib\site-packages\transformers\modeling_utils.py"", line 744, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File ""C:\Users\winuser3\Desktop\GENAI-App\venv\lib\site-packages\accelerate\utils\modeling.py"", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
  File ""C:\Users\winuser3\Desktop\GENAI-App\venv\lib\site-packages\torch\cuda\__init__.py"", line 289, in _lazy_init
    raise AssertionError(""Torch not compiled with CUDA enabled"")
AssertionError: Torch not compiled with CUDA enabled
",humza-sami,"Please share your cudadnn, cuda toolkit version along with GPU model. Althoug following code block automatically detects GPU. 
```python
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    torch_dtype=torch.float16,
    device_map=""auto"",
)
```"
150,instruct model's performance become poor when switching to different format,"I use 7b and 13b-instruct model to do program analysis task. When I use original format 7b-Instruct and 13b-Instruct model, following `example_chat_completion.py`, everything is all right.

But when I use huggingface version model, use the `generate` api. I found the model's response is much worse than origin format one. Also, the same thing happen when deploy online with tool like [lmdeploy](https://github.com/InternLM/lmdeploy/).

It seems only locally run original format model give me a satisfiable answer. How to fix this?",humza-sami,"In the Huggingface generation pipeline, Are you using Instruct prompt instructions ?

`<s>[INST] user message 1 [/INST] response 1 </s><s>[INST] user message 2 [/INST] response 2 </s>`

In example_chat_completion.py, this thing is handled internally while in huggingface we have to do manually."
150,instruct model's performance become poor when switching to different format,"I use 7b and 13b-instruct model to do program analysis task. When I use original format 7b-Instruct and 13b-Instruct model, following `example_chat_completion.py`, everything is all right.

But when I use huggingface version model, use the `generate` api. I found the model's response is much worse than origin format one. Also, the same thing happen when deploy online with tool like [lmdeploy](https://github.com/InternLM/lmdeploy/).

It seems only locally run original format model give me a satisfiable answer. How to fix this?",for-just-we,"> In the Huggingface generation pipeline, Are you using Instruct prompt instructions ?
> 
> `<s>[INST] user message 1 [/INST] response 1 </s><s>[INST] user message 2 [/INST] response 2 </s>`
> 
> In example_chat_completion.py, this thing is handled internally while in huggingface we have to do manually.

yes, I actually also tried different prompt method. For example, I tried follow example-text-completion.py, while the response may not be as good as follow chat_completion, the answer is sort of reliable. But for huggingface model, the answer is always worse."
150,instruct model's performance become poor when switching to different format,"I use 7b and 13b-instruct model to do program analysis task. When I use original format 7b-Instruct and 13b-Instruct model, following `example_chat_completion.py`, everything is all right.

But when I use huggingface version model, use the `generate` api. I found the model's response is much worse than origin format one. Also, the same thing happen when deploy online with tool like [lmdeploy](https://github.com/InternLM/lmdeploy/).

It seems only locally run original format model give me a satisfiable answer. How to fix this?",humza-sami,Can you share code for inference ?
150,instruct model's performance become poor when switching to different format,"I use 7b and 13b-instruct model to do program analysis task. When I use original format 7b-Instruct and 13b-Instruct model, following `example_chat_completion.py`, everything is all right.

But when I use huggingface version model, use the `generate` api. I found the model's response is much worse than origin format one. Also, the same thing happen when deploy online with tool like [lmdeploy](https://github.com/InternLM/lmdeploy/).

It seems only locally run original format model give me a satisfiable answer. How to fix this?",for-just-we,"> inference

The prompt is like:

```
Pay special attention to semantic similarity between the xx,xx,xx 

...

...

Your analysis should determine if there is a substantial possibility of the indirect call effectively invoking the function. 
Provide your answer with only 'yes' (for likely), or 'no' (for unlikely).
```

The code for inference is the same as example_chat.py for llama format. For huggingface format. I transform the prompt into `str` and use the same code in [huggingface example](https://huggingface.co/codellama/CodeLlama-7b-hf)"
150,instruct model's performance become poor when switching to different format,"I use 7b and 13b-instruct model to do program analysis task. When I use original format 7b-Instruct and 13b-Instruct model, following `example_chat_completion.py`, everything is all right.

But when I use huggingface version model, use the `generate` api. I found the model's response is much worse than origin format one. Also, the same thing happen when deploy online with tool like [lmdeploy](https://github.com/InternLM/lmdeploy/).

It seems only locally run original format model give me a satisfiable answer. How to fix this?",jgehring,@for-just-we can you post some code for inference with a relevant example prompt for both this repo and the HF model so we can see where things might go wrong?
150,instruct model's performance become poor when switching to different format,"I use 7b and 13b-instruct model to do program analysis task. When I use original format 7b-Instruct and 13b-Instruct model, following `example_chat_completion.py`, everything is all right.

But when I use huggingface version model, use the `generate` api. I found the model's response is much worse than origin format one. Also, the same thing happen when deploy online with tool like [lmdeploy](https://github.com/InternLM/lmdeploy/).

It seems only locally run original format model give me a satisfiable answer. How to fix this?",humza-sami,"@for-just-we  It would be helpful if you post your inference code here. Anyways, Could you try this **code snippet** and check if it is producing some better results. 

```
from transformers import AutoTokenizer
import transformers
import torch

model = ""YOUR MODEL NAME""

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    torch_dtype=torch.float16,
    device_map=""auto"",
)


prompt = """"""<s>[INST] Pay special attention to semantic similarity between the xx,xx,xx\n\nYour analysis should determine if there is a substantial possibility of the indirect call effectively invoking the function. 
Provide your answer with only 'yes' (for likely), or 'no' (for unlikely). [/INST]""""""

sequences = pipeline(
    prompt,
    do_sample=True,
    top_k=10,
    temperature=0.1,
    top_p=0.95,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    max_length=200,
)
for seq in sequences:
    print(f""Result: {seq['generated_text']}"")

```"
149,use md5 for mac.,Related to facebookresearch/llama@4649acd,facebook-github-bot,"Hi @Ja1Zhou! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%23149). Thanks!"
148,Please help: failed to create process,"Hi, 

Apologies if the solution is obvious but I'm new to this. When running the example infilling script:
`torchrun --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 192 --max_batch_size 4`

I get the following error with no additional details:
`failed to create process`

In fact anything with `torchrun` returns the same error.

I tried: 
- Different versions of CUDA and CPU-only Pytorch
- Checked that `tokenizer_path` is correct and `nproc_per_node` is set to the right MP value
- A comment on another post suggested using `python -m torch.distributed.run` instead of `torchrun`. I get a different error when I do this (happy to give more info)

Any help would be greatly appreciated!",antonioanerao,"I'm facing the same issue. Tried with python 3.8 and 3.7. I'm just an enthusiastic person trying to play with an AI , I have no experience with Python but I would love a tip to solve this. Trying researching and even ChatGPT but no success so far.

I created my Conda env with the following `conda create -conda create --name llama-code-p37 python=3.8 pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge`. Am I missing something?"
148,Please help: failed to create process,"Hi, 

Apologies if the solution is obvious but I'm new to this. When running the example infilling script:
`torchrun --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 192 --max_batch_size 4`

I get the following error with no additional details:
`failed to create process`

In fact anything with `torchrun` returns the same error.

I tried: 
- Different versions of CUDA and CPU-only Pytorch
- Checked that `tokenizer_path` is correct and `nproc_per_node` is set to the right MP value
- A comment on another post suggested using `python -m torch.distributed.run` instead of `torchrun`. I get a different error when I do this (happy to give more info)

Any help would be greatly appreciated!",allelive,"> Sto affrontando lo stesso problema. Provato con Python 3.8 e 3.7. Sono solo una persona entusiasta che cerca di giocare con un'intelligenza artificiale, non ho esperienza con Python ma mi piacerebbe un suggerimento per risolvere questo problema. Ho provato a fare ricerche e persino a ChatGPT, ma finora senza successo.
> 
> Ho creato il mio Conda env con il seguente file `conda create -conda create --name llama-code-p37 python=3.8 pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge`. Mi sto perdendo qualcosa?

Serve python3.10 io uso python3.12 deve essere superiore a pyton10  prova a vedere se risolvi il problema"
148,Please help: failed to create process,"Hi, 

Apologies if the solution is obvious but I'm new to this. When running the example infilling script:
`torchrun --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 192 --max_batch_size 4`

I get the following error with no additional details:
`failed to create process`

In fact anything with `torchrun` returns the same error.

I tried: 
- Different versions of CUDA and CPU-only Pytorch
- Checked that `tokenizer_path` is correct and `nproc_per_node` is set to the right MP value
- A comment on another post suggested using `python -m torch.distributed.run` instead of `torchrun`. I get a different error when I do this (happy to give more info)

Any help would be greatly appreciated!",hiehie1234," Solved from stackoverflow! you can try

https://stackoverflow.com/questions/77425569/llama2-running-pytorch-produces-a-failed-to-create-process"
146,Fine-tuning on other languages,"Hi,

Can I fine-tune the model with other languages such as Korean, Chinese, Japanese, etc.?
In general, other than English, what languages can the model recognize?

Thank you for sharing your work!",humza-sami,"Yes, You can finetune on different subtask. You have to follow the dataset format which they have used for the training."
145,"Codellama-7B can't  reproduce the paper's result 33.5%,why?","I evaluated codellama 7b using greedy decoding, and the Humaneval result for pass@1 was only 28.66%. You are asking how to reproduce the paper's result of 33.5%.  pls show the evaluation parameters and prompt format. Here are some suggestions:
![image](https://github.com/facebookresearch/codellama/assets/142714958/fee2ace9-e64e-4745-a5f1-06cf2caa3423)


",jgehring,I provided further details on HumanEval results in [this comment](https://github.com/facebookresearch/codellama/issues/104#issuecomment-1779575189); does this address your question?
145,"Codellama-7B can't  reproduce the paper's result 33.5%,why?","I evaluated codellama 7b using greedy decoding, and the Humaneval result for pass@1 was only 28.66%. You are asking how to reproduce the paper's result of 33.5%.  pls show the evaluation parameters and prompt format. Here are some suggestions:
![image](https://github.com/facebookresearch/codellama/assets/142714958/fee2ace9-e64e-4745-a5f1-06cf2caa3423)


",jgehring,"I've closed the issue, feel free to re-open if you have further questions."
144,Unable to Download Code-Llama 7B via the download.sh Script,"Hello! I signed up to download the Code-Llama model from Meta. I received the email with the Unique Custom URL.

**However, when I attempt to download the model, the script throws an error. Can you please advise?**

Environment: Ubuntu 22.04

### Steps to Reproduce:
1. Cloned the repo from main branch: `git clone git@github.com:facebookresearch/codellama.git`
2. Executed the script: `./download.sh`
3. Followed the prompts, entered the URL from the email and entered only `7B` as the model to download

### Expected Outcome:
Should have downloaded the model

### Actual Outcome (query params omitted for possible security reasons):
Get the following error:
```
Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.
```

### Full log (query params omitted for possible security reasons):
```
~/workspaces/meta/llama (main)$ ./download.sh
Enter the URL from email: https://download2.llamameta.net/*?queryparams=XXX

Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B
Downloading LICENSE and Acceptable Usage Policy
--2023-10-11 13:10:04--  https://download2.llamameta.net/LICENSE?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

--2023-10-11 13:10:04--  https://download2.llamameta.net/USE_POLICY.md?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.
```

Thank you!",felbdogg,"Try this site

https://huggingface.co/meta-llama/"
144,Unable to Download Code-Llama 7B via the download.sh Script,"Hello! I signed up to download the Code-Llama model from Meta. I received the email with the Unique Custom URL.

**However, when I attempt to download the model, the script throws an error. Can you please advise?**

Environment: Ubuntu 22.04

### Steps to Reproduce:
1. Cloned the repo from main branch: `git clone git@github.com:facebookresearch/codellama.git`
2. Executed the script: `./download.sh`
3. Followed the prompts, entered the URL from the email and entered only `7B` as the model to download

### Expected Outcome:
Should have downloaded the model

### Actual Outcome (query params omitted for possible security reasons):
Get the following error:
```
Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.
```

### Full log (query params omitted for possible security reasons):
```
~/workspaces/meta/llama (main)$ ./download.sh
Enter the URL from email: https://download2.llamameta.net/*?queryparams=XXX

Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B
Downloading LICENSE and Acceptable Usage Policy
--2023-10-11 13:10:04--  https://download2.llamameta.net/LICENSE?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

--2023-10-11 13:10:04--  https://download2.llamameta.net/USE_POLICY.md?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.
```

Thank you!",jgehring,"Hi @jose-at-slalom, the URL from the e-mail is valid for 24 hours. Did the link already expire maybe?"
144,Unable to Download Code-Llama 7B via the download.sh Script,"Hello! I signed up to download the Code-Llama model from Meta. I received the email with the Unique Custom URL.

**However, when I attempt to download the model, the script throws an error. Can you please advise?**

Environment: Ubuntu 22.04

### Steps to Reproduce:
1. Cloned the repo from main branch: `git clone git@github.com:facebookresearch/codellama.git`
2. Executed the script: `./download.sh`
3. Followed the prompts, entered the URL from the email and entered only `7B` as the model to download

### Expected Outcome:
Should have downloaded the model

### Actual Outcome (query params omitted for possible security reasons):
Get the following error:
```
Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.
```

### Full log (query params omitted for possible security reasons):
```
~/workspaces/meta/llama (main)$ ./download.sh
Enter the URL from email: https://download2.llamameta.net/*?queryparams=XXX

Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B
Downloading LICENSE and Acceptable Usage Policy
--2023-10-11 13:10:04--  https://download2.llamameta.net/LICENSE?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

--2023-10-11 13:10:04--  https://download2.llamameta.net/USE_POLICY.md?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.
```

Thank you!",joseeliaschavez,"Hi @jgehring I had started the download within minutes of receiving the email.
Just tried again with a new email, receiving the same error 😢 
Should I post the URL?"
144,Unable to Download Code-Llama 7B via the download.sh Script,"Hello! I signed up to download the Code-Llama model from Meta. I received the email with the Unique Custom URL.

**However, when I attempt to download the model, the script throws an error. Can you please advise?**

Environment: Ubuntu 22.04

### Steps to Reproduce:
1. Cloned the repo from main branch: `git clone git@github.com:facebookresearch/codellama.git`
2. Executed the script: `./download.sh`
3. Followed the prompts, entered the URL from the email and entered only `7B` as the model to download

### Expected Outcome:
Should have downloaded the model

### Actual Outcome (query params omitted for possible security reasons):
Get the following error:
```
Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.
```

### Full log (query params omitted for possible security reasons):
```
~/workspaces/meta/llama (main)$ ./download.sh
Enter the URL from email: https://download2.llamameta.net/*?queryparams=XXX

Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B
Downloading LICENSE and Acceptable Usage Policy
--2023-10-11 13:10:04--  https://download2.llamameta.net/LICENSE?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

--2023-10-11 13:10:04--  https://download2.llamameta.net/USE_POLICY.md?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.
```

Thank you!",lt090004114, I had started the download within minutes of receiving the email. I receiving the same error.
144,Unable to Download Code-Llama 7B via the download.sh Script,"Hello! I signed up to download the Code-Llama model from Meta. I received the email with the Unique Custom URL.

**However, when I attempt to download the model, the script throws an error. Can you please advise?**

Environment: Ubuntu 22.04

### Steps to Reproduce:
1. Cloned the repo from main branch: `git clone git@github.com:facebookresearch/codellama.git`
2. Executed the script: `./download.sh`
3. Followed the prompts, entered the URL from the email and entered only `7B` as the model to download

### Expected Outcome:
Should have downloaded the model

### Actual Outcome (query params omitted for possible security reasons):
Get the following error:
```
Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.
```

### Full log (query params omitted for possible security reasons):
```
~/workspaces/meta/llama (main)$ ./download.sh
Enter the URL from email: https://download2.llamameta.net/*?queryparams=XXX

Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B
Downloading LICENSE and Acceptable Usage Policy
--2023-10-11 13:10:04--  https://download2.llamameta.net/LICENSE?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

--2023-10-11 13:10:04--  https://download2.llamameta.net/USE_POLICY.md?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.
```

Thank you!",DanubioLima,Hi! Receiving same error here!
144,Unable to Download Code-Llama 7B via the download.sh Script,"Hello! I signed up to download the Code-Llama model from Meta. I received the email with the Unique Custom URL.

**However, when I attempt to download the model, the script throws an error. Can you please advise?**

Environment: Ubuntu 22.04

### Steps to Reproduce:
1. Cloned the repo from main branch: `git clone git@github.com:facebookresearch/codellama.git`
2. Executed the script: `./download.sh`
3. Followed the prompts, entered the URL from the email and entered only `7B` as the model to download

### Expected Outcome:
Should have downloaded the model

### Actual Outcome (query params omitted for possible security reasons):
Get the following error:
```
Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.
```

### Full log (query params omitted for possible security reasons):
```
~/workspaces/meta/llama (main)$ ./download.sh
Enter the URL from email: https://download2.llamameta.net/*?queryparams=XXX

Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B
Downloading LICENSE and Acceptable Usage Policy
--2023-10-11 13:10:04--  https://download2.llamameta.net/LICENSE?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

--2023-10-11 13:10:04--  https://download2.llamameta.net/USE_POLICY.md?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.
```

Thank you!",zhouyinn,Hi! Receiving same error here!
144,Unable to Download Code-Llama 7B via the download.sh Script,"Hello! I signed up to download the Code-Llama model from Meta. I received the email with the Unique Custom URL.

**However, when I attempt to download the model, the script throws an error. Can you please advise?**

Environment: Ubuntu 22.04

### Steps to Reproduce:
1. Cloned the repo from main branch: `git clone git@github.com:facebookresearch/codellama.git`
2. Executed the script: `./download.sh`
3. Followed the prompts, entered the URL from the email and entered only `7B` as the model to download

### Expected Outcome:
Should have downloaded the model

### Actual Outcome (query params omitted for possible security reasons):
Get the following error:
```
Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.
```

### Full log (query params omitted for possible security reasons):
```
~/workspaces/meta/llama (main)$ ./download.sh
Enter the URL from email: https://download2.llamameta.net/*?queryparams=XXX

Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B
Downloading LICENSE and Acceptable Usage Policy
--2023-10-11 13:10:04--  https://download2.llamameta.net/LICENSE?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

--2023-10-11 13:10:04--  https://download2.llamameta.net/USE_POLICY.md?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.
```

Thank you!",Kingofolk,"Has this problem been solved? Met the same error, could anyone help me? Thx a lot!"
144,Unable to Download Code-Llama 7B via the download.sh Script,"Hello! I signed up to download the Code-Llama model from Meta. I received the email with the Unique Custom URL.

**However, when I attempt to download the model, the script throws an error. Can you please advise?**

Environment: Ubuntu 22.04

### Steps to Reproduce:
1. Cloned the repo from main branch: `git clone git@github.com:facebookresearch/codellama.git`
2. Executed the script: `./download.sh`
3. Followed the prompts, entered the URL from the email and entered only `7B` as the model to download

### Expected Outcome:
Should have downloaded the model

### Actual Outcome (query params omitted for possible security reasons):
Get the following error:
```
Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.
```

### Full log (query params omitted for possible security reasons):
```
~/workspaces/meta/llama (main)$ ./download.sh
Enter the URL from email: https://download2.llamameta.net/*?queryparams=XXX

Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B
Downloading LICENSE and Acceptable Usage Policy
--2023-10-11 13:10:04--  https://download2.llamameta.net/LICENSE?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

--2023-10-11 13:10:04--  https://download2.llamameta.net/USE_POLICY.md?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.
```

Thank you!",rcontesti,"Hi, I'm having the exact same error. Any clues. Thank you."
142,Fine-tuning Code Llama on my own code ,"Thank you for this amazing effort!

I would like to fine-tune code llama on my own Python code, let's call it MyPackage for now. Ultimately, I would like to ask Code llama questions of the form: ""how do I do [INSERT FEASIBLE TASK NAME] using MyPackage?""

I have my package ready and documented. How do I format the dataset to fine-tune Code Llama?
Are there fine-tuning scripts available in this repository?

",shatealaboxiaowang,same issue.
142,Fine-tuning Code Llama on my own code ,"Thank you for this amazing effort!

I would like to fine-tune code llama on my own Python code, let's call it MyPackage for now. Ultimately, I would like to ask Code llama questions of the form: ""how do I do [INSERT FEASIBLE TASK NAME] using MyPackage?""

I have my package ready and documented. How do I format the dataset to fine-tune Code Llama?
Are there fine-tuning scripts available in this repository?

",david-strejc,"Any answers for this question?
"
142,Fine-tuning Code Llama on my own code ,"Thank you for this amazing effort!

I would like to fine-tune code llama on my own Python code, let's call it MyPackage for now. Ultimately, I would like to ask Code llama questions of the form: ""how do I do [INSERT FEASIBLE TASK NAME] using MyPackage?""

I have my package ready and documented. How do I format the dataset to fine-tune Code Llama?
Are there fine-tuning scripts available in this repository?

",jgehring,"Hi @g12bftd, we don’t host any fine-tuning scripts in this repository. You can check out https://github.com/facebookresearch/llama-recipes which includes fine-tuning recipes for Llama 2 models and works with Code Llama as well.

Wrt dataset formatting, this is hard to answer from the get-go. However, getting LLMs to work with data not seen in training is a frequent use case so I would expect you’d find various further resources on the web. I’d probably start with in-context learning (i.e., putting relevant or abbreviated contents of your repo in your prompt) instead of fine-tuning."
141,while running dowload.sh file getting Bad substitution error,"Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B
download.sh: 14: [[: not found
Downloading LICENSE and Acceptable Usage Policy
download.sh: 19: Bad substitution
",danielchan-25,bash download.sh
140,Can't reproduce the results on Humaneval,"Hello, may I ask how you conducted testing on Humaneval? I attempted to test using the two methods you provided in your Hugging Face blog, including code completion and code infilling on Humaneval. However, I only achieved results of 22% and 25% on Instruct CodeLLama 7B, which is far from the reported 35%.

```
args.max_length = 1024

if args.task == 'code-completion':
    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)
    pipeline = transformers.pipeline(
        ""text-generation"",
        model=args.model_path,
        torch_dtype=torch.float16,
        device_map=""auto"",
    )
    for task_id in problems:
        prompt = problems[task_id]['question'] + '<FILL_ME>\n    return result'
        input_ids = tokenizer(prompt, return_tensors=""pt"")[""input_ids""].to(""cuda"")
        output = model.generate(input_ids, max_new_tokens=args.max_length,)
        output = output[0].to(""cpu"")
        filling = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)
        completion = prompt.replace(""<FILL_ME>"", filling)

elif args.task == 'code-infilling':
    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)
    model = transformers.AutoModelForCausalLM.from_pretrained(
        args.model_path,
        torch_dtype=torch.float16
    ).to(""cuda"")
    for task_id in problems:
        completion = pipeline(
            prompt,
            do_sample=True,
            temperature=0.2,
            top_p=0.9,
            num_return_sequences=1,
            eos_token_id=tokenizer.eos_token_id,
            max_length=args.max_length,
        )[0]['generated_text'].strip()

```",jgehring,"Hi @JulietLJY, I'm a bit confused by the code sample you provided: you're using infilling for completion and completion for infilling (and this time with sampling)? For testing on HumanEval I'd suggest you try the [Code Generation LM Evaluation Harness](https://github.com/bigcode-project/bigcode-evaluation-harness#code-generation-lm-evaluation-harness). There will be small differences in results due to slight differences in prompt formatting and inference code but you should get results that are fairly close to what we reported."
140,Can't reproduce the results on Humaneval,"Hello, may I ask how you conducted testing on Humaneval? I attempted to test using the two methods you provided in your Hugging Face blog, including code completion and code infilling on Humaneval. However, I only achieved results of 22% and 25% on Instruct CodeLLama 7B, which is far from the reported 35%.

```
args.max_length = 1024

if args.task == 'code-completion':
    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)
    pipeline = transformers.pipeline(
        ""text-generation"",
        model=args.model_path,
        torch_dtype=torch.float16,
        device_map=""auto"",
    )
    for task_id in problems:
        prompt = problems[task_id]['question'] + '<FILL_ME>\n    return result'
        input_ids = tokenizer(prompt, return_tensors=""pt"")[""input_ids""].to(""cuda"")
        output = model.generate(input_ids, max_new_tokens=args.max_length,)
        output = output[0].to(""cpu"")
        filling = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)
        completion = prompt.replace(""<FILL_ME>"", filling)

elif args.task == 'code-infilling':
    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)
    model = transformers.AutoModelForCausalLM.from_pretrained(
        args.model_path,
        torch_dtype=torch.float16
    ).to(""cuda"")
    for task_id in problems:
        completion = pipeline(
            prompt,
            do_sample=True,
            temperature=0.2,
            top_p=0.9,
            num_return_sequences=1,
            eos_token_id=tokenizer.eos_token_id,
            max_length=args.max_length,
        )[0]['generated_text'].strip()

```",JulietLJY,My issue has been effectively resolved. I appreciate your prompt response!
140,Can't reproduce the results on Humaneval,"Hello, may I ask how you conducted testing on Humaneval? I attempted to test using the two methods you provided in your Hugging Face blog, including code completion and code infilling on Humaneval. However, I only achieved results of 22% and 25% on Instruct CodeLLama 7B, which is far from the reported 35%.

```
args.max_length = 1024

if args.task == 'code-completion':
    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)
    pipeline = transformers.pipeline(
        ""text-generation"",
        model=args.model_path,
        torch_dtype=torch.float16,
        device_map=""auto"",
    )
    for task_id in problems:
        prompt = problems[task_id]['question'] + '<FILL_ME>\n    return result'
        input_ids = tokenizer(prompt, return_tensors=""pt"")[""input_ids""].to(""cuda"")
        output = model.generate(input_ids, max_new_tokens=args.max_length,)
        output = output[0].to(""cpu"")
        filling = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)
        completion = prompt.replace(""<FILL_ME>"", filling)

elif args.task == 'code-infilling':
    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)
    model = transformers.AutoModelForCausalLM.from_pretrained(
        args.model_path,
        torch_dtype=torch.float16
    ).to(""cuda"")
    for task_id in problems:
        completion = pipeline(
            prompt,
            do_sample=True,
            temperature=0.2,
            top_p=0.9,
            num_return_sequences=1,
            eos_token_id=tokenizer.eos_token_id,
            max_length=args.max_length,
        )[0]['generated_text'].strip()

```",harryhan618,"@JulietLJY Hi, may I ask a followup question? Have you reproduce the result of CodeLLama 7B pass@1 accuracy being 33.5%?
There is a (issue)[https://github.com/bigcode-project/bigcode-evaluation-harness/issues/220] in bigcode-evaluation-harness repo saying that he cannot reproduce either."
139,Combining instruction and infilling,"Figure 2 of the paper shows the training pipeline of all models. Each model undergoes infilling code training. For the instruct models additionally a final instruction fine-tuning is applied. This suggests that both tasks can be combined. However, the paper doesn't make this clear to me and the code examples also seem to only do one thing at a time.

- Is is possible to combine instructing and infilling in a single prompt?
- If yes, what is the correct prompt format?

I imagine something like

```
[INST][SYS] system [/SYS] instruction [/INST][PREFIX] prefix [SUFFIX] suffix [MID] 
```",jgehring,"Hi @kherud, you're right in that we didn't test this specifically but it'll be interesting to try for sure. You can follow the code in [generate.py](https://github.com/facebookresearch/codellama/blob/main/llama/generation.py) to retrieve the correct formatting, the system message should be formatted differently. You can use [`infilling_prompt_tokens()`](https://github.com/facebookresearch/codellama/blob/e66609cfbd73503ef25e597fd82c59084836155d/llama/generation.py#L396) to tokenize the non-instruction part. It might be worth testing if the system and instruction prompts could be part of the prefix as well."
139,Combining instruction and infilling,"Figure 2 of the paper shows the training pipeline of all models. Each model undergoes infilling code training. For the instruct models additionally a final instruction fine-tuning is applied. This suggests that both tasks can be combined. However, the paper doesn't make this clear to me and the code examples also seem to only do one thing at a time.

- Is is possible to combine instructing and infilling in a single prompt?
- If yes, what is the correct prompt format?

I imagine something like

```
[INST][SYS] system [/SYS] instruction [/INST][PREFIX] prefix [SUFFIX] suffix [MID] 
```",yujonglee,@kherud  Would you mind sharing your findings on this?
139,Combining instruction and infilling,"Figure 2 of the paper shows the training pipeline of all models. Each model undergoes infilling code training. For the instruct models additionally a final instruction fine-tuning is applied. This suggests that both tasks can be combined. However, the paper doesn't make this clear to me and the code examples also seem to only do one thing at a time.

- Is is possible to combine instructing and infilling in a single prompt?
- If yes, what is the correct prompt format?

I imagine something like

```
[INST][SYS] system [/SYS] instruction [/INST][PREFIX] prefix [SUFFIX] suffix [MID] 
```",jhcordeiro,@jgehring @kherud did you have a change to study this further? I'd love to read the results if so
137,Help Needed: Fine-tuning codellama-7b-Instruct Model for pinescript Programming language,"I'm having a problem with fine-tuning the codellama-7b-Instruct model for a programming language. The issue is that the model seems to focus too much on the new dataset , and its performance isn't great on new tasks. It's not just overfitting; sometimes, it doesn't do well on new tasks either.

For example:

Base model
```
User: Hey There! How are you
Model: I am good. How can I help ?
```

Finetuned model
```
User: Hey There! How are you
Model: Yes, I can fix your pinescript code. Provide me your issue?
```

I've tried increasing the number of training epochs to make sure it learns properly. I've also prepared my dataset carefully according to codellama's requirements. I used LORA and PEFT for finetuning. My dataset has 60,000 chat examples, each with 1000 tokens in the context. To make the model more robust, I overlapped the examples by 25%.

Here are my training settings:

```
Epochs: 15
Batch Size: 6
Gradient Accumulation Step: 2
Learning Rate: 4e-4
Warmup Ratio: 0.05

Lora r : 32
Lora alpha: 32
Lora dropout: 0.05
Target Modules: [""q_proj"", ""v_proj"", ""k_proj"", ""o_proj"", ""gate_proj"", ""up_proj"", ""down_proj"", ""lm_head""] (all linear layers are trainable)
```

Is there any way so I can merge some adapter layers and check its performance rather than merging all layers. I dont want to finetune again since it takes 10-15 days for the finetuning.

Any help or suggestion would highly be appreciated.",roshan-gopalakrishnan,@Humza1996 did you able to find a solution to this ?  
136,Update README.md,fix chat-completion hyperlink,facebook-github-bot,"Hi @chan27-2! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%23136). Thanks!"
136,Update README.md,fix chat-completion hyperlink,facebook-github-bot,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!
136,Update README.md,fix chat-completion hyperlink,jgehring,Link was updated in 1af62e1f43db1fa5140fa43cb828465a603a48f3
135,Are there instructions on how to uninstall and remove the downloaded models?,,balloonio,They are downloaded under the same directory. I believe you can simply delete the .pth files
135,Are there instructions on how to uninstall and remove the downloaded models?,,jgehring,"That's right, the you can remove the model directories. The download script does not install files in other locations."
134,Keep getting ERROR 403: Forbidden even when I used new link,"I removed all local files, re-cloned the repository, and requested a new download link, but when I run ./download.sh and entered the download link, chose the model, it still gave the ERROR 403: Forbidden. What should I do now? I copied what I have seen here.



Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 70B
Downloading LICENSE and Acceptable Usage Policy
--2023-09-27 21:44:52--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoieW9xMWo1Z2ZvMHlwOXRlOGtmM2VuMDJxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTU5NjIyMTN9fX1dfQ__&Signature=c0eH-sjrSLUcXtpO0QzraO-GsL%7ELrS9dPhNeM5sOrN9IDnN8O9b8NRZrNEvE-BqIqAv4trB0DzoB6WIQXfCviUg6iycMQPuGwPij-JQeZLzDLyO23nzn6p0p869SP6n8LkaX8Q9HpkPJevZBbdthuq69XDhY-oqMsro6kQgx6aMhQmkJyqmJaSWH6JllrIasBr-iNZFuir66X4nTNi1qJJ0hIEZylAgJi8Gjys6nzUcceyFP%7EMXQ08B%7EXPDejacoceE%7EzeyVgwFglscsmWnydxo7qjVNdF5uTHs6pw-2giYbihdsV9vsVEtt0AL447%7E4ldceObKGXBXvtcKIS61HoA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=852286889742198
Resolving download2.llamameta.net (download2.llamameta.net)... 216.137.39.97, 216.137.39.66, 216.137.39.67, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|216.137.39.97|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

--2023-09-27 21:44:52--  https://download2.llamameta.net/USE_POLICY.md?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoieW9xMWo1Z2ZvMHlwOXRlOGtmM2VuMDJxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTU5NjIyMTN9fX1dfQ__&Signature=c0eH-sjrSLUcXtpO0QzraO-GsL%7ELrS9dPhNeM5sOrN9IDnN8O9b8NRZrNEvE-BqIqAv4trB0DzoB6WIQXfCviUg6iycMQPuGwPij-JQeZLzDLyO23nzn6p0p869SP6n8LkaX8Q9HpkPJevZBbdthuq69XDhY-oqMsro6kQgx6aMhQmkJyqmJaSWH6JllrIasBr-iNZFuir66X4nTNi1qJJ0hIEZylAgJi8Gjys6nzUcceyFP%7EMXQ08B%7EXPDejacoceE%7EzeyVgwFglscsmWnydxo7qjVNdF5uTHs6pw-2giYbihdsV9vsVEtt0AL447%7E4ldceObKGXBXvtcKIS61HoA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=852286889742198
Resolving download2.llamameta.net (download2.llamameta.net)... 216.137.39.97, 216.137.39.66, 216.137.39.67, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|216.137.39.97|:443... connected.
HTTP request sent, awaiting response... 206 Partial Content
Length: 4790 (4.7K), 24 remaining [text/markdown]
Saving to: ‘./USE_POLICY.md’

./USE_POLICY.md               100%[+++++++++++++++++++++++++++++++++++++++++++++++++>]   4.68K  --.-KB/s    in 0s      

2023-09-27 21:44:52 (478 KB/s) - ‘./USE_POLICY.md’ saved [4790/4790]

Downloading tokenizer
--2023-09-27 21:44:52--  https://download2.llamameta.net/tokenizer.model?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoieW9xMWo1Z2ZvMHlwOXRlOGtmM2VuMDJxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTU5NjIyMTN9fX1dfQ__&Signature=c0eH-sjrSLUcXtpO0QzraO-GsL%7ELrS9dPhNeM5sOrN9IDnN8O9b8NRZrNEvE-BqIqAv4trB0DzoB6WIQXfCviUg6iycMQPuGwPij-JQeZLzDLyO23nzn6p0p869SP6n8LkaX8Q9HpkPJevZBbdthuq69XDhY-oqMsro6kQgx6aMhQmkJyqmJaSWH6JllrIasBr-iNZFuir66X4nTNi1qJJ0hIEZylAgJi8Gjys6nzUcceyFP%7EMXQ08B%7EXPDejacoceE%7EzeyVgwFglscsmWnydxo7qjVNdF5uTHs6pw-2giYbihdsV9vsVEtt0AL447%7E4ldceObKGXBXvtcKIS61HoA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=852286889742198
Resolving download2.llamameta.net (download2.llamameta.net)... 216.137.39.97, 216.137.39.66, 216.137.39.67, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|216.137.39.97|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-09-27 21:44:53 ERROR 403: Forbidden.

",samuelselvan,"@Capybara233 The link looks to be for codellama but you are using the download.sh from chat llama?
If yes could you try the download.sh from the codellama repo?"
134,Keep getting ERROR 403: Forbidden even when I used new link,"I removed all local files, re-cloned the repository, and requested a new download link, but when I run ./download.sh and entered the download link, chose the model, it still gave the ERROR 403: Forbidden. What should I do now? I copied what I have seen here.



Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 70B
Downloading LICENSE and Acceptable Usage Policy
--2023-09-27 21:44:52--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoieW9xMWo1Z2ZvMHlwOXRlOGtmM2VuMDJxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTU5NjIyMTN9fX1dfQ__&Signature=c0eH-sjrSLUcXtpO0QzraO-GsL%7ELrS9dPhNeM5sOrN9IDnN8O9b8NRZrNEvE-BqIqAv4trB0DzoB6WIQXfCviUg6iycMQPuGwPij-JQeZLzDLyO23nzn6p0p869SP6n8LkaX8Q9HpkPJevZBbdthuq69XDhY-oqMsro6kQgx6aMhQmkJyqmJaSWH6JllrIasBr-iNZFuir66X4nTNi1qJJ0hIEZylAgJi8Gjys6nzUcceyFP%7EMXQ08B%7EXPDejacoceE%7EzeyVgwFglscsmWnydxo7qjVNdF5uTHs6pw-2giYbihdsV9vsVEtt0AL447%7E4ldceObKGXBXvtcKIS61HoA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=852286889742198
Resolving download2.llamameta.net (download2.llamameta.net)... 216.137.39.97, 216.137.39.66, 216.137.39.67, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|216.137.39.97|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

--2023-09-27 21:44:52--  https://download2.llamameta.net/USE_POLICY.md?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoieW9xMWo1Z2ZvMHlwOXRlOGtmM2VuMDJxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTU5NjIyMTN9fX1dfQ__&Signature=c0eH-sjrSLUcXtpO0QzraO-GsL%7ELrS9dPhNeM5sOrN9IDnN8O9b8NRZrNEvE-BqIqAv4trB0DzoB6WIQXfCviUg6iycMQPuGwPij-JQeZLzDLyO23nzn6p0p869SP6n8LkaX8Q9HpkPJevZBbdthuq69XDhY-oqMsro6kQgx6aMhQmkJyqmJaSWH6JllrIasBr-iNZFuir66X4nTNi1qJJ0hIEZylAgJi8Gjys6nzUcceyFP%7EMXQ08B%7EXPDejacoceE%7EzeyVgwFglscsmWnydxo7qjVNdF5uTHs6pw-2giYbihdsV9vsVEtt0AL447%7E4ldceObKGXBXvtcKIS61HoA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=852286889742198
Resolving download2.llamameta.net (download2.llamameta.net)... 216.137.39.97, 216.137.39.66, 216.137.39.67, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|216.137.39.97|:443... connected.
HTTP request sent, awaiting response... 206 Partial Content
Length: 4790 (4.7K), 24 remaining [text/markdown]
Saving to: ‘./USE_POLICY.md’

./USE_POLICY.md               100%[+++++++++++++++++++++++++++++++++++++++++++++++++>]   4.68K  --.-KB/s    in 0s      

2023-09-27 21:44:52 (478 KB/s) - ‘./USE_POLICY.md’ saved [4790/4790]

Downloading tokenizer
--2023-09-27 21:44:52--  https://download2.llamameta.net/tokenizer.model?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoieW9xMWo1Z2ZvMHlwOXRlOGtmM2VuMDJxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTU5NjIyMTN9fX1dfQ__&Signature=c0eH-sjrSLUcXtpO0QzraO-GsL%7ELrS9dPhNeM5sOrN9IDnN8O9b8NRZrNEvE-BqIqAv4trB0DzoB6WIQXfCviUg6iycMQPuGwPij-JQeZLzDLyO23nzn6p0p869SP6n8LkaX8Q9HpkPJevZBbdthuq69XDhY-oqMsro6kQgx6aMhQmkJyqmJaSWH6JllrIasBr-iNZFuir66X4nTNi1qJJ0hIEZylAgJi8Gjys6nzUcceyFP%7EMXQ08B%7EXPDejacoceE%7EzeyVgwFglscsmWnydxo7qjVNdF5uTHs6pw-2giYbihdsV9vsVEtt0AL447%7E4ldceObKGXBXvtcKIS61HoA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=852286889742198
Resolving download2.llamameta.net (download2.llamameta.net)... 216.137.39.97, 216.137.39.66, 216.137.39.67, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|216.137.39.97|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-09-27 21:44:53 ERROR 403: Forbidden.

",Capybara233,"@samuelselvan  > @Capybara233 The link looks to be for codellama but you are using the download.sh from chat llama? If yes could you try the download.sh from the codellama repo?

Hi Thanks for your reply,

I just followed the instruction on https://github.com/facebookresearch/codellama/blob/main/README.md
How would I tell if I had used  the download.sh from chat llama? Thanks!"
134,Keep getting ERROR 403: Forbidden even when I used new link,"I removed all local files, re-cloned the repository, and requested a new download link, but when I run ./download.sh and entered the download link, chose the model, it still gave the ERROR 403: Forbidden. What should I do now? I copied what I have seen here.



Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 70B
Downloading LICENSE and Acceptable Usage Policy
--2023-09-27 21:44:52--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoieW9xMWo1Z2ZvMHlwOXRlOGtmM2VuMDJxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTU5NjIyMTN9fX1dfQ__&Signature=c0eH-sjrSLUcXtpO0QzraO-GsL%7ELrS9dPhNeM5sOrN9IDnN8O9b8NRZrNEvE-BqIqAv4trB0DzoB6WIQXfCviUg6iycMQPuGwPij-JQeZLzDLyO23nzn6p0p869SP6n8LkaX8Q9HpkPJevZBbdthuq69XDhY-oqMsro6kQgx6aMhQmkJyqmJaSWH6JllrIasBr-iNZFuir66X4nTNi1qJJ0hIEZylAgJi8Gjys6nzUcceyFP%7EMXQ08B%7EXPDejacoceE%7EzeyVgwFglscsmWnydxo7qjVNdF5uTHs6pw-2giYbihdsV9vsVEtt0AL447%7E4ldceObKGXBXvtcKIS61HoA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=852286889742198
Resolving download2.llamameta.net (download2.llamameta.net)... 216.137.39.97, 216.137.39.66, 216.137.39.67, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|216.137.39.97|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

--2023-09-27 21:44:52--  https://download2.llamameta.net/USE_POLICY.md?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoieW9xMWo1Z2ZvMHlwOXRlOGtmM2VuMDJxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTU5NjIyMTN9fX1dfQ__&Signature=c0eH-sjrSLUcXtpO0QzraO-GsL%7ELrS9dPhNeM5sOrN9IDnN8O9b8NRZrNEvE-BqIqAv4trB0DzoB6WIQXfCviUg6iycMQPuGwPij-JQeZLzDLyO23nzn6p0p869SP6n8LkaX8Q9HpkPJevZBbdthuq69XDhY-oqMsro6kQgx6aMhQmkJyqmJaSWH6JllrIasBr-iNZFuir66X4nTNi1qJJ0hIEZylAgJi8Gjys6nzUcceyFP%7EMXQ08B%7EXPDejacoceE%7EzeyVgwFglscsmWnydxo7qjVNdF5uTHs6pw-2giYbihdsV9vsVEtt0AL447%7E4ldceObKGXBXvtcKIS61HoA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=852286889742198
Resolving download2.llamameta.net (download2.llamameta.net)... 216.137.39.97, 216.137.39.66, 216.137.39.67, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|216.137.39.97|:443... connected.
HTTP request sent, awaiting response... 206 Partial Content
Length: 4790 (4.7K), 24 remaining [text/markdown]
Saving to: ‘./USE_POLICY.md’

./USE_POLICY.md               100%[+++++++++++++++++++++++++++++++++++++++++++++++++>]   4.68K  --.-KB/s    in 0s      

2023-09-27 21:44:52 (478 KB/s) - ‘./USE_POLICY.md’ saved [4790/4790]

Downloading tokenizer
--2023-09-27 21:44:52--  https://download2.llamameta.net/tokenizer.model?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoieW9xMWo1Z2ZvMHlwOXRlOGtmM2VuMDJxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTU5NjIyMTN9fX1dfQ__&Signature=c0eH-sjrSLUcXtpO0QzraO-GsL%7ELrS9dPhNeM5sOrN9IDnN8O9b8NRZrNEvE-BqIqAv4trB0DzoB6WIQXfCviUg6iycMQPuGwPij-JQeZLzDLyO23nzn6p0p869SP6n8LkaX8Q9HpkPJevZBbdthuq69XDhY-oqMsro6kQgx6aMhQmkJyqmJaSWH6JllrIasBr-iNZFuir66X4nTNi1qJJ0hIEZylAgJi8Gjys6nzUcceyFP%7EMXQ08B%7EXPDejacoceE%7EzeyVgwFglscsmWnydxo7qjVNdF5uTHs6pw-2giYbihdsV9vsVEtt0AL447%7E4ldceObKGXBXvtcKIS61HoA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=852286889742198
Resolving download2.llamameta.net (download2.llamameta.net)... 216.137.39.97, 216.137.39.66, 216.137.39.67, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|216.137.39.97|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-09-27 21:44:53 ERROR 403: Forbidden.

",treeDapps,"Hello,

I'm trying to use wget with my link received from Meta, but I'm still getting the 403 forbidden error. I am using the download.sh from CodeLlama.

Llama>wget ""https://download.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZ3ZuMjd4dzJqdGprMmt6OXBnZWg5bTRmIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NzkwNTI2M319fV19&Signature=EJdMsrm6hguRhN98W~W2~n2nTqblNYu61XqAI2Idy3fOmbLWVOtjABhsz7~MEaqJngqb7vbwV7dP~Cn9wNKch0e7vIMP~o7FnaAr1sF7~N-VmjgOIlCALPAB~FYzYGnpfUUnxLNw6AN6DyMey8EpoL6d1258ExFvadW-iuZK5GRKOqEU1eKV0b22CsJmALsSSV9jO~aYCWiaGpVXvo-Hsj8QGbN9F5fmfexFD2mdUGHUGuQ9g3lmYlwGMLJE9xvOa5eH1E0LK2b1XPov3UqNv16b4pgq7gXGWw1TQknlV8UjHk9tfKSR8BzBC7NAYa79xTncdJDZ~65ez4Xev~DjmQ__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1505957423569507""
Warning: wildcards not supported in HTTP.
The destination name is too long (650), reducing to 241
--2023-10-20 10:22:59--  https://download.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZ3ZuMjd4dzJqdGprMmt6OXBnZWg5bTRmIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NzkwNTI2M319fV19&Signature=EJdMsrm6hguRhN98W~W2~n2nTqblNYu61XqAI2Idy3fOmbLWVOtjABhsz7~MEaqJngqb7vbwV7dP~Cn9wNKch0e7vIMP~o7FnaAr1sF7~N-VmjgOIlCALPAB~FYzYGnpfUUnxLNw6AN6DyMey8EpoL6d1258ExFvadW-iuZK5GRKOqEU1eKV0b22CsJmALsSSV9jO~aYCWiaGpVXvo-Hsj8QGbN9F5fmfexFD2mdUGHUGuQ9g3lmYlwGMLJE9xvOa5eH1E0LK2b1XPov3UqNv16b4pgq7gXGWw1TQknlV8UjHk9tfKSR8BzBC7NAYa79xTncdJDZ~65ez4Xev~DjmQ__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1505957423569507
Resolving download.llamameta.net (download.llamameta.net)... 65.9.121.95, 65.9.121.26, 65.9.121.30, ...
Connecting to download.llamameta.net (download.llamameta.net)|65.9.121.95|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-20 10:22:59 ERROR 403: Forbidden."
134,Keep getting ERROR 403: Forbidden even when I used new link,"I removed all local files, re-cloned the repository, and requested a new download link, but when I run ./download.sh and entered the download link, chose the model, it still gave the ERROR 403: Forbidden. What should I do now? I copied what I have seen here.



Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 70B
Downloading LICENSE and Acceptable Usage Policy
--2023-09-27 21:44:52--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoieW9xMWo1Z2ZvMHlwOXRlOGtmM2VuMDJxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTU5NjIyMTN9fX1dfQ__&Signature=c0eH-sjrSLUcXtpO0QzraO-GsL%7ELrS9dPhNeM5sOrN9IDnN8O9b8NRZrNEvE-BqIqAv4trB0DzoB6WIQXfCviUg6iycMQPuGwPij-JQeZLzDLyO23nzn6p0p869SP6n8LkaX8Q9HpkPJevZBbdthuq69XDhY-oqMsro6kQgx6aMhQmkJyqmJaSWH6JllrIasBr-iNZFuir66X4nTNi1qJJ0hIEZylAgJi8Gjys6nzUcceyFP%7EMXQ08B%7EXPDejacoceE%7EzeyVgwFglscsmWnydxo7qjVNdF5uTHs6pw-2giYbihdsV9vsVEtt0AL447%7E4ldceObKGXBXvtcKIS61HoA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=852286889742198
Resolving download2.llamameta.net (download2.llamameta.net)... 216.137.39.97, 216.137.39.66, 216.137.39.67, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|216.137.39.97|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

--2023-09-27 21:44:52--  https://download2.llamameta.net/USE_POLICY.md?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoieW9xMWo1Z2ZvMHlwOXRlOGtmM2VuMDJxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTU5NjIyMTN9fX1dfQ__&Signature=c0eH-sjrSLUcXtpO0QzraO-GsL%7ELrS9dPhNeM5sOrN9IDnN8O9b8NRZrNEvE-BqIqAv4trB0DzoB6WIQXfCviUg6iycMQPuGwPij-JQeZLzDLyO23nzn6p0p869SP6n8LkaX8Q9HpkPJevZBbdthuq69XDhY-oqMsro6kQgx6aMhQmkJyqmJaSWH6JllrIasBr-iNZFuir66X4nTNi1qJJ0hIEZylAgJi8Gjys6nzUcceyFP%7EMXQ08B%7EXPDejacoceE%7EzeyVgwFglscsmWnydxo7qjVNdF5uTHs6pw-2giYbihdsV9vsVEtt0AL447%7E4ldceObKGXBXvtcKIS61HoA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=852286889742198
Resolving download2.llamameta.net (download2.llamameta.net)... 216.137.39.97, 216.137.39.66, 216.137.39.67, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|216.137.39.97|:443... connected.
HTTP request sent, awaiting response... 206 Partial Content
Length: 4790 (4.7K), 24 remaining [text/markdown]
Saving to: ‘./USE_POLICY.md’

./USE_POLICY.md               100%[+++++++++++++++++++++++++++++++++++++++++++++++++>]   4.68K  --.-KB/s    in 0s      

2023-09-27 21:44:52 (478 KB/s) - ‘./USE_POLICY.md’ saved [4790/4790]

Downloading tokenizer
--2023-09-27 21:44:52--  https://download2.llamameta.net/tokenizer.model?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoieW9xMWo1Z2ZvMHlwOXRlOGtmM2VuMDJxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTU5NjIyMTN9fX1dfQ__&Signature=c0eH-sjrSLUcXtpO0QzraO-GsL%7ELrS9dPhNeM5sOrN9IDnN8O9b8NRZrNEvE-BqIqAv4trB0DzoB6WIQXfCviUg6iycMQPuGwPij-JQeZLzDLyO23nzn6p0p869SP6n8LkaX8Q9HpkPJevZBbdthuq69XDhY-oqMsro6kQgx6aMhQmkJyqmJaSWH6JllrIasBr-iNZFuir66X4nTNi1qJJ0hIEZylAgJi8Gjys6nzUcceyFP%7EMXQ08B%7EXPDejacoceE%7EzeyVgwFglscsmWnydxo7qjVNdF5uTHs6pw-2giYbihdsV9vsVEtt0AL447%7E4ldceObKGXBXvtcKIS61HoA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=852286889742198
Resolving download2.llamameta.net (download2.llamameta.net)... 216.137.39.97, 216.137.39.66, 216.137.39.67, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|216.137.39.97|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-09-27 21:44:53 ERROR 403: Forbidden.

",karamvirs,"@treeDapps I was getting the same issue.
I had requested access to Chat models but was using the code Llama repo.
Use this one and see if it works for you:
https://github.com/facebookresearch/llama

However if you are trying to download the code models, then make sure when requesting the link you selected the code models and not chat, and then use the code repo."
130,Is there any way to create embeddings?,Is there any way to create embeddings with code llama as the base model like OpenAI embedding endpoints?,SaiAvinash2002,"Hey, I'm looking for the embeddings of llama at the output layer.
If you are able to figure out how to get embeddings please do help me dude!"
128,"Torchrun returns ""Failed to create process"" error","Hi,

I'm new to Code Llama, and I'm facing a problem after having cloned the repository and after having downloaded a model.

I've followed the steps described in the documentation and, when I execute the torchrun command, the ""failed to create process"" error is returned. 

It seems to me I didn't miss any step. I attach some print screens:

**First print screen**: After having cloned the repository, I try to execute the commands described in the documentation
<img width=""948"" alt=""codellama-bash"" src=""https://github.com/facebookresearch/codellama/assets/143533030/cfc1b166-2d4a-4737-a230-e4327367410f"">

**Second and third print screens**: The structure of my folders

<img width=""524"" alt=""codellama-folder"" src=""https://github.com/facebookresearch/codellama/assets/143533030/453ebe51-1987-4eda-b7af-7f0ea79015ad"">

<img width=""274"" alt=""codellama-folder-model"" src=""https://github.com/facebookresearch/codellama/assets/143533030/68355689-706d-4884-aa51-da9fd7cd09de"">

It's been two days since I'm trying to understand how to fix this problem.

Could someone help me?

Thank you",UyCode,same issue with mine
126,"RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!","Changing `model.py` like this can fix this problem, maybe there is a better solution:

![image](https://github.com/facebookresearch/codellama/assets/13199771/c2c8a213-a262-4c78-8d41-5488e9a587c1)
",mpu,Should be fixed by PR #127. Thanks for flagging it.
123,Setting OMP_NUM_THREADS environment variable for each process to be 1 in default？," torchrun --nproc_per_node 2 example_infilling.py --ckpt_dir CodeLlama-13b-Python/ --tokenizer_path CodeLlama-13b-Python/tokenizer.model --max_seq_len 192 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.

",LinJia21,and I set OMP_NUM_THREADS=1 torchrun --nproc_per_node 2 example_infilling.py --ckpt_dir CodeLlama-13b-Python/ --tokenizer_path CodeLlama-13b-Python/tokenizer.model --max_seq_len 192 --max_batch_size 4.
123,Setting OMP_NUM_THREADS environment variable for each process to be 1 in default？," torchrun --nproc_per_node 2 example_infilling.py --ckpt_dir CodeLlama-13b-Python/ --tokenizer_path CodeLlama-13b-Python/tokenizer.model --max_seq_len 192 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.

",LinJia21,"RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 10621 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 10622) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/home/lin/.local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/lin/.local/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/lin/.local/lib/python3.8/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/lin/.local/lib/python3.8/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/lin/.local/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/lin/.local/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:"
123,Setting OMP_NUM_THREADS environment variable for each process to be 1 in default？," torchrun --nproc_per_node 2 example_infilling.py --ckpt_dir CodeLlama-13b-Python/ --tokenizer_path CodeLlama-13b-Python/tokenizer.model --max_seq_len 192 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.

",LinJia21,"set below to the run demo.py :
local_rank = 0
torch.cuda.set_device(local_rank)
also error:RuntimeError: CUDA error: invalid device ordinal"
123,Setting OMP_NUM_THREADS environment variable for each process to be 1 in default？," torchrun --nproc_per_node 2 example_infilling.py --ckpt_dir CodeLlama-13b-Python/ --tokenizer_path CodeLlama-13b-Python/tokenizer.model --max_seq_len 192 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.

",mpu,"Hello LinJia21,

your system must have multiple GPUs to run models >= 13b."
122,Please add a simple gradio interface that supports quantization,"We need a simple Gradio interface that support optimizations

So that models can be run on 12 GB or 24 GB cards ",mpu,"Thanks for your suggestion.

It is however stated in [CONTRIBUTING.md](../blob/main/CONTRIBUTING.md) that we do not plan to add features in this repository."
121,Installation issues on Windows,"I installed Conda on Windows 10, installed PyTorch and cudatools packages and cloned the repo.
I also installed menpo wget package and m2-base package for linux commands like bash.
However when I run the download.sh script, I get:

Downloading LICENSE and Acceptable Usage Policy
download.sh: line 18: wget: command not found
download.sh: line 19: wget: command not found
Downloading CodeLlama-7b-Instruct
download.sh: line 53: wget: command not found
download.sh: line 56: wget: command not found
download.sh: line 57: wget: command not found
download.sh: line 58: wget: command not found
Checking checksums
md5sum: checklist.chk: No such file or directory

Why can't it find wget while it's installed?
",william2313,"Never mind, I have installed it on Ubuntu instead."
120,如何通过openai的方式启动codellama呢,这样我的代码，就可以openai的方式调用codellama了,linyalan2000,Maybe you can find it in [api-for-open-llm](https://github.com/xusenlinzy/api-for-open-llm/tree/master).
119,Error when download ,"after I run `bash download.sh` then it's show this error

> Checking checksums
> parseopts.c:76: setup_check: fopen 'checklist.chk': No such file or directory

Please let me know how to process this problem",wuhongsheng,+1
119,Error when download ,"after I run `bash download.sh` then it's show this error

> Checking checksums
> parseopts.c:76: setup_check: fopen 'checklist.chk': No such file or directory

Please let me know how to process this problem",SNDHRSWRN,"me too having problem 
"
118,Can't run the example,"I am getting the following error when I tried to run the example with ""torchrun  example_completion.py     --ckpt_dir CodeLlama-7b/     --tokenizer_path CodeLlama-7b/tokenizer.model     --max_seq_len 128 --max_batch_size 4"".

I have PyTorch 2.0.1 installed on WSL2 using miniconda. Unlike other issues where ""torch.distributed.elastic.multiprocessing.api:failed"" with an existcode 1, I am getting SIGKILL. What could be the reason?

> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 0 (pid: 29066) of binary: /home/zlu/miniconda3/envs/torch/bin/python
Traceback (most recent call last):
  File ""/home/zlu/miniconda3/envs/torch/bin/torchrun"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
example_completion.py FAILED
------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-16_15:03:11
  host      : Lenovo21.
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 29066)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 29066
======================================================",ashantanu,facing the same issue
118,Can't run the example,"I am getting the following error when I tried to run the example with ""torchrun  example_completion.py     --ckpt_dir CodeLlama-7b/     --tokenizer_path CodeLlama-7b/tokenizer.model     --max_seq_len 128 --max_batch_size 4"".

I have PyTorch 2.0.1 installed on WSL2 using miniconda. Unlike other issues where ""torch.distributed.elastic.multiprocessing.api:failed"" with an existcode 1, I am getting SIGKILL. What could be the reason?

> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 0 (pid: 29066) of binary: /home/zlu/miniconda3/envs/torch/bin/python
Traceback (most recent call last):
  File ""/home/zlu/miniconda3/envs/torch/bin/torchrun"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
example_completion.py FAILED
------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-16_15:03:11
  host      : Lenovo21.
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 29066)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 29066
======================================================",zhenganglu,Same issue when running using PyTorch nightly preview with cuda 12.1 in another conda environment.
118,Can't run the example,"I am getting the following error when I tried to run the example with ""torchrun  example_completion.py     --ckpt_dir CodeLlama-7b/     --tokenizer_path CodeLlama-7b/tokenizer.model     --max_seq_len 128 --max_batch_size 4"".

I have PyTorch 2.0.1 installed on WSL2 using miniconda. Unlike other issues where ""torch.distributed.elastic.multiprocessing.api:failed"" with an existcode 1, I am getting SIGKILL. What could be the reason?

> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 0 (pid: 29066) of binary: /home/zlu/miniconda3/envs/torch/bin/python
Traceback (most recent call last):
  File ""/home/zlu/miniconda3/envs/torch/bin/torchrun"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
example_completion.py FAILED
------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-16_15:03:11
  host      : Lenovo21.
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 29066)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 29066
======================================================",CodingFlow,I'm having the same issue and I suspect it is because my machine ran out of ram memory per [this comment](https://github.com/facebookresearch/codellama/issues/30#issuecomment-1694332808).
118,Can't run the example,"I am getting the following error when I tried to run the example with ""torchrun  example_completion.py     --ckpt_dir CodeLlama-7b/     --tokenizer_path CodeLlama-7b/tokenizer.model     --max_seq_len 128 --max_batch_size 4"".

I have PyTorch 2.0.1 installed on WSL2 using miniconda. Unlike other issues where ""torch.distributed.elastic.multiprocessing.api:failed"" with an existcode 1, I am getting SIGKILL. What could be the reason?

> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 0 (pid: 29066) of binary: /home/zlu/miniconda3/envs/torch/bin/python
Traceback (most recent call last):
  File ""/home/zlu/miniconda3/envs/torch/bin/torchrun"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
example_completion.py FAILED
------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-16_15:03:11
  host      : Lenovo21.
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 29066)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 29066
======================================================",zhenganglu,It looks like both the VM and GPU needs 16 GB of memory to run the example.
117,"what‘s the difference among 7b,7b-python and 7b-Instruct","as describe in titlle,much appreciate",GaganHonor,"The difference among 7b, 7b-python, and 7b-Instruct in the context of the CodeLlama project on GitHub is as follows:

7b: The 7b model refers to the base model of CodeLlama. It is trained on sequences of 16k tokens and shows improvements on inputs with up to 100k tokens.

7b-python: The 7b-python model is a variant of the 7b model that specifically focuses on Python code. It is trained on Python code sequences and is designed to provide better performance and accuracy when working with Python code.

7b-Instruct: The 7b-Instruct model is another variant of the 7b model. It is trained on a specific dataset called Code Llama - Instruct, which is designed to provide instructions for code generation. This model is useful when you need to generate code based on specific instructions or requirements.

These models are part of the CodeLlama project developed by Facebook Research. You can find more information about these models on the GitHub page you provided: CodeLlama GitHub.









source :  internet sir 😊🤖"
116,The difference between the playground and the offline model,"I cannot reproduce the equivalent output using the offline model (codellama/CodeLlama-13b-Instruct-hf, https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf)

Here are some offline examples. 

![截屏2023-09-14 23 18 11](https://github.com/facebookresearch/codellama/assets/22885358/c8776063-2558-4ae7-a91d-05499156c0f6)


The Playground example (using https://huggingface.co/spaces/codellama/codellama-13b-chat)
![截屏2023-09-14 22 44 28](https://github.com/facebookresearch/codellama/assets/22885358/43261440-e281-4646-b0fd-5f96e65ae535)

The performance of the offline huggingface model is poor, which is worse than that of playground. Is there something wrong with hyperparameter settings? 

Is the model used in playground different from the offline download model?

Huggingface code: 
```python
tokenizer = CodeLlamaTokenizer.from_pretrained(path_model_id) 
   model = LlamaForCausalLM.from_pretrained( 
    path_model_id, 
    torch_dtype=torch.float16 
).to(""cuda:0"")
```
```python 
user = ""Write a function that computes the set of sums of all contiguous sublists of a given list.""
prompt = f""<s>[INST]{user.strip()} [/INST]""
input_ids = tokenizer(prompt, return_tensors=""pt"")[""input_ids""].to(""cuda:0"")
output = model.generate(
    input_ids,
    max_new_tokens=2000,
    # top_p=0.9,
    # temperature=0.1,
    # # top_k=10,
    # do_sample=True,
    # num_beams=1,
    # repetition_penalty=1.1,
    repetition_penalty=1.05,
    # eos_token_id=tokenizer.eos_token_id,
    # pad_token_id=tokenizer.pad_token_id
)
output = output[0].to(""cpu"")

filling = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)
print(filling)
```
















",HongYukai,I meet the same problem when using codellama/CodeLlama-34b-Instruct-hf. Who can resolve this issue please?
116,The difference between the playground and the offline model,"I cannot reproduce the equivalent output using the offline model (codellama/CodeLlama-13b-Instruct-hf, https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf)

Here are some offline examples. 

![截屏2023-09-14 23 18 11](https://github.com/facebookresearch/codellama/assets/22885358/c8776063-2558-4ae7-a91d-05499156c0f6)


The Playground example (using https://huggingface.co/spaces/codellama/codellama-13b-chat)
![截屏2023-09-14 22 44 28](https://github.com/facebookresearch/codellama/assets/22885358/43261440-e281-4646-b0fd-5f96e65ae535)

The performance of the offline huggingface model is poor, which is worse than that of playground. Is there something wrong with hyperparameter settings? 

Is the model used in playground different from the offline download model?

Huggingface code: 
```python
tokenizer = CodeLlamaTokenizer.from_pretrained(path_model_id) 
   model = LlamaForCausalLM.from_pretrained( 
    path_model_id, 
    torch_dtype=torch.float16 
).to(""cuda:0"")
```
```python 
user = ""Write a function that computes the set of sums of all contiguous sublists of a given list.""
prompt = f""<s>[INST]{user.strip()} [/INST]""
input_ids = tokenizer(prompt, return_tensors=""pt"")[""input_ids""].to(""cuda:0"")
output = model.generate(
    input_ids,
    max_new_tokens=2000,
    # top_p=0.9,
    # temperature=0.1,
    # # top_k=10,
    # do_sample=True,
    # num_beams=1,
    # repetition_penalty=1.1,
    repetition_penalty=1.05,
    # eos_token_id=tokenizer.eos_token_id,
    # pad_token_id=tokenizer.pad_token_id
)
output = output[0].to(""cpu"")

filling = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)
print(filling)
```
















",faabian,"Thank you for reporting @cocovoc and @HongYukai !
As this issue pertains to the HuggingFace implementation of our models, please direct your question to https://github.com/huggingface/transformers/

That said, note you can view the source code of the playground example here https://huggingface.co/spaces/codellama/codellama-13b-chat/tree/main and compare, and that such issues are usually related to the exact formatting and tokenizations. In particular, you may want to check how HF handles `<s>`."
116,The difference between the playground and the offline model,"I cannot reproduce the equivalent output using the offline model (codellama/CodeLlama-13b-Instruct-hf, https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf)

Here are some offline examples. 

![截屏2023-09-14 23 18 11](https://github.com/facebookresearch/codellama/assets/22885358/c8776063-2558-4ae7-a91d-05499156c0f6)


The Playground example (using https://huggingface.co/spaces/codellama/codellama-13b-chat)
![截屏2023-09-14 22 44 28](https://github.com/facebookresearch/codellama/assets/22885358/43261440-e281-4646-b0fd-5f96e65ae535)

The performance of the offline huggingface model is poor, which is worse than that of playground. Is there something wrong with hyperparameter settings? 

Is the model used in playground different from the offline download model?

Huggingface code: 
```python
tokenizer = CodeLlamaTokenizer.from_pretrained(path_model_id) 
   model = LlamaForCausalLM.from_pretrained( 
    path_model_id, 
    torch_dtype=torch.float16 
).to(""cuda:0"")
```
```python 
user = ""Write a function that computes the set of sums of all contiguous sublists of a given list.""
prompt = f""<s>[INST]{user.strip()} [/INST]""
input_ids = tokenizer(prompt, return_tensors=""pt"")[""input_ids""].to(""cuda:0"")
output = model.generate(
    input_ids,
    max_new_tokens=2000,
    # top_p=0.9,
    # temperature=0.1,
    # # top_k=10,
    # do_sample=True,
    # num_beams=1,
    # repetition_penalty=1.1,
    repetition_penalty=1.05,
    # eos_token_id=tokenizer.eos_token_id,
    # pad_token_id=tokenizer.pad_token_id
)
output = output[0].to(""cpu"")

filling = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)
print(filling)
```
















",for-just-we,"> I cannot reproduce the equivalent output using the offline model (codellama/CodeLlama-13b-Instruct-hf, https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf)
> 
> Here are some offline examples.
> 
> ![截屏2023-09-14 23 18 11](https://user-images.githubusercontent.com/22885358/268023932-c8776063-2558-4ae7-a91d-05499156c0f6.png)
> 
> The Playground example (using https://huggingface.co/spaces/codellama/codellama-13b-chat) ![截屏2023-09-14 22 44 28](https://user-images.githubusercontent.com/22885358/268013750-43261440-e281-4646-b0fd-5f96e65ae535.png)
> 
> The performance of the offline huggingface model is poor, which is worse than that of playground. Is there something wrong with hyperparameter settings?
> 
> Is the model used in playground different from the offline download model?
> 
> Huggingface code:
> 
> ```python
> tokenizer = CodeLlamaTokenizer.from_pretrained(path_model_id) 
>    model = LlamaForCausalLM.from_pretrained( 
>     path_model_id, 
>     torch_dtype=torch.float16 
> ).to(""cuda:0"")
> ```
> 
> ```python
> user = ""Write a function that computes the set of sums of all contiguous sublists of a given list.""
> prompt = f""<s>[INST]{user.strip()} [/INST]""
> input_ids = tokenizer(prompt, return_tensors=""pt"")[""input_ids""].to(""cuda:0"")
> output = model.generate(
>     input_ids,
>     max_new_tokens=2000,
>     # top_p=0.9,
>     # temperature=0.1,
>     # # top_k=10,
>     # do_sample=True,
>     # num_beams=1,
>     # repetition_penalty=1.1,
>     repetition_penalty=1.05,
>     # eos_token_id=tokenizer.eos_token_id,
>     # pad_token_id=tokenizer.pad_token_id
> )
> output = output[0].to(""cpu"")
> 
> filling = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)
> print(filling)
> ```

I also notice in my task, I locally run the huggingface model of 7b-Instruct,13b-Instruct and original format of 7b-Instruct,13b-Instruct. I found the performance of huggingface model is worse than original format model, like be got IQ down, I don't know why"
115, Memory Usage Spike During Model Saving in Training Pipeline,"I have a question regarding my fine-tuning pipeline, specifically concerning a memory usage spike when the model saves checkpoint during the training step. This cause sudden CUDA Memory error. 

I would like to provide the following information, including GPU usage logs and code snippets for reference:

```
GPU - Used: 12.2GB, Free: 11.5GB
GPU - Used: 12.3GB, Free: 11.4GB
GPU - Used: 12.3GB, Free: 11.4GB
GPU - Used: 22.2GB, Free: 1.5GB
GPU - Used: 18.8GB, Free: 4.8GB
GPU - Used: 18.8GB, Free: 4.8GB
GPU - Used: 18.8GB, Free: 4.8GB
GPU - Used: 22.2GB, Free: 1.5GB
GPU - Used: 23.0GB, Free: 0.7GB
GPU - Used: 23.3GB, Free: 0.4GB
GPU - Used: 23.3GB, Free: 0.4GB
GPU - Used: 23.3GB, Free: 0.4GB
GPU - Used: 23.6GB, Free: 0.1GB
GPU - Used: 19.9GB, Free: 3.8GB
GPU - Used: 19.9GB, Free: 3.8GB
GPU - Used: 22.1GB, Free: 1.6GB
```

```
import json
import torch
import pandas as pd
import datasets
from peft import LoraConfig,PeftModel
from transformers import (AutoModelForCausalLM,AutoTokenizer,TrainingArguments,BitsAndBytesConfig)
import transformers
from trl import SFTTrainer
from training_args import *
import os

import logging
import sys

output_dir = ""CodeLlama-7b-Instruct-HF-results/trl-trainer/Complete_dataset_training/""


if not os.path.exists(output_dir):
    # If the directory doesn't exist, create it
    os.makedirs(output_dir)
    print(f""Directory '{output_dir}' created."")
else:
    print(f""Directory '{output_dir}' already exists."")


# Create a logger instance
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Create a formatter with the desired format
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

# Create a stream handler to output log messages to the console
stream_handler = logging.StreamHandler(sys.stdout)
stream_handler.setFormatter(formatter)
logger.addHandler(stream_handler)

# Create a file handler to log messages to a file
file_handler = logging.FileHandler(f'{output_dir}/trl-trainer-codellama.txt', encoding='utf-8')  # Specify the file name here
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)
console_handler = logging.StreamHandler(stream=sys.stdout)


# DEVICE = ""cuda:0"" if torch.cuda.is_available() else 'cpu'



MODEL_NAME = ""CodeLlama-7b-Instruct-HF/""

# loading dataset
dataset = datasets.load_from_disk(""../dataset/complete_overlapped_chat_format_171781/"")
# loading model
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,use_safetensors=True,load_in_8bit=True,trust_remote_code=True,device_map='auto')
# loading tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_special_tokens=False, add_eos_token=False, add_bos_token=False)
# LORA Configuration
peft_config = LoraConfig(
    lora_alpha=32,
    lora_dropout=0.05,
    r = 12,
    bias=""none"",
    task_type = ""CAUSAL_LM"",
    target_modules = [""q_proj"", ""v_proj"",""k_proj"",""o_proj"",""gate_proj"",""up_proj"",""down_proj"",""lm_head""]
)



training_arguments = TrainingArguments(
    per_device_train_batch_size=8,
    gradient_accumulation_steps=4,
    optim=""paged_adamw_32bit"",
    learning_rate=4e-4,
    fp16=True,
    max_grad_norm=0.3,
    num_train_epochs=3,
    warmup_ratio=0.05,
    logging_steps=5,
    save_total_limit=5,
    save_strategy=""steps"",
    save_steps=1,
    group_by_length=True,
    output_dir=output_dir,
    report_to=""tensorboard"",
    save_safetensors=True,
    lr_scheduler_type=""cosine"",
    seed=42)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field=""text"",
    max_seq_length=4096,
    tokenizer=tokenizer,
    args=training_arguments,
)

trainer.tokenizer.pad_token = False
trainer.tokenizer.pad_token

try:
    trainer.train()
except Exception as e:
    logger.error(f""Error in Logs due to {e}"")
```
",GaganHonor,"are GPU drivers up to date ?

your code looks reasonable, still you can try -- 

```py
training_arguments = TrainingArguments(
    per_device_train_batch_size=4,  # Reduced batch size
    gradient_accumulation_steps=8,  # Increased gradient accumulation steps
    optim=""paged_adamw_32bit"",
    learning_rate=4e-4,
    fp16=True,
    max_grad_norm=0.3,
    num_train_epochs=3,
    warmup_ratio=0.05,
    logging_steps=5,
    save_total_limit=5,
    save_strategy=""steps"",
    save_steps=1,
    group_by_length=True,
    output_dir=output_dir,
    report_to=""tensorboard"",
    save_safetensors=True,
    lr_scheduler_type=""cosine"",
    seed=42
)
```"
114,"Support code review, comment filling, bug modification","Does Codellama support code review, comment filling, bug fixing? Or can only do code filling and code generation？",GaganHonor,"Code Llama can support code review, comment filling, bug fixing, as well as code filling and code generation."
114,"Support code review, comment filling, bug modification","Does Codellama support code review, comment filling, bug fixing? Or can only do code filling and code generation？",tomisacat,"If you're open to use [Ollama](https://github.com/ollama/ollama), you may check my GitHub Action as an alternative: [ai_code_review](https://github.com/tomisacat/ai_code_review)"
113,403 error: forbidden for all other files besides `consolidated.00.pth`,"When running the `bash download.sh` and pasting in the signed URL - it downloads only a single file with no problem:

```
--2023-09-12 14:29:39--  https://download2.llamameta.net/USE_POLICY.md?Policy=xxxxxxx
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.75, 65.8.164.24, 65.8.164.126, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.75|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-09-12 14:29:40 ERROR 403: Forbidden.

Downloading CodeLlama-13b-Python
--2023-09-12 14:29:40--  https://download2.llamameta.net/CodeLlama-13b-Python/consolidated.00.pth?Policy=xxxxx
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.75, 65.8.164.24, 65.8.164.126, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.75|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 13016329643 (12G) [binary/octet-stream]
Saving to: ‘./CodeLlama-13b-Python/consolidated.00.pth’

```

It downloads the `consolidated.0*.pth` files just fine, but none of the other json files can be downloaded because of this forbidden error. 

Yes, I am doing the download within the 24-hour window (as you can see the consolidated files are downloading). I just received the email like 10 min ago and I am having this issue.  
Please advise, thanks!",reyfuu,me to iam having this issue. this is my second time submit form to meta
113,403 error: forbidden for all other files besides `consolidated.00.pth`,"When running the `bash download.sh` and pasting in the signed URL - it downloads only a single file with no problem:

```
--2023-09-12 14:29:39--  https://download2.llamameta.net/USE_POLICY.md?Policy=xxxxxxx
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.75, 65.8.164.24, 65.8.164.126, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.75|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-09-12 14:29:40 ERROR 403: Forbidden.

Downloading CodeLlama-13b-Python
--2023-09-12 14:29:40--  https://download2.llamameta.net/CodeLlama-13b-Python/consolidated.00.pth?Policy=xxxxx
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.75, 65.8.164.24, 65.8.164.126, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.75|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 13016329643 (12G) [binary/octet-stream]
Saving to: ‘./CodeLlama-13b-Python/consolidated.00.pth’

```

It downloads the `consolidated.0*.pth` files just fine, but none of the other json files can be downloaded because of this forbidden error. 

Yes, I am doing the download within the 24-hour window (as you can see the consolidated files are downloading). I just received the email like 10 min ago and I am having this issue.  
Please advise, thanks!",cmosguy,"> me to iam having this issue. this is my second time submit form to meta

Hey @reyfuu, did you ever figure out a solution?"
112,Batching generates broken answers,"There are multiple posts on the internet about llama2 models generating bad output when running more than one instruction using the batch option.
I can confirm that this true on all llama2 and llamacode models.  
One instruction works as expected, but 2 instructions make the model go crazy and output junk. Combined instructions length is within the max_seq_len, so there is no truncation...
It seems that the model becomes ""less smart"" when batching.

The question is why and how to fix?

```
    dialogs = [
        [          
              {
                ""role"": ""system"",
                ""content"": """"""for each address in the following text return a json object [{\""""t\"""":street,\""""c\"""":city,\""""s\"""":state,\""""d\"""":country}]""""""
},
            {""role"": ""user"", ""content"":text}],
            
             [          
              {
                ""role"": ""system"",
                ""content"": """"""for each address in the following text return a json object[{\""""t\"""":street,\""""c\"""":city,\""""s\"""":state,\""""d\"""":country}]""""""
},
            {""role"": ""user"", ""content"":text}]
    ]

    
    results = generator.chat_completion(
        dialogs,  
        max_gen_len=max_gen_len,
        temperature=temperature,
        top_p=top_p,
    )
```",GaganHonor,"there is no specific fix available for this problem at the moment.

 someone op will comment here soon 🤖"
112,Batching generates broken answers,"There are multiple posts on the internet about llama2 models generating bad output when running more than one instruction using the batch option.
I can confirm that this true on all llama2 and llamacode models.  
One instruction works as expected, but 2 instructions make the model go crazy and output junk. Combined instructions length is within the max_seq_len, so there is no truncation...
It seems that the model becomes ""less smart"" when batching.

The question is why and how to fix?

```
    dialogs = [
        [          
              {
                ""role"": ""system"",
                ""content"": """"""for each address in the following text return a json object [{\""""t\"""":street,\""""c\"""":city,\""""s\"""":state,\""""d\"""":country}]""""""
},
            {""role"": ""user"", ""content"":text}],
            
             [          
              {
                ""role"": ""system"",
                ""content"": """"""for each address in the following text return a json object[{\""""t\"""":street,\""""c\"""":city,\""""s\"""":state,\""""d\"""":country}]""""""
},
            {""role"": ""user"", ""content"":text}]
    ]

    
    results = generator.chat_completion(
        dialogs,  
        max_gen_len=max_gen_len,
        temperature=temperature,
        top_p=top_p,
    )
```",99991,"I can not reproduce your issue. It seems to work as expected, see below.

#### Code

```python
from typing import Optional
import fire
from llama import Llama

def main(
    ckpt_dir: str,
    tokenizer_path: str,
    temperature: float = 0.2,
    top_p: float = 0.95,
    max_seq_len: int = 512,
    max_batch_size: int = 8,
    max_gen_len: Optional[int] = None,
):
    generator = Llama.build(
        ckpt_dir=ckpt_dir,
        tokenizer_path=tokenizer_path,
        max_seq_len=max_seq_len,
        max_batch_size=max_batch_size,
    )

    text = ""The street is 068 Angelina Walks in West Hayfield of Virginia, 63622.""

    instructions = [
        [
            {
                ""role"": ""system"",
                ""content"": """"""for each address in the following text return a json object [{\""""t\"""":street,\""""c\"""":city,\""""s\"""":state,\""""d\"""":country}]""""""
            },
            {
                ""role"": ""user"",
                ""content"": text
            }
        ],
    ] * 3 # same instruction 3 times

    results = generator.chat_completion(
        instructions,
        max_gen_len=max_gen_len,
        temperature=temperature,
        top_p=top_p,
    )

    for result in results:
        print(result['generation']['role'].capitalize(), ""says:\n"")
        print(result['generation']['content'])
        print(""\n==================================\n"")

if __name__ == ""__main__"":
    fire.Fire(main)
```

#### Output

```bash
$ torchrun --nproc_per_node 1 my_main.py     --ckpt_dir CodeLlama-7b-Instruct/     --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Loaded in 11.62 seconds
Assistant says:

 {
""t"": ""068 Angelina Walks"",
""c"": ""West Hayfield"",
""s"": ""Virginia"",
""d"": ""USA""
}

==================================

Assistant says:

 {
""t"": ""068 Angelina Walks"",
""c"": ""West Hayfield"",
""s"": ""Virginia"",
""d"": ""USA""
}

==================================

Assistant says:

 {
""t"": ""068 Angelina Walks"",
""c"": ""West Hayfield"",
""s"": ""Virginia"",
""d"": ""USA""
}

==================================
```"
109,Unable to get complete output and seems pending while running 13b-instruct," I cloned and deployed the project locally from https://huggingface.co/spaces/codellama/codellama-13b-chat/tree/main. But after I run this 13b-instruct model,I find there are two issues:

1. model always doesn't provide me complete answer.I try to track the ouput of model and find that it seems to repeat to print empty string until length of output exceeds max_new_token like this:
<img width=""642"" alt=""no complete answer"" src=""https://github.com/facebookresearch/codellama/assets/42244960/84be5304-595b-402d-921b-80e811a130a7"">

2. model output duplicate content until length exceeds max_new_token.
instruction:write a c++ code to do quick sort.

<img width=""314"" alt=""duplicate"" src=""https://github.com/facebookresearch/codellama/assets/42244960/2d18b030-2ab1-4808-9703-85e8ed4da382"">

current configuration:Top-k:10 Top-p:0.1 Temperature:0.7 Max new tokens:1024 I tried to fine-tune these parameters with little success.
Could anybody share any ideas to address this strange issue?

",syhw,"Hi, this can be due to several factors: either you're not using the correct tokenizer (in HugginFace's code you need to specify the Code Llama tokenizer, as it at least used to be the Llama 2 tokenizer by default), or you didn't wrap your instructions in `[INST][/INST]` if you're serving it barebones with your own serving script. "
109,Unable to get complete output and seems pending while running 13b-instruct," I cloned and deployed the project locally from https://huggingface.co/spaces/codellama/codellama-13b-chat/tree/main. But after I run this 13b-instruct model,I find there are two issues:

1. model always doesn't provide me complete answer.I try to track the ouput of model and find that it seems to repeat to print empty string until length of output exceeds max_new_token like this:
<img width=""642"" alt=""no complete answer"" src=""https://github.com/facebookresearch/codellama/assets/42244960/84be5304-595b-402d-921b-80e811a130a7"">

2. model output duplicate content until length exceeds max_new_token.
instruction:write a c++ code to do quick sort.

<img width=""314"" alt=""duplicate"" src=""https://github.com/facebookresearch/codellama/assets/42244960/2d18b030-2ab1-4808-9703-85e8ed4da382"">

current configuration:Top-k:10 Top-p:0.1 Temperature:0.7 Max new tokens:1024 I tried to fine-tune these parameters with little success.
Could anybody share any ideas to address this strange issue?

",syhw,Let us know if that fixes it!
109,Unable to get complete output and seems pending while running 13b-instruct," I cloned and deployed the project locally from https://huggingface.co/spaces/codellama/codellama-13b-chat/tree/main. But after I run this 13b-instruct model,I find there are two issues:

1. model always doesn't provide me complete answer.I try to track the ouput of model and find that it seems to repeat to print empty string until length of output exceeds max_new_token like this:
<img width=""642"" alt=""no complete answer"" src=""https://github.com/facebookresearch/codellama/assets/42244960/84be5304-595b-402d-921b-80e811a130a7"">

2. model output duplicate content until length exceeds max_new_token.
instruction:write a c++ code to do quick sort.

<img width=""314"" alt=""duplicate"" src=""https://github.com/facebookresearch/codellama/assets/42244960/2d18b030-2ab1-4808-9703-85e8ed4da382"">

current configuration:Top-k:10 Top-p:0.1 Temperature:0.7 Max new tokens:1024 I tried to fine-tune these parameters with little success.
Could anybody share any ideas to address this strange issue?

",i-love-doufunao,"> Let us know if that fixes it!

Thanks for your reply, I've review codellama demo description again and found that the version of transformers in my demo is lower than 4.33, so I upgraded it to 4.33 and this issue seems fixed."
107,Download sh not working,"[x] Visit the Meta AI website and accept the License agreement.
Done

[x] Wait for approval, and you will receive a signed URL via email.
Done

[] Run the download.sh script and pass the provided URL when prompted to initiate the download. Ensure you copy the URL text itself, not using the 'Copy link address' option.
Get error

[x] Ensure you have wget and
[x]  md5sum installed as prerequisites.
Done

[] Run the script by executing ""bash download.sh.""
Error

[x] Be aware that the download links expire after 24 hours and a certain number of downloads. If you encounter a ""403: Forbidden"" error, you can request a new link.
(just downloaded, haven't been able to get prompt yet).

[x] I also ran `pip install -e .`

This is the response in cmd:

```
\GitHub\codellama>bash download.sh
download.sh: line 2: $'\r': command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do
```
```
codellama>bash --version
GNU bash, version 5.0.17(1)-release (x86_64-pc-linux-gnu)
Copyright (C) 2019 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>

This is free software; you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
. ```


Perhaps I missed something?
I've successfully been running other models in python on this PC. ",D4rkMoons,"Youre problem is that it looks like your using cygwin or anything windows
Use Linux
you can see the `'/'` on linux and `""\""`

![image](https://github.com/facebookresearch/codellama/assets/77543961/723ee59b-8199-4e63-b3e9-59c968810d99)

"
107,Download sh not working,"[x] Visit the Meta AI website and accept the License agreement.
Done

[x] Wait for approval, and you will receive a signed URL via email.
Done

[] Run the download.sh script and pass the provided URL when prompted to initiate the download. Ensure you copy the URL text itself, not using the 'Copy link address' option.
Get error

[x] Ensure you have wget and
[x]  md5sum installed as prerequisites.
Done

[] Run the script by executing ""bash download.sh.""
Error

[x] Be aware that the download links expire after 24 hours and a certain number of downloads. If you encounter a ""403: Forbidden"" error, you can request a new link.
(just downloaded, haven't been able to get prompt yet).

[x] I also ran `pip install -e .`

This is the response in cmd:

```
\GitHub\codellama>bash download.sh
download.sh: line 2: $'\r': command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do
```
```
codellama>bash --version
GNU bash, version 5.0.17(1)-release (x86_64-pc-linux-gnu)
Copyright (C) 2019 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>

This is free software; you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
. ```


Perhaps I missed something?
I've successfully been running other models in python on this PC. ",Kainkainkain,Ah ok I'll try WSL maybe then.  
107,Download sh not working,"[x] Visit the Meta AI website and accept the License agreement.
Done

[x] Wait for approval, and you will receive a signed URL via email.
Done

[] Run the download.sh script and pass the provided URL when prompted to initiate the download. Ensure you copy the URL text itself, not using the 'Copy link address' option.
Get error

[x] Ensure you have wget and
[x]  md5sum installed as prerequisites.
Done

[] Run the script by executing ""bash download.sh.""
Error

[x] Be aware that the download links expire after 24 hours and a certain number of downloads. If you encounter a ""403: Forbidden"" error, you can request a new link.
(just downloaded, haven't been able to get prompt yet).

[x] I also ran `pip install -e .`

This is the response in cmd:

```
\GitHub\codellama>bash download.sh
download.sh: line 2: $'\r': command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do
```
```
codellama>bash --version
GNU bash, version 5.0.17(1)-release (x86_64-pc-linux-gnu)
Copyright (C) 2019 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>

This is free software; you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
. ```


Perhaps I missed something?
I've successfully been running other models in python on this PC. ",Kainkainkain,@D4rkMoons I'm unsure your screenshot is this the right reply?
107,Download sh not working,"[x] Visit the Meta AI website and accept the License agreement.
Done

[x] Wait for approval, and you will receive a signed URL via email.
Done

[] Run the download.sh script and pass the provided URL when prompted to initiate the download. Ensure you copy the URL text itself, not using the 'Copy link address' option.
Get error

[x] Ensure you have wget and
[x]  md5sum installed as prerequisites.
Done

[] Run the script by executing ""bash download.sh.""
Error

[x] Be aware that the download links expire after 24 hours and a certain number of downloads. If you encounter a ""403: Forbidden"" error, you can request a new link.
(just downloaded, haven't been able to get prompt yet).

[x] I also ran `pip install -e .`

This is the response in cmd:

```
\GitHub\codellama>bash download.sh
download.sh: line 2: $'\r': command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do
```
```
codellama>bash --version
GNU bash, version 5.0.17(1)-release (x86_64-pc-linux-gnu)
Copyright (C) 2019 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>

This is free software; you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
. ```


Perhaps I missed something?
I've successfully been running other models in python on this PC. ",isaacayz,Has this been resolved or do you still need help?
107,Download sh not working,"[x] Visit the Meta AI website and accept the License agreement.
Done

[x] Wait for approval, and you will receive a signed URL via email.
Done

[] Run the download.sh script and pass the provided URL when prompted to initiate the download. Ensure you copy the URL text itself, not using the 'Copy link address' option.
Get error

[x] Ensure you have wget and
[x]  md5sum installed as prerequisites.
Done

[] Run the script by executing ""bash download.sh.""
Error

[x] Be aware that the download links expire after 24 hours and a certain number of downloads. If you encounter a ""403: Forbidden"" error, you can request a new link.
(just downloaded, haven't been able to get prompt yet).

[x] I also ran `pip install -e .`

This is the response in cmd:

```
\GitHub\codellama>bash download.sh
download.sh: line 2: $'\r': command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do
```
```
codellama>bash --version
GNU bash, version 5.0.17(1)-release (x86_64-pc-linux-gnu)
Copyright (C) 2019 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>

This is free software; you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
. ```


Perhaps I missed something?
I've successfully been running other models in python on this PC. ",syhw,"@Kainkainkain the advice in the thread is correct, either change the script to use Windows style escapes (and hope that this is sufficient for your cygwin bash to work), or use WSL/linux."
107,Download sh not working,"[x] Visit the Meta AI website and accept the License agreement.
Done

[x] Wait for approval, and you will receive a signed URL via email.
Done

[] Run the download.sh script and pass the provided URL when prompted to initiate the download. Ensure you copy the URL text itself, not using the 'Copy link address' option.
Get error

[x] Ensure you have wget and
[x]  md5sum installed as prerequisites.
Done

[] Run the script by executing ""bash download.sh.""
Error

[x] Be aware that the download links expire after 24 hours and a certain number of downloads. If you encounter a ""403: Forbidden"" error, you can request a new link.
(just downloaded, haven't been able to get prompt yet).

[x] I also ran `pip install -e .`

This is the response in cmd:

```
\GitHub\codellama>bash download.sh
download.sh: line 2: $'\r': command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do
```
```
codellama>bash --version
GNU bash, version 5.0.17(1)-release (x86_64-pc-linux-gnu)
Copyright (C) 2019 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>

This is free software; you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
. ```


Perhaps I missed something?
I've successfully been running other models in python on this PC. ",D4rkMoons,"@Kainkainkain Yep this is the right Place :) ; My way of expression might have been Tempered by talkin to many chatGPT's 😅
Altho dont even remember replying 🤦‍♂️ 
But yea ... ✌🐧
Linux     ="" / "" |   WinDos ="" \ ""
"
106,update inference example,The example pertaining to inference safety check results in a 404 error,facebook-github-bot,"Hi @shreerajshrestha! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%23106). Thanks!"
106,update inference example,The example pertaining to inference safety check results in a 404 error,facebook-github-bot,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!
106,update inference example,The example pertaining to inference safety check results in a 404 error,Romainsauvestre,this was fixed in https://github.com/facebookresearch/codellama/pull/110
105,Downloading models is not working,"I am trying to download the 7b-Python model using the instructions in the README file but I'm getting this when requesting any of the 7b models. 

>> Enter the list of models to download without spaces (7b,13b,34b,7b-Python,13b-Python,34b-Python,7b-Instruct,13b-Instruct,34b-Instruct), or press Enter for all: CodeLlama-7b-Python
Downloading LICENSE and Acceptable Usage Policy
download.sh: line 18: wget: command not found
download.sh: line 19: wget: command not found
Unknown model: CodeLlama-7b-Python

I have requested a new link already but it's still not working. I can't figure out what I'm doing wrong.
",cmosguy,I keep getting `ERROR 403: Forbidden` responses for the other files besides `consolidated.00.pth`.  I do not understand why the `consolidated.00.pth` works but the others do not.
105,Downloading models is not working,"I am trying to download the 7b-Python model using the instructions in the README file but I'm getting this when requesting any of the 7b models. 

>> Enter the list of models to download without spaces (7b,13b,34b,7b-Python,13b-Python,34b-Python,7b-Instruct,13b-Instruct,34b-Instruct), or press Enter for all: CodeLlama-7b-Python
Downloading LICENSE and Acceptable Usage Policy
download.sh: line 18: wget: command not found
download.sh: line 19: wget: command not found
Unknown model: CodeLlama-7b-Python

I have requested a new link already but it's still not working. I can't figure out what I'm doing wrong.
",MaelitoP,"@amtece As you can see in the stacktrace `wget: command not found`:

You need to install some packages to be able to correctly run `download.sh` script, wget & md5sum.
If your on a linux distro, [coreutils](http://packages.ubuntu.com/coreutils) should contain `md5sum` already.

So you just have to do `apt install wget` !"
105,Downloading models is not working,"I am trying to download the 7b-Python model using the instructions in the README file but I'm getting this when requesting any of the 7b models. 

>> Enter the list of models to download without spaces (7b,13b,34b,7b-Python,13b-Python,34b-Python,7b-Instruct,13b-Instruct,34b-Instruct), or press Enter for all: CodeLlama-7b-Python
Downloading LICENSE and Acceptable Usage Policy
download.sh: line 18: wget: command not found
download.sh: line 19: wget: command not found
Unknown model: CodeLlama-7b-Python

I have requested a new link already but it's still not working. I can't figure out what I'm doing wrong.
",frfernandezdev,"I have a similar problem to execute `./download.sh`

![image](https://github.com/facebookresearch/codellama/assets/19200102/a338430e-484a-4c0c-b55d-808e148b61af)
"
105,Downloading models is not working,"I am trying to download the 7b-Python model using the instructions in the README file but I'm getting this when requesting any of the 7b models. 

>> Enter the list of models to download without spaces (7b,13b,34b,7b-Python,13b-Python,34b-Python,7b-Instruct,13b-Instruct,34b-Instruct), or press Enter for all: CodeLlama-7b-Python
Downloading LICENSE and Acceptable Usage Policy
download.sh: line 18: wget: command not found
download.sh: line 19: wget: command not found
Unknown model: CodeLlama-7b-Python

I have requested a new link already but it's still not working. I can't figure out what I'm doing wrong.
",hss245,@frfernandezdev Your token has expired so you can renew and it will work or you have surpassed the number of downloads. Try post 1 or 2 hours.
104,Request for CodeLlama's Specific Production Parameters on Human-Eval Dataset,"Dear Maintainer,

I hope this message finds you well. I have been trying to reproduce the performance of CodeLlama on the Human-Eval dataset, as mentioned in the paper. However, despite my best efforts, I have been unable to achieve the state-of-the-art (SOTA) accuracy reported in the paper.

To further investigate and understand the differences in results, I would greatly appreciate it if you could provide me with the specific production parameters used by CodeLlama on the Human-Eval dataset. Having access to these parameters would allow me to align my implementation more closely with the original work.

I understand and respect any concerns about confidentiality or limitations on sharing proprietary information. If it is not possible to disclose the exact production parameters, I would greatly appreciate any guidance or insights you can provide to help improve the accuracy of my implementation.

This is my evaluation result, and here is the code I used for generation:
![image](https://github.com/facebookresearch/codellama/assets/55141908/d119c454-35e1-40b2-a567-da2f251fd1c7)
![image](https://github.com/facebookresearch/codellama/assets/55141908/6fd00ec5-9b12-4588-9610-f49b67bcebe4)


Thank you for your time and consideration. I look forward to your response and any assistance you can provide.",wanghanbinpanda,"Hello, I'm also attempting to reproduce the results of Codellama on Humaneval. Could you please clarify how you input the prompts from Humaneval into the model? Do you input the original prompts directly from Humaneval, or do you use any other templates?

thx"
104,Request for CodeLlama's Specific Production Parameters on Human-Eval Dataset,"Dear Maintainer,

I hope this message finds you well. I have been trying to reproduce the performance of CodeLlama on the Human-Eval dataset, as mentioned in the paper. However, despite my best efforts, I have been unable to achieve the state-of-the-art (SOTA) accuracy reported in the paper.

To further investigate and understand the differences in results, I would greatly appreciate it if you could provide me with the specific production parameters used by CodeLlama on the Human-Eval dataset. Having access to these parameters would allow me to align my implementation more closely with the original work.

I understand and respect any concerns about confidentiality or limitations on sharing proprietary information. If it is not possible to disclose the exact production parameters, I would greatly appreciate any guidance or insights you can provide to help improve the accuracy of my implementation.

This is my evaluation result, and here is the code I used for generation:
![image](https://github.com/facebookresearch/codellama/assets/55141908/d119c454-35e1-40b2-a567-da2f251fd1c7)
![image](https://github.com/facebookresearch/codellama/assets/55141908/6fd00ec5-9b12-4588-9610-f49b67bcebe4)


Thank you for your time and consideration. I look forward to your response and any assistance you can provide.",xxw11,"> Hello, I'm also attempting to reproduce the results of Codellama on Humaneval. Could you please clarify how you input the prompts from Humaneval into the model? Do you input the original prompts directly from Humaneval, or do you use any other templates?
> 
> thx

I made modifications to this code repository. https://github.com/abacaj/code-eval"
104,Request for CodeLlama's Specific Production Parameters on Human-Eval Dataset,"Dear Maintainer,

I hope this message finds you well. I have been trying to reproduce the performance of CodeLlama on the Human-Eval dataset, as mentioned in the paper. However, despite my best efforts, I have been unable to achieve the state-of-the-art (SOTA) accuracy reported in the paper.

To further investigate and understand the differences in results, I would greatly appreciate it if you could provide me with the specific production parameters used by CodeLlama on the Human-Eval dataset. Having access to these parameters would allow me to align my implementation more closely with the original work.

I understand and respect any concerns about confidentiality or limitations on sharing proprietary information. If it is not possible to disclose the exact production parameters, I would greatly appreciate any guidance or insights you can provide to help improve the accuracy of my implementation.

This is my evaluation result, and here is the code I used for generation:
![image](https://github.com/facebookresearch/codellama/assets/55141908/d119c454-35e1-40b2-a567-da2f251fd1c7)
![image](https://github.com/facebookresearch/codellama/assets/55141908/6fd00ec5-9b12-4588-9610-f49b67bcebe4)


Thank you for your time and consideration. I look forward to your response and any assistance you can provide.",wanghanbinpanda,"> > Hello, I'm also attempting to reproduce the results of Codellama on Humaneval. Could you please clarify how you input the prompts from Humaneval into the model? Do you input the original prompts directly from Humaneval, or do you use any other templates?
> > thx
> 
> I made modifications to this code repository. https://github.com/abacaj/code-eval

Thank you very much!"
104,Request for CodeLlama's Specific Production Parameters on Human-Eval Dataset,"Dear Maintainer,

I hope this message finds you well. I have been trying to reproduce the performance of CodeLlama on the Human-Eval dataset, as mentioned in the paper. However, despite my best efforts, I have been unable to achieve the state-of-the-art (SOTA) accuracy reported in the paper.

To further investigate and understand the differences in results, I would greatly appreciate it if you could provide me with the specific production parameters used by CodeLlama on the Human-Eval dataset. Having access to these parameters would allow me to align my implementation more closely with the original work.

I understand and respect any concerns about confidentiality or limitations on sharing proprietary information. If it is not possible to disclose the exact production parameters, I would greatly appreciate any guidance or insights you can provide to help improve the accuracy of my implementation.

This is my evaluation result, and here is the code I used for generation:
![image](https://github.com/facebookresearch/codellama/assets/55141908/d119c454-35e1-40b2-a567-da2f251fd1c7)
![image](https://github.com/facebookresearch/codellama/assets/55141908/6fd00ec5-9b12-4588-9610-f49b67bcebe4)


Thank you for your time and consideration. I look forward to your response and any assistance you can provide.",itaigat,"Hey @xxw11, in the paper we present results on HumanEval pass@1 using greedy decoding, for pass@10 and pass@100 we used temperature of 0.8. It seems like you use temperature of 0.25, can you please the above mentioned setup?"
104,Request for CodeLlama's Specific Production Parameters on Human-Eval Dataset,"Dear Maintainer,

I hope this message finds you well. I have been trying to reproduce the performance of CodeLlama on the Human-Eval dataset, as mentioned in the paper. However, despite my best efforts, I have been unable to achieve the state-of-the-art (SOTA) accuracy reported in the paper.

To further investigate and understand the differences in results, I would greatly appreciate it if you could provide me with the specific production parameters used by CodeLlama on the Human-Eval dataset. Having access to these parameters would allow me to align my implementation more closely with the original work.

I understand and respect any concerns about confidentiality or limitations on sharing proprietary information. If it is not possible to disclose the exact production parameters, I would greatly appreciate any guidance or insights you can provide to help improve the accuracy of my implementation.

This is my evaluation result, and here is the code I used for generation:
![image](https://github.com/facebookresearch/codellama/assets/55141908/d119c454-35e1-40b2-a567-da2f251fd1c7)
![image](https://github.com/facebookresearch/codellama/assets/55141908/6fd00ec5-9b12-4588-9610-f49b67bcebe4)


Thank you for your time and consideration. I look forward to your response and any assistance you can provide.",ZHUANGMINGXI,"> > Hello, I'm also attempting to reproduce the results of Codellama on Humaneval. Could you please clarify how you input the prompts from Humaneval into the model? Do you input the original prompts directly from Humaneval, or do you use any other templates?
> > thx
> 
> I made modifications to this code repository. https://github.com/abacaj/code-eval

Hello, I tested the script you provided, but I found that many of the answers are empty. Have you encountered this issue? I hope to receive your response."
104,Request for CodeLlama's Specific Production Parameters on Human-Eval Dataset,"Dear Maintainer,

I hope this message finds you well. I have been trying to reproduce the performance of CodeLlama on the Human-Eval dataset, as mentioned in the paper. However, despite my best efforts, I have been unable to achieve the state-of-the-art (SOTA) accuracy reported in the paper.

To further investigate and understand the differences in results, I would greatly appreciate it if you could provide me with the specific production parameters used by CodeLlama on the Human-Eval dataset. Having access to these parameters would allow me to align my implementation more closely with the original work.

I understand and respect any concerns about confidentiality or limitations on sharing proprietary information. If it is not possible to disclose the exact production parameters, I would greatly appreciate any guidance or insights you can provide to help improve the accuracy of my implementation.

This is my evaluation result, and here is the code I used for generation:
![image](https://github.com/facebookresearch/codellama/assets/55141908/d119c454-35e1-40b2-a567-da2f251fd1c7)
![image](https://github.com/facebookresearch/codellama/assets/55141908/6fd00ec5-9b12-4588-9610-f49b67bcebe4)


Thank you for your time and consideration. I look forward to your response and any assistance you can provide.",xxw11,"


> > > Hello, I'm also attempting to reproduce the results of Codellama on Humaneval. Could you please clarify how you input the prompts from Humaneval into the model? Do you input the original prompts directly from Humaneval, or do you use any other templates?
> > > thx
> > 
> > 
> > I made modifications to this code repository. https://github.com/abacaj/code-eval
> 
> Hello, I tested the script you provided, but I found that many of the answers are empty. Have you encountered this issue? I hope to receive your response.

It might be worth verifying if there are any issues with the loading process of your model and tokenizer."
104,Request for CodeLlama's Specific Production Parameters on Human-Eval Dataset,"Dear Maintainer,

I hope this message finds you well. I have been trying to reproduce the performance of CodeLlama on the Human-Eval dataset, as mentioned in the paper. However, despite my best efforts, I have been unable to achieve the state-of-the-art (SOTA) accuracy reported in the paper.

To further investigate and understand the differences in results, I would greatly appreciate it if you could provide me with the specific production parameters used by CodeLlama on the Human-Eval dataset. Having access to these parameters would allow me to align my implementation more closely with the original work.

I understand and respect any concerns about confidentiality or limitations on sharing proprietary information. If it is not possible to disclose the exact production parameters, I would greatly appreciate any guidance or insights you can provide to help improve the accuracy of my implementation.

This is my evaluation result, and here is the code I used for generation:
![image](https://github.com/facebookresearch/codellama/assets/55141908/d119c454-35e1-40b2-a567-da2f251fd1c7)
![image](https://github.com/facebookresearch/codellama/assets/55141908/6fd00ec5-9b12-4588-9610-f49b67bcebe4)


Thank you for your time and consideration. I look forward to your response and any assistance you can provide.",violetch24,"Hi @itaigat ,

I've been trying to reproduce CodeLlama-7b pass scores on HumanEval in the paper. I used [bigcode-evaluation-harness](https://github.com/bigcode-project/bigcode-evaluation-harness) for evalutaion with HumanEval task, using evaluate's [code_eval](https://github.com/huggingface/evaluate/blob/main/metrics/code_eval/code_eval.py) as metric. The model I used is from [HuggingFace](https://huggingface.co/codellama/CodeLlama-7b-hf). I used greedy decoding for pass@1 score. However, the pass@1 score I got in my local environment (29.9%) is different from the score in the paper (33.5%); the pass@10 score I got in my local environment (57.9%) is also different (59.6%).

Below is my code for evaluation:
```bash
        git clone https://github.com/bigcode-project/bigcode-evaluation-harness.git
	cd bigcode-evaluation-harness
	pip install e .
```

### pass@1:
        python  main.py \ # under bigcode-evaluation-harness/
	      --model codellama/CodeLlama-7b-hf \
	      --tasks humaneval \
	      --do_sample False \
	      --n_samples 1 \
	      --allow_code_execution \
	      --save_generations

### pass@10:
        python  main.py \
            --model codellama/CodeLlama-7b-hf \
            --tasks humaneval \
            --temperature 0.8 \
            --n_samples 10 \
            --top_p 0.95 \
            --allow_code_execution \
            --save_generations

I wonder if I left out something and I would be grateful if you could provide any suggestion or guidance for the code above to achieve the same score as in the paper.
It would also be helpful if you could let me know if the evaluation is processed on GPU/CPU?

Thanks!"
104,Request for CodeLlama's Specific Production Parameters on Human-Eval Dataset,"Dear Maintainer,

I hope this message finds you well. I have been trying to reproduce the performance of CodeLlama on the Human-Eval dataset, as mentioned in the paper. However, despite my best efforts, I have been unable to achieve the state-of-the-art (SOTA) accuracy reported in the paper.

To further investigate and understand the differences in results, I would greatly appreciate it if you could provide me with the specific production parameters used by CodeLlama on the Human-Eval dataset. Having access to these parameters would allow me to align my implementation more closely with the original work.

I understand and respect any concerns about confidentiality or limitations on sharing proprietary information. If it is not possible to disclose the exact production parameters, I would greatly appreciate any guidance or insights you can provide to help improve the accuracy of my implementation.

This is my evaluation result, and here is the code I used for generation:
![image](https://github.com/facebookresearch/codellama/assets/55141908/d119c454-35e1-40b2-a567-da2f251fd1c7)
![image](https://github.com/facebookresearch/codellama/assets/55141908/6fd00ec5-9b12-4588-9610-f49b67bcebe4)


Thank you for your time and consideration. I look forward to your response and any assistance you can provide.",jgehring,"When using bigcode-evaluation-harness I'd suggest evaluating on the `humaneval-unstripped` task, which corresponds to the formatting we used for the numbers in the paper. For `codellama/CodeLlama-7b-hf`, I get 31.1% with the harness and greedy decoding for `humaneval-unstripped`.

This is still 2 percentage points worse then the 33.5% we got internally. I would blame the remaining gap on slight differences in inference engines. Tiny differences in computations can accumulate, in particular for smaller, less accurate models. E.g, models might be relatively unsure about tokens at specific predictions (as in, the difference between the predicted token and the next most-likely token would be very small) and since future tokens are conditioned on past tokens we can quickly end up with different outputs.

I looked at the results we got for HumanEval internally and with the harness for greedy decoding with CodeLlama-7b. There are 44 examples out of the 164 for which the output differs, and the result (wrt all tests passing) is different for 6 examples. In 5 cases, we see correct solutions whereas the harness produced failing code, and in 1 case our prediction fails whereas the solution with the harness passes. Hence we get 4 more examples correct, which corresponds to 2.4% absolute on HumanEval.

That being said, deltas due to inference engine differences should even out when computing pass@k or likely with stronger models. For example, computing pass@k on humaneval-unstripped from 200 samples with temperature=0.2 and top_p=0.95, with the harness I get: pass@1: 30.3; pass@10: 45.8; pass@100: 58.6. Internally, we get pass@1: 30.7; pass@10: 47.2; pass@100: 58.8 (compare with Fig. 6 in https://arxiv.org/abs/2308.12950)."
102,--ckpt_dir receiving no value,"I'm having trouble running the model:

    codellama/llama/generation.py"", line 85, in build
    assert len(checkpoints) > 0, f""no checkpoint files found in {ckpt_dir}""
    AssertionError: no checkpoint files found in  --ckpt_dir

This is what I input:

    torchrun --nproc_per_node 1 llamacoder.py \ --ckpt_dir CodeLlama-34b-Instruct/ \ --tokenizer_path CodeLlama-34b- 
    Instruct/tokenizer.model \ --max_seq_len 16000 --max_batch_size 4

In the directory CodeLlama-34b-Instruct it contains: checklist.chk, consolidated.00.pth, consolidated.01.pth, consolidated.02.pth, consolidated.03.pth, params, and tokenizer.model am I supposed to point it to one of the paths or am I perhaps missing a file or two? Am I maybe supposed to be putting it into quotation marks?

Any help would be greatly appreciated, thanks for taking the time to read my issue.",gotsathacker1,"Alright so I did solve my issues I believe, I believe it was the fact that I wasn't running it properly through WSL, but now that I am it seems to have cleared up the ckpt_dir issue. Although there were some things of note at least, for one, since there are 4 checkpoint paths it was referring to it as MP=4 and in the generation.py script I had to change the code to accommodate the world size because I was getting the assertionerror that MP=4 and WORLD SIZE= 1: 

    def build(
            ckpt_dir: str,
            tokenizer_path: str,
            max_seq_len: int,
            max_batch_size: int,
            model_parallel_size: Optional[int] = None,
        ) -> ""Llama"":
            if not torch.distributed.is_initialized():
                torch.distributed.init_process_group(""nccl"")
            if not model_parallel_is_initialized():
                if model_parallel_size is None:
                    model_parallel_size = 4
                initialize_model_parallel(model_parallel_size)

Which did solve that issue, but then for some reason it wouldn't register where the tokenizer_path is even though it's simply --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model in the command line. So I got rid of this part and instead opted to change the code in my script to read like this:

    generator = Llama.build(
            ckpt_dir=ckpt_dir,
            tokenizer_path=""CodeLlama-34b-Instruct/tokenizer.model"",
            max_seq_len=max_seq_len,
            max_batch_size=max_batch_size,
        )

Which seems to have solved that issue but I've run into an OOM error so now I've got to go and solve that, I don't believe that that'll be too much of an issue but at least these things I figured that I'd mention in case anyone else was coming across those issues."
101,setup.py error,"root@main:/mnt/d/repos/LLAMA2/llama-main# ./setup.py
./setup.py: line 4: from: command not found
./setup.py: line 7: syntax error near unexpected token `('
./setup.py: line 7: `def get_requirements(path: str):'",YhgzXxfz,"Hey @ls2013 , could you provide more context on running this script? (conda env etc)

Firstly, you can try running `python -c ""from setuptools import find_packages, setup""` in your terminal and see whether the import works.

If that works, probably there is an issue in the text-encoding of the setup file. Could you double check?"
101,setup.py error,"root@main:/mnt/d/repos/LLAMA2/llama-main# ./setup.py
./setup.py: line 4: from: command not found
./setup.py: line 7: syntax error near unexpected token `('
./setup.py: line 7: `def get_requirements(path: str):'",ls2013,"   Running$ python -c ""from setuptools import find_packages, setup""

in terminal does not produce any errors.


$ ./setup.py./setup.py: line 4: from: command not found./setup.py: line 7: syntax error near unexpected token `('./setup.py: line 7: `def get_requirements(path: str):'
setup.py encoding is UTF-8 

    On Tuesday, September 12, 2023 at 04:30:46 AM EDT, YhgzXxfz ***@***.***> wrote:  
 
 


Hey @ls2013 , could you provide more context on running this script? (conda env etc)

Firstly, you can try running python -c ""from setuptools import find_packages, setup"" in your terminal and see whether the import works.

If that works, probably there is an issue in the text-encoding of the setup file. Could you double check?

—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you were mentioned.Message ID: ***@***.***>
  "
101,setup.py error,"root@main:/mnt/d/repos/LLAMA2/llama-main# ./setup.py
./setup.py: line 4: from: command not found
./setup.py: line 7: syntax error near unexpected token `('
./setup.py: line 7: `def get_requirements(path: str):'",ls2013," The environment is Windows Subsystem for Linux:
$uname -aLinux main 5.15.90.1-microsoft-standard-WSL2 #1 SMP Fri Jan 27 02:56:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

    On Tuesday, September 12, 2023 at 07:08:38 AM EDT, Les Sokolowski ***@***.***> wrote:  
 
    Running$ python -c ""from setuptools import find_packages, setup""

in terminal does not produce any errors.


$ ./setup.py./setup.py: line 4: from: command not found./setup.py: line 7: syntax error near unexpected token `('./setup.py: line 7: `def get_requirements(path: str):'
setup.py encoding is UTF-8 

    On Tuesday, September 12, 2023 at 04:30:46 AM EDT, YhgzXxfz ***@***.***> wrote:  
 
 


Hey @ls2013 , could you provide more context on running this script? (conda env etc)

Firstly, you can try running python -c ""from setuptools import find_packages, setup"" in your terminal and see whether the import works.

If that works, probably there is an issue in the text-encoding of the setup file. Could you double check?

—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you were mentioned.Message ID: ***@***.***>
    "
100,"Any plans to create or release smaller models (3B, 1B)?","Hi! Do you have any plans to create or release smaller Code Llama models?
They may be of use to speed-up inference via speculative sampling (https://github.com/ggerganov/llama.cpp/pull/2926).",jspisak,thanks for the request - this is definitely something the community has been asking for. We are in roadmap planning now and we will take this into account. Btw love the GGML work!
99,Get unreadable result after running example,"Hi  All,  when I ran an official example like this :
torchrun --nproc_per_node 1 example_instructions.py  --ckpt_dir CodeLlama-7b-Instruct/  --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model --max_seq_len 192 --max_batch_size 4 

I got a result thant can not be read like this:

User: In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last month?

> Assistant:  ⁇ cd ⁇ sted ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ 

==================================

User: What is the difference between inorder and preorder traversal? Give an example in Python.

> Assistant: estaururop Rank ⁇  ⁇  ⁇  ⁇  ⁇  condu ⁇  doors ⁇  seq ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ 

==================================

System: Provide answers in JavaScript

User: Write a function that computes the set of sums of all contiguous sublists of a given list.

> Assistant:  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ 

I ran it on M1 MacBook, Does any body has any idea about it ?

",liqiang28,"It's really weird,  if I use my own question, I can get the correct answer, I had any idea about this .

User: use javascript, give a code to generate 10 addition calculation formula

> Assistant:  Here is an example of how you can generate 10 addition calculation formulas using JavaScript:
```
function generateFormulas(numFormulas) {
  let formulas = [];
  for (let i = 0; i < numFormulas; i++) {
    let num1 = Math.floor(Math.random() * 10);
    let num2 = Math.floor(Math.random() * 10);
    let formula = `(${num1} + ${num2})`;
    formulas.push(formula);
  }
  return formulas;
}

console.log(generateFormulas(10));
```
This code uses the `Math.floor()` method to generate two random numbers between 0 and 10,

==================================
"
99,Get unreadable result after running example,"Hi  All,  when I ran an official example like this :
torchrun --nproc_per_node 1 example_instructions.py  --ckpt_dir CodeLlama-7b-Instruct/  --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model --max_seq_len 192 --max_batch_size 4 

I got a result thant can not be read like this:

User: In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last month?

> Assistant:  ⁇ cd ⁇ sted ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ 

==================================

User: What is the difference between inorder and preorder traversal? Give an example in Python.

> Assistant: estaururop Rank ⁇  ⁇  ⁇  ⁇  ⁇  condu ⁇  doors ⁇  seq ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ 

==================================

System: Provide answers in JavaScript

User: Write a function that computes the set of sums of all contiguous sublists of a given list.

> Assistant:  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ 

I ran it on M1 MacBook, Does any body has any idea about it ?

",adiyoss,"Hi @liqiang28, 
Did you manage to solve this issue? It is not clear from your second comment.
If not, can you please provide more details regarding the issue? "
98,"How do you ""prompt"" llama???","How do you ""prompt"" llama???",wanghanbinpanda,hi~~do u get the answer？
98,"How do you ""prompt"" llama???","How do you ""prompt"" llama???",jgehring,"Hi @AntDX316, please take a look at the [completion example](https://github.com/facebookresearch/codellama/blob/main/example_completion.py)."
97,What are the GPU requirements for running the code llama models?,"There is no information about prereqs of what GPU and memory that is requited for running the models during inference.

Please help.",mehulparmariitr,Can somebody help with this? I am getting the same error!
96,Finetuning code llama for Multi-File code generation on private repository,"Hello. I'm trying to finetune code llama for a multifile code generation task on my private repository. 
The goal is to have the LLM generate code for some common bugs / issues across multiple files in my private repository. 

Based on what I have been able to understand so far, the assumption is that doing this will require multiple stages of training / fine-tuning. I read the CodeLlama paper and am trying to create my own ""specialization pipeline"" for my repository and tasks.

1) The first fine-tuning will be done to give the model some comprehension about the repository structure (file paths, summary of what the file is doing and the code itself). This will require 100% code-coverage and the goal would be to have the model overfit. In this case, we will only look at model loss and will have no evaluation or test data sets.

2) Once the model has some comprehension about the repository structure, a second-pass task-specific fine-tuning can be done on a much smaller dataset which will be specific to the task. E.g. We can have the issues, old-code and refactored code as our dataset fields. We can then check for model loss, evaluation loss and the test results to measure the performance of the model.

The reason I want to do it this way is that while the fixes (the fixed code) is common, the files in which the code has to change might be different. So, the model needs to have some understanding of the files etc present in the repository. 

Does this approach sound good or feasible? Are there alternative ways of doing this? If so, would you be able to point me to some resources that I can read and learn from. 

Thanks.",shatealaboxiaowang,same issue.
95,Can't run codellama!,"```
(py465) awahab@adnna:~/nexus-collaborative-project-planning-merge-notes/codellama$ torchrun --nproc_per_node 1 example_completion.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model     --max_seq_len 512
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/llama/generation.py"", line 86, in build
    assert model_parallel_size == len(
AssertionError: Loading a checkpoint for MP=4 but world size is 1
```

this is my error when running the ahove ",bank010,"
> ```
> (py465) awahab@adnna:~/nexus-collaborative-project-planning-merge-notes/codellama$ torchrun --nproc_per_node 1 example_completion.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model     --max_seq_len 512
> > initializing model parallel with size 1
> > initializing ddp with size 1
> > initializing pipeline with size 1
> Traceback (most recent call last):
>   File ""/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/example_completion.py"", line 55, in <module>
>     fire.Fire(main)
>   File ""/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py"", line 141, in Fire
>     component_trace = _Fire(component, args, parsed_flag_args, context, name)
>   File ""/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py"", line 475, in _Fire
>     component, remaining_args = _CallAndUpdateTrace(
>   File ""/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
>     component = fn(*varargs, **kwargs)
>   File ""/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/example_completion.py"", line 20, in main
>     generator = Llama.build(
>   File ""/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/llama/generation.py"", line 86, in build
>     assert model_parallel_size == len(
> AssertionError: Loading a checkpoint for MP=4 but world size is 1
> ```
> 
> 这是我运行 AHOVE 时的错误

torchrun --nproc_per_node 4 example_completion.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model     --max_seq_len 512

"
95,Can't run codellama!,"```
(py465) awahab@adnna:~/nexus-collaborative-project-planning-merge-notes/codellama$ torchrun --nproc_per_node 1 example_completion.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model     --max_seq_len 512
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/llama/generation.py"", line 86, in build
    assert model_parallel_size == len(
AssertionError: Loading a checkpoint for MP=4 but world size is 1
```

this is my error when running the ahove ",RuiLvCOMCEO,"anacorda prompt,python3.9,
(cuda118) G:\Projects\llama\CodeLlama>torchrun --nproc_per_node 4 example_instructions.py --ckpt_dir CodeLlama-34b-Instruct/ --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model --max_seq_len 512 --max_batch_size 4
NOTE: Redirects are currently not supported in Windows or MacOs.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
*****************************************
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [ActiViz]:29500 (system error: 10049 - The requested address is not valid in its context.).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [ActiViz]:29500 (system error: 10049 - The requested address is not valid in its context.).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [ActiViz]:29500 (system error: 10049 - The requested address is not valid in its context.).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [ActiViz]:29500 (system error: 10049 - The requested address is not valid in its context.).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [ActiViz]:29500 (system error: 10049 - The requested address is not valid in its context.).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [ActiViz]:29500 (system error: 10049 - The requested address is not valid in its context.).
Traceback (most recent call last):
  File ""G:\Projects\llama\CodeLlama\example_instructions.py"", line 68, in <module>
Traceback (most recent call last):
  File ""G:\Projects\llama\CodeLlama\example_instructions.py"", line 68, in <module>
    fire.Fire(main)
fire.Fire(main)
  File ""D:\Anaconda3\envs\cuda118\Lib\site-packages\fire\core.py"", line 141, in Fire
  File ""D:\Anaconda3\envs\cuda118\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
component_trace = _Fire(component, args, parsed_flag_args, context, name)"
95,Can't run codellama!,"```
(py465) awahab@adnna:~/nexus-collaborative-project-planning-merge-notes/codellama$ torchrun --nproc_per_node 1 example_completion.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model     --max_seq_len 512
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/llama/generation.py"", line 86, in build
    assert model_parallel_size == len(
AssertionError: Loading a checkpoint for MP=4 but world size is 1
```

this is my error when running the ahove ",jgehring,"As mentioned by @bank010, the 34b model needs to be run with `--nproc_per_node 4`.

@RuiLvCOMCEO, your issue seems to relate to running Code Llama on Windows; see #32 for more info and a potential solution, and please use that issue for further discussion."
94,Possible to continue autoregressive pre-training on custom dataset,"Is it possible to continue the initial autoregressive pre-training on a custom dataset, as was done for Code Llama - Python? This would in principle allow for the fine-tuning of Code Llama models in other programming languages. If so, would you please provide an example training script? Any information or help would be much appreciated!",bank010,"> 是否可以继续在自定义数据集上进行初始自回归预训练，就像对Code Llama - Python所做的那样？原则上，这将允许在其他编程语言中微调Code Llama模型。如果是这样，请您提供一个示例训练脚本吗？任何信息或帮助将不胜感激！

Hello, have you found the right way？
"
93,Unable to run example_complete.py on windows 10,"I tried running the example code given but theres an EOFerror, out of input that keeps popping up. Anyways to solve it?

torchrun --nproc_per_node 1 example_completion.py --ckpt_dir CodeLlama-7b --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 128 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/home/yx/codellama/example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/yx/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/home/yx/codellama/llama/generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
EOFError: Ran out of input
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 94531) of binary: /home/yx/codellama/.conda/bin/python
Traceback (most recent call last):
  File ""/home/yx/codellama/.conda/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-05_17:06:02
  host      : DESKTOP-860T855.
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 94531)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html",moonman239,"I seem to be having a similar issue. I cloned the repository, downloaded the model, ran the setup instructions, then ran the following command, all in a WSL environment running Ubuntu:

```
torchrun --nproc_per_node 1 example_instructions.py \
>     --ckpt_dir CodeLlama-7b-Instruct/ \
 --tokenizer_path CodeLlama-7b-Instruct/tokenizer.mo>     --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \
>     --max_seq_len 512 --max_batch_size 4
```

and got this output:

```
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
/home/moonman239/.local/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
Traceback (most recent call last):
  File ""/mnt/c/Users/moonman239/codellama/example_instructions.py"", line 68, in <module>
    fire.Fire(main)
  File ""/home/moonman239/.local/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/moonman239/.local/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/moonman239/.local/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/mnt/c/Users/moonman239/codellama/example_instructions.py"", line 20, in main
    generator = Llama.build(
  File ""/mnt/c/Users/moonman239/codellama/llama/generation.py"", line 120, in build
    model = Transformer(model_args)
  File ""/mnt/c/Users/moonman239/codellama/llama/model.py"", line 270, in __init__
    self.layers.append(TransformerBlock(layer_id, params))
  File ""/mnt/c/Users/moonman239/codellama/llama/model.py"", line 232, in __init__
    self.attention = Attention(args)
  File ""/mnt/c/Users/moonman239/codellama/llama/model.py"", line 131, in __init__
    self.wo = RowParallelLinear(
  File ""/home/moonman239/.local/lib/python3.10/site-packages/fairscale/nn/model_parallel/layers.py"", line 349, in __init__
    self.weight = Parameter(torch.Tensor(self.out_features, self.input_size_per_partition))
RuntimeError: r.cuMemAddressReserve_ INTERNAL ASSERT FAILED at ""../c10/cuda/driver_api.cpp"":22, please report a bug to PyTorch.
[2023-11-13 13:45:52,497] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 247714) of binary: /usr/local/bin/python3
Traceback (most recent call last):
  File ""/home/moonman239/.local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/moonman239/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/moonman239/.local/lib/python3.10/site-packages/torch/distributed/run.py"", line 806, in main
    run(args)
  File ""/home/moonman239/.local/lib/python3.10/site-packages/torch/distributed/run.py"", line 797, in run
    elastic_launch(
  File ""/home/moonman239/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/moonman239/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_instructions.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-11-13_13:45:52
  host      : DESKTOP-7BUAAQB.
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 247714)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
```
"
93,Unable to run example_complete.py on windows 10,"I tried running the example code given but theres an EOFerror, out of input that keeps popping up. Anyways to solve it?

torchrun --nproc_per_node 1 example_completion.py --ckpt_dir CodeLlama-7b --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 128 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/home/yx/codellama/example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/yx/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/home/yx/codellama/llama/generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
EOFError: Ran out of input
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 94531) of binary: /home/yx/codellama/.conda/bin/python
Traceback (most recent call last):
  File ""/home/yx/codellama/.conda/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-05_17:06:02
  host      : DESKTOP-860T855.
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 94531)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html",FuncAccount,"I got similar issue, ran in windows 11 environment and got this error
E:\ML_Project\LLaMa2\codellama>torchrun --nproc_per_node 1 example_completion.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 128 --max_batch_size 4
NOTE: Redirects are currently not supported in Windows or MacOs.
[W ..\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [DESKTOP-UDLTJ95]:29500 (system error: 10049 - The requested address is not valid in its context.).
[W ..\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [DESKTOP-UDLTJ95]:29500 (system error: 10049 - The requested address is not valid in its context.).
[W ..\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [DESKTOP-UDLTJ95]:29500 (system error: 10049 - The requested address is not valid in its context.).
[W ..\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [DESKTOP-UDLTJ95]:29500 (system error: 10049 - The requested address is not valid in its context.).
Traceback (most recent call last):
  File ""E:\ML_Project\LLaMa2\codellama\example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""C:\Users\Yijie\AppData\Roaming\Python\Python310\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""C:\Users\Yijie\AppData\Roaming\Python\Python310\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""C:\Users\Yijie\AppData\Roaming\Python\Python310\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""E:\ML_Project\LLaMa2\codellama\example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""E:\ML_Project\LLaMa2\codellama\llama\generation.py"", line 76, in build
    torch.distributed.init_process_group(""nccl"")
  File ""C:\Users\Yijie\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\distributed\distributed_c10d.py"", line 907, in init_process_group
    default_pg = _new_process_group_helper(
  File ""C:\Users\Yijie\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\distributed\distributed_c10d.py"", line 1013, in _new_process_group_helper
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 26072) of binary: C:\Users\Yijie\AppData\Local\Programs\Python\Python310\python.exe
Traceback (most recent call last):
  File ""C:\Users\Yijie\AppData\Local\Programs\Python\Python310\lib\runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""C:\Users\Yijie\AppData\Local\Programs\Python\Python310\lib\runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Yijie\AppData\Local\Programs\Python\Python310\Scripts\torchrun.exe\__main__.py"", line 7, in <module>
  File ""C:\Users\Yijie\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""C:\Users\Yijie\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\Users\Yijie\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\Users\Yijie\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""C:\Users\Yijie\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:

"
92,How can I fine-tuning codellama with our own dataset?,As title. Is there any way to achieve that?,zachschillaci27,"This depends, what kind of fine-tuning do you want to do? What does your dataset consist of? If it's instruction tuning or RLHF then there's a good reference available here: https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32. If you provide more information I can try to be of more help."
92,How can I fine-tuning codellama with our own dataset?,As title. Is there any way to achieve that?,Hkh9966,"We have our own code repo, so we wanted to leverage fine-tune to have our own code generation model."
92,How can I fine-tuning codellama with our own dataset?,As title. Is there any way to achieve that?,jgehring,"Hi @Hkh9966, fine-tuning functionality is not included in this code-base. I suggest using other software packages for this and maybe following tutorials available online. For example, most tutorials on fine-tuning Llama 2 or models from HuggingFace will also apply to Code Llama."
90,Feature:  Enable codellama on Intel GPUs,"Motivation:
Thanks for creating this repository . There is an ongoing effort planned to collaborate from Intel GPU to enable out of the box runtime functionality of code llama on our XPU/GPU Devices. There is also a parallel effort on Llama recipes which is under discussion between Intel and Meta , and we plan to provide consolidated support to all framework/models from Meta research to be able to run on our graphics cards. 
Mentioning PR : https://github.com/facebookresearch/llama-recipes/pull/116 (llama recipes also in progress). ",abhilash1910,@brozi  @syhw  requesting review and also wanted to discuss future collaborative development plan with support from Intel GPUs.  Would be great to have a discussion on this . Thanks.
89,Doesn't codellama know when to stop? The EOS doesn't work?,"I just use the codellama-7b，and I found that it won't stop until it reaches the max_tokens.
So how to solve this problem? Thank u !


Here are a few examples of what I tested:


`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 50,
        ""temperature"": 0
    }'
{""id"":""cmpl-df1438614f2a42369f20d240dad02ba0"",""object"":""text_completion"",""created"":1693811331,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\n"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":55,""completion_tokens"":50}}`




`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 500,
        ""temperature"": 0
    }'
{""id"":""cmpl-577731d1354a4b34a17a824f137c4d40"",""object"":""text_completion"",""created"":1693811355,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\nThe city is known for its many parks, including Golden Gate Park, which is the largest park in the United States. The city is also known for its many museums, including the San Francisco Museum of Modern Art, the San Francisco Museum of Art, the California Academy of Sciences, the de Young Museum, and the Legion of Honor.\nThe city is also known for its many restaurants, including the iconic Chinatown, the Mission District, and the North Beach area.\nThe city is also known for its many sports teams, including the San Francisco 49ers, the San Francisco Giants, the San Francisco Warriors, the San Francisco Seals, and the San Francisco Giants.\nThe city is also known for its many beaches, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many landmarks, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many attractions, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many hotels, including the Fairmont San Francisco, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fair"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":505,""completion_tokens"":500}}`",AdarshAcharya5,"Hi, I'm relatively new here, but here is my analysis:
In model.py, the max_seq_len is  :

![Screenshot from 2023-09-04 15-00-29_2](https://github.com/facebookresearch/codellama/assets/132294330/5562ef48-4dd0-486a-ac7a-47e8aee095c2)

Here, based on your prompt, the total generation length is calculated as min(2048, 500 + 5). Hence giving 500 completion tokens.

![Screenshot from 2023-09-04 15-01-57_2](https://github.com/facebookresearch/codellama/assets/132294330/38c26e37-fd62-44c3-80d3-cc57c1e71976)

The loop runs from min_prompt_len (0 probably) to total_len value (505 in your example).

![Screenshot from 2023-09-04 15-02-28_2](https://github.com/facebookresearch/codellama/assets/132294330/a8ec2679-20e6-43d0-a648-bfaadd3b56ab)

The code for breaking the generation loop when eos token is reached : 

![Screenshot from 2023-09-04 15-36-42_2](https://github.com/facebookresearch/codellama/assets/132294330/bef44c54-e316-40ce-b89d-049dfcb9ae2b)

stop_reached is a tensor of length prompt_tokens, and based on this snippet and my reading, I'm assuming that if the current token_id is a pad_id and the next_token is an eos_token, the corresponding position in the stop_reached tensor would become True. The generation only breaks if all the values in stop_reached tensor becomes True, which may not be happening in your example. I understand that this may not answer your question and that I may be pointing out the obvious, but I hope it helps in some way!. The last snippet is hard for me to understand as well, and I would appreciate if someone could explain it to me as well! :).







"
89,Doesn't codellama know when to stop? The EOS doesn't work?,"I just use the codellama-7b，and I found that it won't stop until it reaches the max_tokens.
So how to solve this problem? Thank u !


Here are a few examples of what I tested:


`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 50,
        ""temperature"": 0
    }'
{""id"":""cmpl-df1438614f2a42369f20d240dad02ba0"",""object"":""text_completion"",""created"":1693811331,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\n"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":55,""completion_tokens"":50}}`




`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 500,
        ""temperature"": 0
    }'
{""id"":""cmpl-577731d1354a4b34a17a824f137c4d40"",""object"":""text_completion"",""created"":1693811355,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\nThe city is known for its many parks, including Golden Gate Park, which is the largest park in the United States. The city is also known for its many museums, including the San Francisco Museum of Modern Art, the San Francisco Museum of Art, the California Academy of Sciences, the de Young Museum, and the Legion of Honor.\nThe city is also known for its many restaurants, including the iconic Chinatown, the Mission District, and the North Beach area.\nThe city is also known for its many sports teams, including the San Francisco 49ers, the San Francisco Giants, the San Francisco Warriors, the San Francisco Seals, and the San Francisco Giants.\nThe city is also known for its many beaches, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many landmarks, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many attractions, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many hotels, including the Fairmont San Francisco, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fair"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":505,""completion_tokens"":500}}`",jgehring,"Hi @activezhao, I'm not sure which code-base you're using here; this repository does not offer an HTTP API. Also, you're asking for a natural text completion rather than code generation.

That being said, when experiencing generations that are too long you can try to use the instruct models or implement a custom stopping criterion based on, e.g., paragraphs or newlines.

Feel free to re-open this issue if you have further questions."
89,Doesn't codellama know when to stop? The EOS doesn't work?,"I just use the codellama-7b，and I found that it won't stop until it reaches the max_tokens.
So how to solve this problem? Thank u !


Here are a few examples of what I tested:


`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 50,
        ""temperature"": 0
    }'
{""id"":""cmpl-df1438614f2a42369f20d240dad02ba0"",""object"":""text_completion"",""created"":1693811331,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\n"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":55,""completion_tokens"":50}}`




`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 500,
        ""temperature"": 0
    }'
{""id"":""cmpl-577731d1354a4b34a17a824f137c4d40"",""object"":""text_completion"",""created"":1693811355,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\nThe city is known for its many parks, including Golden Gate Park, which is the largest park in the United States. The city is also known for its many museums, including the San Francisco Museum of Modern Art, the San Francisco Museum of Art, the California Academy of Sciences, the de Young Museum, and the Legion of Honor.\nThe city is also known for its many restaurants, including the iconic Chinatown, the Mission District, and the North Beach area.\nThe city is also known for its many sports teams, including the San Francisco 49ers, the San Francisco Giants, the San Francisco Warriors, the San Francisco Seals, and the San Francisco Giants.\nThe city is also known for its many beaches, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many landmarks, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many attractions, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many hotels, including the Fairmont San Francisco, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fair"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":505,""completion_tokens"":500}}`",activezhao,"> Hi @activezhao, I'm not sure which code-base you're using here; this repository does not offer an HTTP API. Also, you're asking for a natural text completion rather than code generation.
> 
> That being said, when experiencing generations that are too long you can try to use the instruct models or implement a custom stopping criterion based on, e.g., paragraphs or newlines.
> 
> Feel free to re-open this issue if you have further questions.

Hi, @jgehring , thanks for your answer, I just use CodeLlama-7b and for code generation, and I use vLLM to deploy it.

But, when I ask a question of code completion, the response always ends with many \n\n\n\n or \t\t\t\t, It doesn't know it should stop now. Because of no EOS effects, we have to limit the value of ""max_tokens"" and use ""stop"" param to let infer stop, this is actually very inconvenient.

I'm curious why the code completion scenario doesn't work in EOS? "
89,Doesn't codellama know when to stop? The EOS doesn't work?,"I just use the codellama-7b，and I found that it won't stop until it reaches the max_tokens.
So how to solve this problem? Thank u !


Here are a few examples of what I tested:


`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 50,
        ""temperature"": 0
    }'
{""id"":""cmpl-df1438614f2a42369f20d240dad02ba0"",""object"":""text_completion"",""created"":1693811331,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\n"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":55,""completion_tokens"":50}}`




`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 500,
        ""temperature"": 0
    }'
{""id"":""cmpl-577731d1354a4b34a17a824f137c4d40"",""object"":""text_completion"",""created"":1693811355,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\nThe city is known for its many parks, including Golden Gate Park, which is the largest park in the United States. The city is also known for its many museums, including the San Francisco Museum of Modern Art, the San Francisco Museum of Art, the California Academy of Sciences, the de Young Museum, and the Legion of Honor.\nThe city is also known for its many restaurants, including the iconic Chinatown, the Mission District, and the North Beach area.\nThe city is also known for its many sports teams, including the San Francisco 49ers, the San Francisco Giants, the San Francisco Warriors, the San Francisco Seals, and the San Francisco Giants.\nThe city is also known for its many beaches, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many landmarks, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many attractions, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many hotels, including the Fairmont San Francisco, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fair"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":505,""completion_tokens"":500}}`",activezhao,"> Hi, I'm relatively new here, but here is my analysis: In model.py, the max_seq_len is :
> 
> ![Screenshot from 2023-09-04 15-00-29_2](https://user-images.githubusercontent.com/132294330/265419381-5562ef48-4dd0-486a-ac7a-47e8aee095c2.png)
> 
> Here, based on your prompt, the total generation length is calculated as min(2048, 500 + 5). Hence giving 500 completion tokens.
> 
> ![Screenshot from 2023-09-04 15-01-57_2](https://user-images.githubusercontent.com/132294330/265419770-38c26e37-fd62-44c3-80d3-cc57c1e71976.png)
> 
> The loop runs from min_prompt_len (0 probably) to total_len value (505 in your example).
> 
> ![Screenshot from 2023-09-04 15-02-28_2](https://user-images.githubusercontent.com/132294330/265420346-a8ec2679-20e6-43d0-a648-bfaadd3b56ab.png)
> 
> The code for breaking the generation loop when eos token is reached :
> 
> ![Screenshot from 2023-09-04 15-36-42_2](https://user-images.githubusercontent.com/132294330/265427688-bef44c54-e316-40ce-b89d-049dfcb9ae2b.png)
> 
> stop_reached is a tensor of length prompt_tokens, and based on this snippet and my reading, I'm assuming that if the current token_id is a pad_id and the next_token is an eos_token, the corresponding position in the stop_reached tensor would become True. The generation only breaks if all the values in stop_reached tensor becomes True, which may not be happening in your example. I understand that this may not answer your question and that I may be pointing out the obvious, but I hope it helps in some way!. The last snippet is hard for me to understand as well, and I would appreciate if someone could explain it to me as well! :).

Hi, @AdarshAcharya5,  thanks for your answer about the code analysis, u r so good, I have learnt a lot, I will still try to know how to get the EOS works."
89,Doesn't codellama know when to stop? The EOS doesn't work?,"I just use the codellama-7b，and I found that it won't stop until it reaches the max_tokens.
So how to solve this problem? Thank u !


Here are a few examples of what I tested:


`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 50,
        ""temperature"": 0
    }'
{""id"":""cmpl-df1438614f2a42369f20d240dad02ba0"",""object"":""text_completion"",""created"":1693811331,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\n"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":55,""completion_tokens"":50}}`




`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 500,
        ""temperature"": 0
    }'
{""id"":""cmpl-577731d1354a4b34a17a824f137c4d40"",""object"":""text_completion"",""created"":1693811355,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\nThe city is known for its many parks, including Golden Gate Park, which is the largest park in the United States. The city is also known for its many museums, including the San Francisco Museum of Modern Art, the San Francisco Museum of Art, the California Academy of Sciences, the de Young Museum, and the Legion of Honor.\nThe city is also known for its many restaurants, including the iconic Chinatown, the Mission District, and the North Beach area.\nThe city is also known for its many sports teams, including the San Francisco 49ers, the San Francisco Giants, the San Francisco Warriors, the San Francisco Seals, and the San Francisco Giants.\nThe city is also known for its many beaches, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many landmarks, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many attractions, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many hotels, including the Fairmont San Francisco, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fair"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":505,""completion_tokens"":500}}`",yangzhj53,"> > Hi @activezhao, I'm not sure which code-base you're using here; this repository does not offer an HTTP API. Also, you're asking for a natural text completion rather than code generation.
> > That being said, when experiencing generations that are too long you can try to use the instruct models or implement a custom stopping criterion based on, e.g., paragraphs or newlines.
> > Feel free to re-open this issue if you have further questions.
> 
> Hi, @jgehring , thanks for your answer, I just use CodeLlama-7b and for code generation, and I use vLLM to deploy it.
> 
> But, when I ask a question of code completion, the response always ends with many \n\n\n\n or \t\t\t\t, It doesn't know it should stop now. Because of no EOS effects, we have to limit the value of ""max_tokens"" and use ""stop"" param to let infer stop, this is actually very inconvenient.
> 
> I'm curious why the code completion scenario doesn't work in EOS?

Hi, I meet the same question that many '\n\n\n\n' outputed and I wanna know if you have solved it."
89,Doesn't codellama know when to stop? The EOS doesn't work?,"I just use the codellama-7b，and I found that it won't stop until it reaches the max_tokens.
So how to solve this problem? Thank u !


Here are a few examples of what I tested:


`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 50,
        ""temperature"": 0
    }'
{""id"":""cmpl-df1438614f2a42369f20d240dad02ba0"",""object"":""text_completion"",""created"":1693811331,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\n"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":55,""completion_tokens"":50}}`




`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 500,
        ""temperature"": 0
    }'
{""id"":""cmpl-577731d1354a4b34a17a824f137c4d40"",""object"":""text_completion"",""created"":1693811355,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\nThe city is known for its many parks, including Golden Gate Park, which is the largest park in the United States. The city is also known for its many museums, including the San Francisco Museum of Modern Art, the San Francisco Museum of Art, the California Academy of Sciences, the de Young Museum, and the Legion of Honor.\nThe city is also known for its many restaurants, including the iconic Chinatown, the Mission District, and the North Beach area.\nThe city is also known for its many sports teams, including the San Francisco 49ers, the San Francisco Giants, the San Francisco Warriors, the San Francisco Seals, and the San Francisco Giants.\nThe city is also known for its many beaches, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many landmarks, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many attractions, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many hotels, including the Fairmont San Francisco, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fair"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":505,""completion_tokens"":500}}`",activezhao,"> > > Hi @activezhao, I'm not sure which code-base you're using here; this repository does not offer an HTTP API. Also, you're asking for a natural text completion rather than code generation.
> > > That being said, when experiencing generations that are too long you can try to use the instruct models or implement a custom stopping criterion based on, e.g., paragraphs or newlines.
> > > Feel free to re-open this issue if you have further questions.
> > 
> > 
> > Hi, @jgehring , thanks for your answer, I just use CodeLlama-7b and for code generation, and I use vLLM to deploy it.
> > But, when I ask a question of code completion, the response always ends with many \n\n\n\n or \t\t\t\t, It doesn't know it should stop now. Because of no EOS effects, we have to limit the value of ""max_tokens"" and use ""stop"" param to let infer stop, this is actually very inconvenient.
> > I'm curious why the code completion scenario doesn't work in EOS?
> 
> Hi, I meet the same question that many '\n\n\n\n' outputed and I wanna know if you have solved it.

No, I have not solved it, but I use ”stop“ parameter setting several ""\n"", such as ""\n\n\n\n\n"" to let infer stop early.

But, in fact, I have no better ideas."
89,Doesn't codellama know when to stop? The EOS doesn't work?,"I just use the codellama-7b，and I found that it won't stop until it reaches the max_tokens.
So how to solve this problem? Thank u !


Here are a few examples of what I tested:


`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 50,
        ""temperature"": 0
    }'
{""id"":""cmpl-df1438614f2a42369f20d240dad02ba0"",""object"":""text_completion"",""created"":1693811331,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\n"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":55,""completion_tokens"":50}}`




`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 500,
        ""temperature"": 0
    }'
{""id"":""cmpl-577731d1354a4b34a17a824f137c4d40"",""object"":""text_completion"",""created"":1693811355,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\nThe city is known for its many parks, including Golden Gate Park, which is the largest park in the United States. The city is also known for its many museums, including the San Francisco Museum of Modern Art, the San Francisco Museum of Art, the California Academy of Sciences, the de Young Museum, and the Legion of Honor.\nThe city is also known for its many restaurants, including the iconic Chinatown, the Mission District, and the North Beach area.\nThe city is also known for its many sports teams, including the San Francisco 49ers, the San Francisco Giants, the San Francisco Warriors, the San Francisco Seals, and the San Francisco Giants.\nThe city is also known for its many beaches, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many landmarks, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many attractions, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many hotels, including the Fairmont San Francisco, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fair"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":505,""completion_tokens"":500}}`",yangzhj53,"> > > > Hi @activezhao, I'm not sure which code-base you're using here; this repository does not offer an HTTP API. Also, you're asking for a natural text completion rather than code generation.
> > > > That being said, when experiencing generations that are too long you can try to use the instruct models or implement a custom stopping criterion based on, e.g., paragraphs or newlines.
> > > > Feel free to re-open this issue if you have further questions.
> > > 
> > > 
> > > Hi, @jgehring , thanks for your answer, I just use CodeLlama-7b and for code generation, and I use vLLM to deploy it.
> > > But, when I ask a question of code completion, the response always ends with many \n\n\n\n or \t\t\t\t, It doesn't know it should stop now. Because of no EOS effects, we have to limit the value of ""max_tokens"" and use ""stop"" param to let infer stop, this is actually very inconvenient.
> > > I'm curious why the code completion scenario doesn't work in EOS?
> > 
> > 
> > Hi, I meet the same question that many '\n\n\n\n' outputed and I wanna know if you have solved it.
> 
> No, I have not solved it, but I use ”stop“ parameter setting several ""\n"", such as ""\n\n\n\n\n"" to let infer stop early.
> 
> But, in fact, I have no better ideas.

Thanks for your response, but it seems no ""stop"" parameter in any function, could you tell me the detail or have you modified the code?"
89,Doesn't codellama know when to stop? The EOS doesn't work?,"I just use the codellama-7b，and I found that it won't stop until it reaches the max_tokens.
So how to solve this problem? Thank u !


Here are a few examples of what I tested:


`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 50,
        ""temperature"": 0
    }'
{""id"":""cmpl-df1438614f2a42369f20d240dad02ba0"",""object"":""text_completion"",""created"":1693811331,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\n"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":55,""completion_tokens"":50}}`




`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 500,
        ""temperature"": 0
    }'
{""id"":""cmpl-577731d1354a4b34a17a824f137c4d40"",""object"":""text_completion"",""created"":1693811355,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\nThe city is known for its many parks, including Golden Gate Park, which is the largest park in the United States. The city is also known for its many museums, including the San Francisco Museum of Modern Art, the San Francisco Museum of Art, the California Academy of Sciences, the de Young Museum, and the Legion of Honor.\nThe city is also known for its many restaurants, including the iconic Chinatown, the Mission District, and the North Beach area.\nThe city is also known for its many sports teams, including the San Francisco 49ers, the San Francisco Giants, the San Francisco Warriors, the San Francisco Seals, and the San Francisco Giants.\nThe city is also known for its many beaches, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many landmarks, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many attractions, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many hotels, including the Fairmont San Francisco, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fair"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":505,""completion_tokens"":500}}`",activezhao,"> > > > > Hi @activezhao, I'm not sure which code-base you're using here; this repository does not offer an HTTP API. Also, you're asking for a natural text completion rather than code generation.
> > > > > That being said, when experiencing generations that are too long you can try to use the instruct models or implement a custom stopping criterion based on, e.g., paragraphs or newlines.
> > > > > Feel free to re-open this issue if you have further questions.
> > > > 
> > > > 
> > > > Hi, @jgehring , thanks for your answer, I just use CodeLlama-7b and for code generation, and I use vLLM to deploy it.
> > > > But, when I ask a question of code completion, the response always ends with many \n\n\n\n or \t\t\t\t, It doesn't know it should stop now. Because of no EOS effects, we have to limit the value of ""max_tokens"" and use ""stop"" param to let infer stop, this is actually very inconvenient.
> > > > I'm curious why the code completion scenario doesn't work in EOS?
> > > 
> > > 
> > > Hi, I meet the same question that many '\n\n\n\n' outputed and I wanna know if you have solved it.
> > 
> > No, I have not solved it, but I use ”stop“ parameter setting several ""\n"", such as ""\n\n\n\n\n"" to let infer stop early.
> > 
> > But, in fact, I have no better ideas.
> 
> Thanks for your response, but it seems no ""stop"" parameter in any function, could you tell me the detail or have you modified the code?

I depoley codeLlama with vLLM，and the request parameters have a parameter named ""stop"", the infer will stop early when the next token is the ""stop"" words, u can just see the doc of vLLM."
89,Doesn't codellama know when to stop? The EOS doesn't work?,"I just use the codellama-7b，and I found that it won't stop until it reaches the max_tokens.
So how to solve this problem? Thank u !


Here are a few examples of what I tested:


`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 50,
        ""temperature"": 0
    }'
{""id"":""cmpl-df1438614f2a42369f20d240dad02ba0"",""object"":""text_completion"",""created"":1693811331,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\n"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":55,""completion_tokens"":50}}`




`curl http://localhost:8000/v1/completions     -H ""Content-Type: application/json""     -d '{
        ""model"": ""codeLlama"",
        ""prompt"": ""San Francisco is a"",
        ""max_tokens"": 500,
        ""temperature"": 0
    }'
{""id"":""cmpl-577731d1354a4b34a17a824f137c4d40"",""object"":""text_completion"",""created"":1693811355,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\nThe city is known for its many parks, including Golden Gate Park, which is the largest park in the United States. The city is also known for its many museums, including the San Francisco Museum of Modern Art, the San Francisco Museum of Art, the California Academy of Sciences, the de Young Museum, and the Legion of Honor.\nThe city is also known for its many restaurants, including the iconic Chinatown, the Mission District, and the North Beach area.\nThe city is also known for its many sports teams, including the San Francisco 49ers, the San Francisco Giants, the San Francisco Warriors, the San Francisco Seals, and the San Francisco Giants.\nThe city is also known for its many beaches, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many landmarks, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many attractions, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many hotels, including the Fairmont San Francisco, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fair"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":505,""completion_tokens"":500}}`",yangzhj53,"> > > > > > Hi @activezhao, I'm not sure which code-base you're using here; this repository does not offer an HTTP API. Also, you're asking for a natural text completion rather than code generation.
> > > > > > That being said, when experiencing generations that are too long you can try to use the instruct models or implement a custom stopping criterion based on, e.g., paragraphs or newlines.
> > > > > > Feel free to re-open this issue if you have further questions.
> > > > > 
> > > > > 
> > > > > Hi, @jgehring , thanks for your answer, I just use CodeLlama-7b and for code generation, and I use vLLM to deploy it.
> > > > > But, when I ask a question of code completion, the response always ends with many \n\n\n\n or \t\t\t\t, It doesn't know it should stop now. Because of no EOS effects, we have to limit the value of ""max_tokens"" and use ""stop"" param to let infer stop, this is actually very inconvenient.
> > > > > I'm curious why the code completion scenario doesn't work in EOS?
> > > > 
> > > > 
> > > > Hi, I meet the same question that many '\n\n\n\n' outputed and I wanna know if you have solved it.
> > > 
> > > 
> > > No, I have not solved it, but I use ”stop“ parameter setting several ""\n"", such as ""\n\n\n\n\n"" to let infer stop early.
> > > But, in fact, I have no better ideas.
> > 
> > 
> > Thanks for your response, but it seems no ""stop"" parameter in any function, could you tell me the detail or have you modified the code?
> 
> I depoley codeLlama with vLLM，and the request parameters have a parameter named ""stop"", the infer will stop early when the next token is the ""stop"" words, u can just see the doc of vLLM.

I see. Thanks!"
88,Why are Code Llama - Python 7B and 13B  incapable of filling in code given the surrounding context?,,jgehring,"The reasoning here is that infilling models are particularly useful for integration into IDEs, which usually support multiple languages. If single-language or domain-specific models are desired, I'd propose to apply further fine-tuning of the base models with in-domain data."
87,Why is an emoji ☺ added during encoding and decoding?," def encode_infilling(self, s: str) -> List[int]:
      """"""Encode a string without an implicit leading space.""""""
      return self.sp_model.encode(""☺"" + s)[2:]

  def decode_infilling(self, t: List[int]) -> str:
      """"""Decode a string without an implicit leading space.""""""
      return self.sp_model.decode([self.sp_model.piece_to_id(""☺"")] + t)[1:]

I don't understand why '☺' is added to the input during encoding? Could you help me? Thanks!",faabian,"Thanks for the question. SentencePiece tokenizers add an implicit leading space when encoding a sentence (i.e. ""Hello world"" would be encoded as "" Hello world"" would be in the middle of a string). Because in our prompt format, the middle and suffix should not have an added implicit leading space, we need to suppress it when encoding with SentencePiece. This ""hack"" achieves that by simulating that the sentence is contained in a larger string. We could use any prefix to add under the condition that its last token does not merge to the right with any other token. In our vocabulary, the emoji achieves that. Obviously this does not extend to other tokenizers, and the clean solution would be to add a switch to SentencePiece whether to suppress the implicit leading space upon a single encoding (there is a global switch but not a per-prompt one).

See also the solution by HuggingFace which does not need this ""hack"": https://github.com/huggingface/transformers/blob/5af2c6269672cda01c24ad48fab13f14a3ffb746/src/transformers/models/code_llama/tokenization_code_llama.py#L255"
85,Added 'start.py' and modified 'readme.md,"Hello,
    Firstly, I'd like to express my appreciation for introducing the codellama model. Currently, for newcomers who are just beginning to explore the codellama model, running it directly can be somewhat challenging. In order to help everyone quickly run their own codellama model, I intend to create a simple script (start.py) that can be used in conjunction with the codellama pretrained model downloaded from Hugging Face. This will make it easy for users to get started with codellama.
    My pull request mainly involves adding a 'start.py' script and making modifications to 'readme.md' to provide instructions on how to use 'start.py' to initiate the codellama model execution.

Warm regards",jgehring,"Hi @hyhhyh402, the examples in this repo are intended to work with the models downloaded from the Meta AI website."
84,Added 'start.py' and modified 'readme.md,"Hello,
        Firstly, I'd like to express my appreciation for introducing the codellama model. Currently, for newcomers who are just beginning to explore the codellama model, running it directly can be somewhat challenging. In order to help everyone quickly run their own codellama model, I intend to create a simple script (start.py) that can be used in conjunction with the codellama pretrained model downloaded from Hugging Face. This will make it easy for users to get started with codellama.
       My pull request mainly involves adding a 'start.py' script and making modifications to 'readme.md' to provide instructions on how to use 'start.py' to initiate the codellama model execution.

Warm regards",facebook-github-bot,"Hi @hyhhyh402! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%2384). Thanks!"
83,Run the Codellama model quickly based on the Hugging Face pretrained model,"Hello,

I've noticed that for some learners who are just getting started with the Codellama model, running a Codellama model using your code directly can be a bit challenging. To make it easier for everyone to quickly run their own Codellama model, I plan to create a simple script based on the Codellama pretrained model downloaded from Hugging Face. With this script, users can start using Codellama effortlessly. I intend to submit a pull request for this. Please keep an eye out for it.",lvhan028,"Hi, @hyhhyh402 have you submitted a PR?"
83,Run the Codellama model quickly based on the Hugging Face pretrained model,"Hello,

I've noticed that for some learners who are just getting started with the Codellama model, running a Codellama model using your code directly can be a bit challenging. To make it easier for everyone to quickly run their own Codellama model, I plan to create a simple script based on the Codellama pretrained model downloaded from Hugging Face. With this script, users can start using Codellama effortlessly. I intend to submit a pull request for this. Please keep an eye out for it.",yinhong-hu,"yes, my PR is in https://github.com/facebookresearch/codellama/pull/85"
83,Run the Codellama model quickly based on the Hugging Face pretrained model,"Hello,

I've noticed that for some learners who are just getting started with the Codellama model, running a Codellama model using your code directly can be a bit challenging. To make it easier for everyone to quickly run their own Codellama model, I plan to create a simple script based on the Codellama pretrained model downloaded from Hugging Face. With this script, users can start using Codellama effortlessly. I intend to submit a pull request for this. Please keep an eye out for it.",yinhong-hu,"> Hi, @hyhhyh402 have you submitted a PR?
yes, my is in https://github.com/facebookresearch/codellama/pull/85
"
83,Run the Codellama model quickly based on the Hugging Face pretrained model,"Hello,

I've noticed that for some learners who are just getting started with the Codellama model, running a Codellama model using your code directly can be a bit challenging. To make it easier for everyone to quickly run their own Codellama model, I plan to create a simple script based on the Codellama pretrained model downloaded from Hugging Face. With this script, users can start using Codellama effortlessly. I intend to submit a pull request for this. Please keep an eye out for it.",realhaik,"> Hello,
> 
> I've noticed that for some learners who are just getting started with the Codellama model, running a Codellama model using your code directly can be a bit challenging. To make it easier for everyone to quickly run their own Codellama model, I plan to create a simple script based on the Codellama pretrained model downloaded from Hugging Face. With this script, users can start using Codellama effortlessly. I intend to submit a pull request for this. Please keep an eye out for it.

On problem, the HF llama models are trash, and the answers are completely broken. It is very sad that people don't understand this."
83,Run the Codellama model quickly based on the Hugging Face pretrained model,"Hello,

I've noticed that for some learners who are just getting started with the Codellama model, running a Codellama model using your code directly can be a bit challenging. To make it easier for everyone to quickly run their own Codellama model, I plan to create a simple script based on the Codellama pretrained model downloaded from Hugging Face. With this script, users can start using Codellama effortlessly. I intend to submit a pull request for this. Please keep an eye out for it.",bronzwikgk,Where can I find the Url to the script.
82,"Running 7B model on RTX 4070 (12GB) causes ""out of memory"" error","Running example_completion.py with a 7B model on an RTX 4070 (12GB VRAM) results in an ""out of memory"" error.

Is it possible to run example_completion.py without having to purchase a higher-spec GPU than the RTX 4070?

* GPU Info (before running example_completion.py)

```
# nvidia-smi
Sun Sep  3 13:32:02 2023
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.86.05              Driver Version: 535.86.05    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 4070        Off | 00000000:01:00.0  On |                  N/A |
|  0%   47C    P2              32W / 200W |    697MiB / 12282MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
```

* Command and Error

```
# torchrun --nproc_per_node 1 example_completion.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 128 --max_batch_size 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/root/codellama/example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/root/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/root/codellama/llama/generation.py"", line 105, in build
    model = Transformer(model_args)
  File ""/root/codellama/llama/model.py"", line 260, in __init__
    self.layers.append(TransformerBlock(layer_id, params))
  File ""/root/codellama/llama/model.py"", line 222, in __init__
    self.attention = Attention(args)
  File ""/root/codellama/llama/model.py"", line 100, in __init__
    self.wq = ColumnParallelLinear(
  File ""/usr/local/lib/python3.10/dist-packages/fairscale/nn/model_parallel/layers.py"", line 262, in __init__
    self.weight = Parameter(torch.Tensor(self.output_size_per_partition, self.in_features))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.69 GiB total capacity; 10.85 GiB already allocated; 28.25 MiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 4749) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/usr/local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-03_13:25:53
  host      : daiv
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 4749)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```

* Git revision

```
# git rev-parse HEAD
e064c1c24c377cc0875711440ef4c0a6eaf0147b
```

* OS

```
# lsb_release -a
Description:	Ubuntu 22.04.3 LTS
```",zono,It seems like quantization will make it executable. I'm currently testing it
82,"Running 7B model on RTX 4070 (12GB) causes ""out of memory"" error","Running example_completion.py with a 7B model on an RTX 4070 (12GB VRAM) results in an ""out of memory"" error.

Is it possible to run example_completion.py without having to purchase a higher-spec GPU than the RTX 4070?

* GPU Info (before running example_completion.py)

```
# nvidia-smi
Sun Sep  3 13:32:02 2023
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.86.05              Driver Version: 535.86.05    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 4070        Off | 00000000:01:00.0  On |                  N/A |
|  0%   47C    P2              32W / 200W |    697MiB / 12282MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
```

* Command and Error

```
# torchrun --nproc_per_node 1 example_completion.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 128 --max_batch_size 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/root/codellama/example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/root/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/root/codellama/llama/generation.py"", line 105, in build
    model = Transformer(model_args)
  File ""/root/codellama/llama/model.py"", line 260, in __init__
    self.layers.append(TransformerBlock(layer_id, params))
  File ""/root/codellama/llama/model.py"", line 222, in __init__
    self.attention = Attention(args)
  File ""/root/codellama/llama/model.py"", line 100, in __init__
    self.wq = ColumnParallelLinear(
  File ""/usr/local/lib/python3.10/dist-packages/fairscale/nn/model_parallel/layers.py"", line 262, in __init__
    self.weight = Parameter(torch.Tensor(self.output_size_per_partition, self.in_features))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.69 GiB total capacity; 10.85 GiB already allocated; 28.25 MiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 4749) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/usr/local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-03_13:25:53
  host      : daiv
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 4749)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```

* Git revision

```
# git rev-parse HEAD
e064c1c24c377cc0875711440ef4c0a6eaf0147b
```

* OS

```
# lsb_release -a
Description:	Ubuntu 22.04.3 LTS
```",Romainsauvestre,"> It seems like quantization will make it executable. I'm currently testing it

@zono did you manage to make it work?
You can have a look at this [blog post](https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172) and [repo](https://github.com/ggerganov/llama.cpp) for pointers."
82,"Running 7B model on RTX 4070 (12GB) causes ""out of memory"" error","Running example_completion.py with a 7B model on an RTX 4070 (12GB VRAM) results in an ""out of memory"" error.

Is it possible to run example_completion.py without having to purchase a higher-spec GPU than the RTX 4070?

* GPU Info (before running example_completion.py)

```
# nvidia-smi
Sun Sep  3 13:32:02 2023
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.86.05              Driver Version: 535.86.05    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 4070        Off | 00000000:01:00.0  On |                  N/A |
|  0%   47C    P2              32W / 200W |    697MiB / 12282MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
```

* Command and Error

```
# torchrun --nproc_per_node 1 example_completion.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 128 --max_batch_size 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/root/codellama/example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/root/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/root/codellama/llama/generation.py"", line 105, in build
    model = Transformer(model_args)
  File ""/root/codellama/llama/model.py"", line 260, in __init__
    self.layers.append(TransformerBlock(layer_id, params))
  File ""/root/codellama/llama/model.py"", line 222, in __init__
    self.attention = Attention(args)
  File ""/root/codellama/llama/model.py"", line 100, in __init__
    self.wq = ColumnParallelLinear(
  File ""/usr/local/lib/python3.10/dist-packages/fairscale/nn/model_parallel/layers.py"", line 262, in __init__
    self.weight = Parameter(torch.Tensor(self.output_size_per_partition, self.in_features))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.69 GiB total capacity; 10.85 GiB already allocated; 28.25 MiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 4749) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/usr/local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-03_13:25:53
  host      : daiv
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 4749)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```

* Git revision

```
# git rev-parse HEAD
e064c1c24c377cc0875711440ef4c0a6eaf0147b
```

* OS

```
# lsb_release -a
Description:	Ubuntu 22.04.3 LTS
```",LinJia21,3070 can run 7B model
82,"Running 7B model on RTX 4070 (12GB) causes ""out of memory"" error","Running example_completion.py with a 7B model on an RTX 4070 (12GB VRAM) results in an ""out of memory"" error.

Is it possible to run example_completion.py without having to purchase a higher-spec GPU than the RTX 4070?

* GPU Info (before running example_completion.py)

```
# nvidia-smi
Sun Sep  3 13:32:02 2023
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.86.05              Driver Version: 535.86.05    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 4070        Off | 00000000:01:00.0  On |                  N/A |
|  0%   47C    P2              32W / 200W |    697MiB / 12282MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
```

* Command and Error

```
# torchrun --nproc_per_node 1 example_completion.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 128 --max_batch_size 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/root/codellama/example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/root/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/root/codellama/llama/generation.py"", line 105, in build
    model = Transformer(model_args)
  File ""/root/codellama/llama/model.py"", line 260, in __init__
    self.layers.append(TransformerBlock(layer_id, params))
  File ""/root/codellama/llama/model.py"", line 222, in __init__
    self.attention = Attention(args)
  File ""/root/codellama/llama/model.py"", line 100, in __init__
    self.wq = ColumnParallelLinear(
  File ""/usr/local/lib/python3.10/dist-packages/fairscale/nn/model_parallel/layers.py"", line 262, in __init__
    self.weight = Parameter(torch.Tensor(self.output_size_per_partition, self.in_features))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.69 GiB total capacity; 10.85 GiB already allocated; 28.25 MiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 4749) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/usr/local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-03_13:25:53
  host      : daiv
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 4749)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```

* Git revision

```
# git rev-parse HEAD
e064c1c24c377cc0875711440ef4c0a6eaf0147b
```

* OS

```
# lsb_release -a
Description:	Ubuntu 22.04.3 LTS
```",RiccardoBarbieri,"> 3070 can run 7B model

I am currently trying to run the 7B model with my 3070 but I get a CUDA OutOfMemoryError, tried reducing batch size but I still get the error

```
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 7.78 GiB of which 61.00 MiB is free. Including non-PyTorch memory, this process has 7.39 GiB memory in use. Of the allocated memory 7.21 GiB is allocated by PyTorch, and 5.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
```"
82,"Running 7B model on RTX 4070 (12GB) causes ""out of memory"" error","Running example_completion.py with a 7B model on an RTX 4070 (12GB VRAM) results in an ""out of memory"" error.

Is it possible to run example_completion.py without having to purchase a higher-spec GPU than the RTX 4070?

* GPU Info (before running example_completion.py)

```
# nvidia-smi
Sun Sep  3 13:32:02 2023
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.86.05              Driver Version: 535.86.05    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 4070        Off | 00000000:01:00.0  On |                  N/A |
|  0%   47C    P2              32W / 200W |    697MiB / 12282MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
```

* Command and Error

```
# torchrun --nproc_per_node 1 example_completion.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 128 --max_batch_size 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/root/codellama/example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/root/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/root/codellama/llama/generation.py"", line 105, in build
    model = Transformer(model_args)
  File ""/root/codellama/llama/model.py"", line 260, in __init__
    self.layers.append(TransformerBlock(layer_id, params))
  File ""/root/codellama/llama/model.py"", line 222, in __init__
    self.attention = Attention(args)
  File ""/root/codellama/llama/model.py"", line 100, in __init__
    self.wq = ColumnParallelLinear(
  File ""/usr/local/lib/python3.10/dist-packages/fairscale/nn/model_parallel/layers.py"", line 262, in __init__
    self.weight = Parameter(torch.Tensor(self.output_size_per_partition, self.in_features))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.69 GiB total capacity; 10.85 GiB already allocated; 28.25 MiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 4749) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/usr/local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-03_13:25:53
  host      : daiv
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 4749)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```

* Git revision

```
# git rev-parse HEAD
e064c1c24c377cc0875711440ef4c0a6eaf0147b
```

* OS

```
# lsb_release -a
Description:	Ubuntu 22.04.3 LTS
```",shaunakjoshi12,@zono and @RiccardoBarbieri were you able to run successfully on a GPU with 12GB mem? Cos I am facing the same thing. Mine is GTX 1080 ti
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",artcore-c,"I ask Code Llama what to do and got this...

 To convert a Bash shell script to a Z Shell (ZSH) script, you can follow these steps:

1. Open the original Bash script in a plain text editor like Notepad or TextEdit.
2. Replace all instances of `bash` at the beginning of each line with `#!/bin/zsh`. This tells the operating system to run the script using the ZSH interpreter.
3. Remove any lines that begin with `source`, since they are specific to Bash and do not apply to ZSH.
4. Change any references to the `$BASH_SOURCE` environment variable to `${(%):-%N}`. This variable contains the full pathname of the current script file in ZSH, similar to how `$BASH_SOURCE` works in Bash.
5. Check for any other Bash-specific features or commands that may not work correctly in ZSH, and modify them accordingly.
6. Save the modified script with a `.zsh` extension, making sure to preserve the same permissions and ownership as the original script.
7. Make the script executable by running `chmod +x /path/to/script.zsh`.
8. Run the script by typing `/path/to/script.zsh` followed by any arguments or options required.

Note that converting a Bash script to ZSH may involve more than just replacing `bash` with `#!/bin/zsh` at the top of the script. Depending on the complexity of the script, you may need to make changes throughout the code to accommodate differences between the two shells.

Can anyone confirm this is correct? I use bin not bash so it looks like it might work, but it seems overly complex, I guess I'll try... the original argument is ```bash download.sh```, surely this has a simpler conversion than what CL suggests?"
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",artcore-c," I rephrased the question, since I've already created the directory I think I just add curl before the URL...
 
 To convert the `download.sh` command to work with a Z Shell (ZSH) terminal, you can replace the `bash` shebang (`#!/bin/bash`) with the equivalent ZSH shebang (`#!/usr/local/bin/zsh`). Here's an example of how the updated script might look:
```zsh
#!/usr/local/bin/zsh

# Download a file from the internet
curl -O https://example.com/file.txt
```
Alternatively, if you want to continue using the `bash` shebang but still make the script compatible with both `bash` and `zsh`, you can add the `-l` option after the shebang, like this:
```bash
#!/bin/bash -l

# Download a file from the internet
curl -O https://example.com/file.txt
```
The `-l` option tells `bash` to behave more like `zsh` when interpreting the script, which can help avoid compatibility issues."
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",artcore-c,"No that didn't work, copy/pasta my terminal output below...


❯ cd ~/CodeLlama-git

❯ curl https://download2.llamameta.net/\*\?Policy\=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiYnp1ZmZjcGlqZWVvN29pcTZ5enFrdXk1IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTM3NjIzOTJ9fX1dfQ__\&Signature\=qRypH23MgwW25RT26Ews8xZIXT2sDg6KnAmlyToba7p31BxnBTM3aht%7Ez2L9Mrzu8RVLz0My--7J3mAzf1TJzwZNE-8VMARvTA%7EoD-LGXxjwM8cJA47ay%7Ey3gFk7FgT0MStEkKnpTpED1GoJuGyu47TQqDsgkoV0wO%7EqnmLictV40oXfTkXEE0Sn68CAnh0AlGk4SL33l7OII6VsxDYSqV7ueZgOnLmtbT1wsxlKxNJzErme4Vp3Xc%7EXGwaoRDTcpRjkkVi7CUxzjhqsLb%7E7nXC5o6llAR33GujRuppEnLQy04dkuYrbY4QsPLQui1J38rLSCr7jfpLJi2Oqv3wgbw__\&Key-Pair-Id\=K15QRJLYKIFSLZ\&Download-Request-ID\=2897926777036283

<!DOCTYPE html>
<html lang=""en"">
<head>
    <meta charset=""utf-8"">
    <title>Detected User-agent is not wget cloudfront-check-download-url</title>
</head>
<body>
    <p>Please follow the instructions on our <a href=""https://github.com/facebookresearch/codellama"">Code Llama GitHub repository</a> page to access the Code Llama model downloads.</p>
    <p>Github <a href=""https://github.com/facebookresearch/codellama/blob/main/README.md"">README</a></p>
    <p>Github <a href=""https://github.com/facebookresearch/codellama/blob/main/download.sh"">download.sh</a></p>
</body>
</html>"
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",artcore-c,Can't I just clone the repository like any normal git download?
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",artcore-c,"Rephrased the question again... we shall see?


 To download the Code Llama model using a Mac with a Z-shell terminal, you can follow these steps:

1. First, open the Terminal app on your Mac.
2. Navigate to the directory where you want to save the downloaded files. For example, if you want to save the files in a directory called `CodeLlama-git` inside your home folder, you can type `cd ~/CodeLlama-git` and press Enter.
3. Next, enter the following command to download the Code Llama model:
```bash
wget --no-check-certificate \
     --content-disposition \
     --directory-prefix=./ \
     https://dl.fbaipublicfiles.com/codella/model.tar.gz
```
This will download the Code Llama model archive file named `model.tar.gz` into the current working directory.
4. Once the download is complete, extract the contents of the archive file by entering the following command:
```bash
tar xvf model.tar.gz
```
This will create a new subdirectory called `model` containing the extracted files.
5. Finally, navigate to the newly created `model` subdirectory and verify that the downloaded files are present by listing the contents of the directory:
```bash
ls -la
```
This should show you the list of files and directories contained within the `model` subdirectory.

That's it! You should now have successfully downloaded the Code Llama model onto your Mac using the Z-shell terminal."
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",artcore-c,"Well at least I got an error 403 forbidden, it looks like I'm getting somewhere anyway...

my terminal output below after following the above (from Perplexity Labs) Code Llama's advice:

❯ wget --no-check-certificate \
     --content-disposition \
     --directory-prefix=./ \
     https://dl.fbaipublicfiles.com/codella/model.tar.gz
--2023-09-02 19:28:20--  https://dl.fbaipublicfiles.com/codella/model.tar.gz
Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 2600:9000:2353:3400:13:6e38:acc0:93a1, 2600:9000:2353:c600:13:6e38:acc0:93a1, 2600:9000:2353:8400:13:6e38:acc0:93a1, ...
Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|2600:9000:2353:3400:13:6e38:acc0:93a1|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-09-02 19:28:20 ERROR 403: Forbidden."
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",artcore-c,"and with just ```wget```

❯ wget https://download2.llamameta.net/\*\?Policy\=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiYnp1ZmZjcGlqZWVvN29pcTZ5enFrdXk1IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTM3NjIzOTJ9fX1dfQ__\&Signature\=qRypH23MgwW25RT26Ews8xZIXT2sDg6KnAmlyToba7p31BxnBTM3aht%7Ez2L9Mrzu8RVLz0My--7J3mAzf1TJzwZNE-8VMARvTA%7EoD-LGXxjwM8cJA47ay%7Ey3gFk7FgT0MStEkKnpTpED1GoJuGyu47TQqDsgkoV0wO%7EqnmLictV40oXfTkXEE0Sn68CAnh0AlGk4SL33l7OII6VsxDYSqV7ueZgOnLmtbT1wsxlKxNJzErme4Vp3Xc%7EXGwaoRDTcpRjkkVi7CUxzjhqsLb%7E7nXC5o6llAR33GujRuppEnLQy04dkuYrbY4QsPLQui1J38rLSCr7jfpLJi2Oqv3wgbw__\&Key-Pair-Id\=K15QRJLYKIFSLZ\&Download-Request-ID\=2897926777036283
Warning: wildcards not supported in HTTP.
The destination name is too long (652), reducing to 236
--2023-09-02 19:34:29--  https://download2.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiYnp1ZmZjcGlqZWVvN29pcTZ5enFrdXk1IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTM3NjIzOTJ9fX1dfQ__&Signature=qRypH23MgwW25RT26Ews8xZIXT2sDg6KnAmlyToba7p31BxnBTM3aht%7Ez2L9Mrzu8RVLz0My--7J3mAzf1TJzwZNE-8VMARvTA%7EoD-LGXxjwM8cJA47ay%7Ey3gFk7FgT0MStEkKnpTpED1GoJuGyu47TQqDsgkoV0wO%7EqnmLictV40oXfTkXEE0Sn68CAnh0AlGk4SL33l7OII6VsxDYSqV7ueZgOnLmtbT1wsxlKxNJzErme4Vp3Xc%7EXGwaoRDTcpRjkkVi7CUxzjhqsLb%7E7nXC5o6llAR33GujRuppEnLQy04dkuYrbY4QsPLQui1J38rLSCr7jfpLJi2Oqv3wgbw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=2897926777036283
Resolving download2.llamameta.net (download2.llamameta.net)... 2607:7700:0:10:0:2:6c9d:a261, 2607:7700:0:10:0:2:6c9d:a248, 2607:7700:0:10:0:2:6c9d:a27a, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|2607:7700:0:10:0:2:6c9d:a261|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-09-02 19:34:30 ERROR 403: Forbidden."
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",artcore-c,"OMG! so simple I just cloned the repo from the github page, hrmmm, must be something I'm missing? 
oh yeah the tokens, well I give up, for now anyway.

my terminal:

❯ git clone https://github.com/facebookresearch/codellama.git
Cloning into 'codellama'...
remote: Enumerating objects: 30, done.
remote: Counting objects: 100% (15/15), done.
remote: Compressing objects: 100% (11/11), done.
remote: Total 30 (delta 6), reused 4 (delta 4), pack-reused 15
Receiving objects: 100% (30/30), 21.95 KiB | 702.00 KiB/s, done.
Resolving deltas: 100% (6/6), done."
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",GaganHonor,:shipit: By any chance is this your first time :trollface:
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",artcore-c,"> :shipit: By any chance is this your first time :trollface:

no, but I'm not an expert, my experience has been mostly with Raspberry pi OS and Mac OS, I've built some Web UI with React, and compile my own Blender app from daily source (I'm much better at 3D modeling 🤨). I'm sure I could get it to work strictly following the directions using bash with .sh, I just want it to work with .zsh, and downloaded to my folder of choice... I've blindly followed things like this in the past and had problems with paths getting all messed up. I just don't understand why the whole long URL token thing for version downloads, couldn't they just be listed where we could select what we want without all the fuss?
"
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",artcore-c,"btw this particular code, and llm in general are yeah maybe new to me, but I've been collecting books and publications about AI (and robotics...) for the last 40 some years. to me this is just the current hype, again, and it will pass, just as before, every time it passes. after the money goes in and doesn't see a return anyway, that's usually the case. see any honda asimo running around yet, exactly... anyway, I was interested in messing about with it to see what I could come up with, nothing too serious just for fun.
"
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",artcore-c,"I'm interested in modeling the Claustrum network with asynchronous switching in c#, among other things, I might not be keen on the current syntax of this project, but I have my ideas and I'll find a way to implement them eventually, save for lack of assistance or peer 99.9% of the time. at least I have an end goal in mind...
"
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",artcore-c,"and yeah uh like what is this? change? I see none, what does Llama see that I'm missing I wonder?

Copy/pasta Code Llama's output below:

 I apologize for the mistake in my previous response. The correct way to fix the syntax error in your `.zshrc` file is to remove the extraneous parentheses.

So, instead of having:
```bash
export PATH=$PATH:/usr/local/bin:$HOME/.npm-packages/bin
```
You should change it to:
```bash
export PATH=$PATH:/usr/local/bin:$HOME/.npm-packages/bin
```
This should resolve the syntax error and allow you to use the `npm` command again."
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",GaganHonor,Ok keep sharing 
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",artcore-c,"> Ok keep sharing

I had a very long chat with code llama last eve, and have concluded it's ability to simulate comprehension of text by way of comparing patterns and structures results in incomprehensible output, and when combined with a heavy load of marketing (llama is very quick to offer a long list of selling points regarding it's value), leaves a kind of bitter taste, this thing has more ""training"" on why it's so great than any code that actually works, and just to make absolutely sure I run not walk to the exits, it adds a layer of ""fluff"", all rainbowy and happy sort of nonsense. If llama were an employee I'd fire them yesterday. seriously it's just a hype thing, seen plenty over the years nothing new. ah well"
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",GaganHonor,"Thanks for the facts , Please share your work somewhere 💐"
81,Bash .sh vs Bin .zsh Mac OS,"I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing

""mkdir ~/CodeLlama-git""

and then

""cd ~/CodeLlama-git""

which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
maybe someone has the answer?",gautierdag,"Hi @artcore-c,
Thank you for your question, it sounds like this is not a problem with codellama but with your local environment.

Feel free to re-open this issue if you have further questions on this."
80,Error when requesting a download link,"I received this email:

Sorry, you are not eligible to access Llama 2.

Thank you for your interest in using Llama 2. Unfortunately, you do not meet the criteria to obtain a license at this time. ",GaganHonor,"something is wrong!
see this image : https://github.com/GaganHonor/codellama-gagan-singh/blob/main/media/MAILDEMO.jpg

I request you to follow readme.md correctly and try again :)"
80,Error when requesting a download link,"I received this email:

Sorry, you are not eligible to access Llama 2.

Thank you for your interest in using Llama 2. Unfortunately, you do not meet the criteria to obtain a license at this time. ",LakoMoor,"> something is wrong! see this image : https://github.com/GaganHonor/codellama-gagan-singh/blob/main/media/MAILDEMO.jpg
> 
> I request you to follow readme.md correctly and try again :)

in readme.md i see only:
In order to download the model weights and tokenizers, please visit the [Meta AI website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License."
80,Error when requesting a download link,"I received this email:

Sorry, you are not eligible to access Llama 2.

Thank you for your interest in using Llama 2. Unfortunately, you do not meet the criteria to obtain a license at this time. ",GaganHonor,re fill this form with new id & check if country is not restricted ? If specific error share logs here 😊
80,Error when requesting a download link,"I received this email:

Sorry, you are not eligible to access Llama 2.

Thank you for your interest in using Llama 2. Unfortunately, you do not meet the criteria to obtain a license at this time. ",LakoMoor,"> re fill this form with new id & check if country is not restricted ? If specific error share logs here 😊

i try my country and  U.S. I also tried different emails. The rejection is the same.((("
80,Error when requesting a download link,"I received this email:

Sorry, you are not eligible to access Llama 2.

Thank you for your interest in using Llama 2. Unfortunately, you do not meet the criteria to obtain a license at this time. ",LakoMoor,"![image](https://github.com/facebookresearch/codellama/assets/44299427/9a9914c0-6598-4038-9c9d-c1f01972fff8)
"
80,Error when requesting a download link,"I received this email:

Sorry, you are not eligible to access Llama 2.

Thank you for your interest in using Llama 2. Unfortunately, you do not meet the criteria to obtain a license at this time. ",GaganHonor,"hmmmm , Ok I request you to use a new device and your real email id provided by Gmail or outlook etc. , That's only possible solution , U.S is not restricted  . Thanks , Let me know further "
80,Error when requesting a download link,"I received this email:

Sorry, you are not eligible to access Llama 2.

Thank you for your interest in using Llama 2. Unfortunately, you do not meet the criteria to obtain a license at this time. ",LakoMoor,"> hmmmm , Ok I request you to use a new device and your real email id provided by Gmail or outlook etc. , That's only possible solution , U.S is not restricted . Thanks , Let me know further

I have already tried using the phone, my work laptop, and even tried through the server. I also used Gmail and my work email :D"
80,Error when requesting a download link,"I received this email:

Sorry, you are not eligible to access Llama 2.

Thank you for your interest in using Llama 2. Unfortunately, you do not meet the criteria to obtain a license at this time. ",LakoMoor,"Ok, I bought a server in the USA. Through him I tried to fill out a questionnaire. I received a message with the url :) Thank you for everything)"
80,Error when requesting a download link,"I received this email:

Sorry, you are not eligible to access Llama 2.

Thank you for your interest in using Llama 2. Unfortunately, you do not meet the criteria to obtain a license at this time. ",GaganHonor,You mean RDP ? Virtual machine ? 
80,Error when requesting a download link,"I received this email:

Sorry, you are not eligible to access Llama 2.

Thank you for your interest in using Llama 2. Unfortunately, you do not meet the criteria to obtain a license at this time. ",LakoMoor,"> You mean RDP ? Virtual machine ?

Yep. VPS server"
80,Error when requesting a download link,"I received this email:

Sorry, you are not eligible to access Llama 2.

Thank you for your interest in using Llama 2. Unfortunately, you do not meet the criteria to obtain a license at this time. ",GaganHonor,strange that you have to use VPS for this  still congrats :)
80,Error when requesting a download link,"I received this email:

Sorry, you are not eligible to access Llama 2.

Thank you for your interest in using Llama 2. Unfortunately, you do not meet the criteria to obtain a license at this time. ",zoujiulong,I meet same problem，how to solve
80,Error when requesting a download link,"I received this email:

Sorry, you are not eligible to access Llama 2.

Thank you for your interest in using Llama 2. Unfortunately, you do not meet the criteria to obtain a license at this time. ",GaganHonor,"> I meet same problem，how to solve

Try buying a VPS , and your server loaction should be USA and that's all , you can use AWS Free Tier , Azure Free server "
79,Finetune CodeLlama-7b-Instruct-hf on private dataset,"I hope this message finds you well. I recently had the opportunity to experiment with the Codellama-7b-Instruct model from GitHub repository and was pleased to observe its promising performance. Encouraged by these initial results, I am interested in fine-tuning this model on my proprietary code chat dataset. I have single 3090 with 24GB VRAM.

To provide you with more context, my dataset has the following structure:
```
1. <s>[INST] {{user}} [/INST] {{assistant}} </s><s>[INST] {{user}} [/INST] {{assistant}} </s>
2. <s>[INST] {{user}} [/INST] {{assistant}} </s><s>[INST] {{user}} [/INST] {{assistant}} </s>
```
I have a total of 1000 such chat examples in my dataset.

Could you kindly guide me through the recommended pipeline or steps to effectively fine-tune the Codellama-7b-Instruct model on my specific chat dataset? I look forward to your guidance.

**EDIT**

I follow this pipeline but its giving me following error:

```
from transformers import AutoModelForCausalLM,AutoTokenizer
from transformers import LlamaForCausalLM, LlamaTokenizer
import transformers
import torch
from pathlib import Path
import os
import sys

MODEL_NAME = ""codellama/CodeLlama-7b-Instruct-hf""

model =LlamaForCausalLM.from_pretrained(MODEL, load_in_8bit=True, device_map='auto', torch_dtype=torch.bfloat16)
tokenizer = LlamaTokenizer.from_pretrained(""codellama/CodeLlama-7b-Instruct-hf"")

model.train()

def create_peft_config(model):
    from peft import (
        get_peft_model,
        LoraConfig,
        TaskType,
        prepare_model_for_int8_training,
    )

    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules = [""q_proj"", ""v_proj""]
    )

    # prepare int-8 model for training
    model = prepare_model_for_int8_training(model)
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    return model, peft_config

# create peft config
model, lora_config = create_peft_config(model)

from transformers import TrainerCallback
from contextlib import nullcontext
enable_profiler = False
output_dir = ""result""

config = {
    'lora_config': lora_config,
    'learning_rate': 1e-4,
    'num_train_epochs': 1,
    'gradient_accumulation_steps': 2,
    'per_device_train_batch_size': 10,
    'gradient_checkpointing': False,
}

# Set up profiler
if enable_profiler:
    wait, warmup, active, repeat = 1, 1, 2, 1
    total_steps = (wait + warmup + active) * (1 + repeat)
    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)
    profiler = torch.profiler.profile(
        schedule=schedule,
        on_trace_ready=torch.profiler.tensorboard_trace_handler(f""{output_dir}/logs/tensorboard""),
        record_shapes=True,
        profile_memory=True,
        with_stack=True)
    
    class ProfilerCallback(TrainerCallback):
        def __init__(self, profiler):
            self.profiler = profiler
            
        def on_step_end(self, *args, **kwargs):
            self.profiler.step()

    profiler_callback = ProfilerCallback(profiler)
else:
    profiler = nullcontext()

from transformers import default_data_collator, Trainer, TrainingArguments

# Define training args
training_args = TrainingArguments(
    output_dir=output_dir,
    overwrite_output_dir=True,
    bf16=True,  # Use BF16 if available
    # logging strategies
    logging_dir=f""{output_dir}/logs"",
    logging_strategy=""steps"",
    logging_steps=10,
    save_strategy=""no"",
    optim=""adamw_torch_fused"",
    max_steps=total_steps if enable_profiler else -1,
    **{k:v for k,v in config.items() if k != 'lora_config'}
)

with profiler:
    # Create Trainer instance
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=X_train,
        data_collator=default_data_collator,
        callbacks=[profiler_callback] if enable_profiler else [],
    )

    # Start training
    trainer.train()

```

**ERROR**

```
2680     return loss_mb.reduce_mean().detach().to(self.args.device)
   2682 with self.compute_loss_context_manager():
-> 2683     loss = self.compute_loss(model, inputs)
   2685 if self.args.n_gpu > 1:
   2686     loss = loss.mean()  # mean() to average on multi-gpu parallel training

ValueError: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask.
```

",GaganHonor,:trollface:
78,Use md5 instead of md5sum on Apple silicon.,[Inspiration](https://github.com/facebookresearch/llama/commit/4649acd77948182bf39d056ec2f543aeecdc3ca4),facebook-github-bot,"Hi @kharon4! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%2378). Thanks!"
78,Use md5 instead of md5sum on Apple silicon.,[Inspiration](https://github.com/facebookresearch/llama/commit/4649acd77948182bf39d056ec2f543aeecdc3ca4),godpeny,"https://github.com/facebookresearch/codellama/pull/46
duplicated"
77,Running CodeLllama-13B on single GPU,"In the ReadMe file, it is mentioned that to run 13B  model, MP value should be 2. I have only 1 GPU, is there a way to run this model on single GPU (I am fine if efficiency is lost, what I care as of now is to run the 13B model)",GaganHonor,"```py
accumulation_steps = 2  # Number of batches to accumulate gradients

for batch_index, batch in enumerate(data_loader):
    # Forward pass and compute loss
    loss = model.forward(batch)

    # Scale the loss by the accumulation_steps
    loss = loss / accumulation_steps

    # Backward pass and compute gradients
    loss.backward()

    if (batch_index + 1) % accumulation_steps == 0:
        # Update the model's parameters
        optimizer.step()
        model.zero_grad()
```

By using gradient accumulation, you can effectively simulate a larger batch size while training on a single GPU."
77,Running CodeLllama-13B on single GPU,"In the ReadMe file, it is mentioned that to run 13B  model, MP value should be 2. I have only 1 GPU, is there a way to run this model on single GPU (I am fine if efficiency is lost, what I care as of now is to run the 13B model)",manoj21192,"@GaganHonor : Few doubts though I understand the above code
1) Since I have only 1 GPU, do I need to set accumulation_steps = 1 for 13B model whose MP value=2
2) Above changes needs to be done in which file ? "
77,Running CodeLllama-13B on single GPU,"In the ReadMe file, it is mentioned that to run 13B  model, MP value should be 2. I have only 1 GPU, is there a way to run this model on single GPU (I am fine if efficiency is lost, what I care as of now is to run the 13B model)",GaganHonor,"> @GaganHonor : Few doubts though I understand the above code
> 
> 1. Since I have only 1 GPU, do I need to set accumulation_steps = 1 for 13B model whose MP value=2
> 2. Above changes needs to be done in which file ?

Try Test Try ( personal opinion ) 
and its in config"
77,Running CodeLllama-13B on single GPU,"In the ReadMe file, it is mentioned that to run 13B  model, MP value should be 2. I have only 1 GPU, is there a way to run this model on single GPU (I am fine if efficiency is lost, what I care as of now is to run the 13B model)",manoj21192,"> > @GaganHonor : Few doubts though I understand the above code
> > 
> > 1. Since I have only 1 GPU, do I need to set accumulation_steps = 1 for 13B model whose MP value=2
> > 2. Above changes needs to be done in which file ?
> 
> Try Test Try ( personal opinion ) and its in config

I am sorry but I am unable to find any .py file which contains above code. I have cloned the repository from github, could you please let me know the name of python file where this code is present? I cant find any config.py file. I know above code must be running somewhere in backend but I am unable to locate this code to make changes to run 13B model on single GPU"
77,Running CodeLllama-13B on single GPU,"In the ReadMe file, it is mentioned that to run 13B  model, MP value should be 2. I have only 1 GPU, is there a way to run this model on single GPU (I am fine if efficiency is lost, what I care as of now is to run the 13B model)",GaganHonor,"SOURCE : 13B MODEL GENERATED THIS ANSWER FOR YOU 💀
If you are unable to locate the accumulation_steps variable in the codebase, you can try the following steps to find it:

Search for the variable: Use your text editor's search functionality to search for the variable accumulation_steps within the codebase. This will help you locate where it is defined and used.

Check related files: Look for files or modules that are related to model training or optimization. Common names for such files include train.py, model.py, or files that contain functions related to training or optimization.

Look for model training loop: The accumulation_steps variable is typically used within a loop that iterates over the dataset batches for training. Look for a loop that iterates over the data loader or dataset and performs forward pass, loss computation, backward pass, and parameter update steps.

Consult documentation or code comments: If you are working with a codebase that has documentation or code comments, check if there are any references or explanations regarding the usage of accumulation_steps.

Seek assistance from the code author or community: If you are still unable to find the accumulation_steps variable, consider reaching out to the code author or the community associated with the codebase. They may be able to provide specific guidance or point you to the relevant code section.

Remember to adapt the modifications to the appropriate location once you find the accumulation_steps variable in the codebase."
77,Running CodeLllama-13B on single GPU,"In the ReadMe file, it is mentioned that to run 13B  model, MP value should be 2. I have only 1 GPU, is there a way to run this model on single GPU (I am fine if efficiency is lost, what I care as of now is to run the 13B model)",geromepamintuan,https://download2.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoicG1jeTAzeW9qYXYzOHdodGtzYXRjaWhwIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTQwNzY5MDB9fX1dfQ__&Signature=c0CaG3Ph%7EGra7rdugunQaTLGh9d2MjpcUsg%7E7gNLeMuz94p%7EYeC4wKHC0nWM-S5SLaXCNP85cGavjI1VDvpCrtdKHhDWifaVJuJYr1XrU1oP1aSlMw0auEfO2ZLxQ2IgIwaKgcrcgwWrUvylJyThEQCUQNVqk5fp466hHj%7EfM%7EG1AbXFrsgh5LNw3m81zkCeloWC7isnSGwqUpSofUrQVFdsPRab55dIsMxTiX9r3gtpRnb9hbN%7E7YHFwI2I4hAg51iFASEqbpQP8p9ckzEaYupO93Ico8CCXS%7EQpxqcF860LxYgAgYL%7EPur8E9Msez0P30bFF8RVttCLL9D7O7wCA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=208515475329671
77,Running CodeLllama-13B on single GPU,"In the ReadMe file, it is mentioned that to run 13B  model, MP value should be 2. I have only 1 GPU, is there a way to run this model on single GPU (I am fine if efficiency is lost, what I care as of now is to run the 13B model)",manoj21192,"> https://download2.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoicG1jeTAzeW9qYXYzOHdodGtzYXRjaWhwIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTQwNzY5MDB9fX1dfQ__&Signature=c0CaG3Ph%7EGra7rdugunQaTLGh9d2MjpcUsg%7E7gNLeMuz94p%7EYeC4wKHC0nWM-S5SLaXCNP85cGavjI1VDvpCrtdKHhDWifaVJuJYr1XrU1oP1aSlMw0auEfO2ZLxQ2IgIwaKgcrcgwWrUvylJyThEQCUQNVqk5fp466hHj%7EfM%7EG1AbXFrsgh5LNw3m81zkCeloWC7isnSGwqUpSofUrQVFdsPRab55dIsMxTiX9r3gtpRnb9hbN%7E7YHFwI2I4hAg51iFASEqbpQP8p9ckzEaYupO93Ico8CCXS%7EQpxqcF860LxYgAgYL%7EPur8E9Msez0P30bFF8RVttCLL9D7O7wCA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=208515475329671

I have already downloaded all the models, didnt understand how that's going to resolve my query"
77,Running CodeLllama-13B on single GPU,"In the ReadMe file, it is mentioned that to run 13B  model, MP value should be 2. I have only 1 GPU, is there a way to run this model on single GPU (I am fine if efficiency is lost, what I care as of now is to run the 13B model)",DyeKuu,"You may wanna take a look at https://github.com/facebookresearch/codellama/issues/82 for quantization if the usecase is inference only.

If you could run batch_size = 1, then as discussed above https://github.com/facebookresearch/codellama/issues/77#issuecomment-1703664262, the gradient accumulation could help you simulate a large batch size training.

If you could not even run batch_size = 1, then the only way I could think of is to do CPU-offloading (a pretty naive way of pipeline parallelism) for partial model but I presume it requires you to do a lot of heavy-lifting work."
77,Running CodeLllama-13B on single GPU,"In the ReadMe file, it is mentioned that to run 13B  model, MP value should be 2. I have only 1 GPU, is there a way to run this model on single GPU (I am fine if efficiency is lost, what I care as of now is to run the 13B model)",manoj21192,"> You may wanna take a look at #82 for quantization if the usecase is inference only.
> 
> If you could run batch_size = 1, then as discussed above [#77 (comment)](https://github.com/facebookresearch/codellama/issues/77#issuecomment-1703664262), the gradient accumulation could help you simulate a large batch size training.
> 
> If you could not even run batch_size = 1, then the only way I could think of is to do CPU-offloading (a pretty naive way of pipeline parallelism) for partial model but I presume it requires you to do a lot of heavy-lifting work.

I am unable to find the file where code for gradient accumulation is written can you tell me the name of file?"
77,Running CodeLllama-13B on single GPU,"In the ReadMe file, it is mentioned that to run 13B  model, MP value should be 2. I have only 1 GPU, is there a way to run this model on single GPU (I am fine if efficiency is lost, what I care as of now is to run the 13B model)",DragonAngel1st,"I'm not too sure what you guys/girls are trying to explain with your solutions, but should we not be able to run the CodeLlama-13b-Instruct with a NVIDIA RTX 4090 with 24mb of GPU ram? The model should fit in it's memory. Also GH, your are talking about the config file for PyTorch framework and not the CodeLlama codebase from this git repository. Why would we want to change it in the PyTorch library files, is there not a way to configure this with a missing parameter in one of the Llama.build() function?

From my point of view GH, you seem to know a lot of torch stuff, but have your tried the default example_instructions.py file by running it with the torchrun wrapper? That's the program we are trying to run and modify not the default in the frameworks."
76,Display of additional vague comments once code is generated,"I am using a CodeLlama-7b model to generate a code. I provided the following prompt , ""prompts=[""Write a function in python to add two numbers and return their sum""] and got the below output.
Can someone explain why it is showing additional vague comments once code is generated and how to make sure these are not generated along with code?
![Capture](https://github.com/facebookresearch/codellama/assets/52530801/074df101-6809-4734-b62f-cee7743e6bdb)
",AdarshAcharya5,Have you tried running it multiple times?. Has it happened again or just this once?
76,Display of additional vague comments once code is generated,"I am using a CodeLlama-7b model to generate a code. I provided the following prompt , ""prompts=[""Write a function in python to add two numbers and return their sum""] and got the below output.
Can someone explain why it is showing additional vague comments once code is generated and how to make sure these are not generated along with code?
![Capture](https://github.com/facebookresearch/codellama/assets/52530801/074df101-6809-4734-b62f-cee7743e6bdb)
",GaganHonor,"To reduce or eliminate such comments in the generated code, you can try the following steps:

**Prompt Refinement:** Make your prompt more specific and explicit. Clearly state that you only want the model to generate the code and exclude any additional comments. For example, you can modify your prompt to: ""Write a Python function called add that takes two numbers as input and returns their sum.""

**Post-processing:** After generating the code, you can apply post-processing techniques to filter out the additional comments. You can use regular expressions or string matching to identify and remove the unwanted comments from the generated output.

**Model Fine-tuning:** If you have access to the model's training pipeline, you can consider fine-tuning the model on a dataset that specifically focuses on generating code without comments. By training the model on such data, it can learn to generate code outputs that are more aligned with your requirements."
76,Display of additional vague comments once code is generated,"I am using a CodeLlama-7b model to generate a code. I provided the following prompt , ""prompts=[""Write a function in python to add two numbers and return their sum""] and got the below output.
Can someone explain why it is showing additional vague comments once code is generated and how to make sure these are not generated along with code?
![Capture](https://github.com/facebookresearch/codellama/assets/52530801/074df101-6809-4734-b62f-cee7743e6bdb)
",manoj21192,"@AdarshAcharya5 : Yes, comments appearing everytime code is run.
@GaganHonor : My prompt was already very specific, slightly modifying the prompt to what you mentioned above, is giving erroneous results (not even generating the code), please refer below snapshot. 
![Error1](https://github.com/facebookresearch/codellama/assets/52530801/e1fa2753-7fc4-4e4c-a00b-adf408965a75)


"
76,Display of additional vague comments once code is generated,"I am using a CodeLlama-7b model to generate a code. I provided the following prompt , ""prompts=[""Write a function in python to add two numbers and return their sum""] and got the below output.
Can someone explain why it is showing additional vague comments once code is generated and how to make sure these are not generated along with code?
![Capture](https://github.com/facebookresearch/codellama/assets/52530801/074df101-6809-4734-b62f-cee7743e6bdb)
",manoj21192,"It worked, I was running the codellama-7b and not the Instruct version, hence these comments were being generated."
76,Display of additional vague comments once code is generated,"I am using a CodeLlama-7b model to generate a code. I provided the following prompt , ""prompts=[""Write a function in python to add two numbers and return their sum""] and got the below output.
Can someone explain why it is showing additional vague comments once code is generated and how to make sure these are not generated along with code?
![Capture](https://github.com/facebookresearch/codellama/assets/52530801/074df101-6809-4734-b62f-cee7743e6bdb)
",GaganHonor,"Congratulations 🎊  Sorry i wasn't much help , keep growing 😊"
75,Hardware requirement for 100K tokens,What hardware is required to run the 34B Instruct model with 100K context length?,Kushalamummigatti,"@dittops were you able to solve this? And where is the change need to be done for 100k length. 
"
75,Hardware requirement for 100K tokens,What hardware is required to run the 34B Instruct model with 100K context length?,jgehring,"Hi @dittops, in our experiments we used 4 A100s. However, the codellama code-base provides a clean reference implementation and is hence intentionally lacking speed and memory optimizations. As such, attempting to use 100k tokens of context on 4 A100s will produce out-of-memory errors.

I would suggest looking into other inference code-bases utilizing [xformers](https://github.com/facebookresearch/xformers), HuggingFace's [Accelerate](https://huggingface.co/docs/accelerate/index) library, or software packages that offer quantized models.

Feel free to re-open the issue if you have further questions."
73,so long loading checkpoint time,"How long does it take to load the 7b model? During my running process, it takes 200s to load a 7b model.",GaganHonor,depends on your system !
73,so long loading checkpoint time,"How long does it take to load the 7b model? During my running process, it takes 200s to load a 7b model.",gautierdag,"Hi @downeykking,
As @GaganHonor mentioned the loading and inference time of models really depends on your system!

Feel free to re-open this issue if you have further questions on this."
72,Can codellama achieve what it claims on HumanEval?,"I used codellama python 7B  to generate code for human eval dataset (164 questions). I can only get 3 or 4 problems correct, much worse than codellama claims (38%). Is there any trick to make it work? I use the default setting in example_completion.py.

One of the problem that all llama faces is it does not know when to stop. The first few line of codes seems correct. Then it generates non-related codes. 

Can someone show their codes to use codellama to reproduce the human eval results? Thank you!",GaganHonor,"train it! 
![IMessedUpMyBadGIF](https://github.com/facebookresearch/codellama/assets/141204441/4c840e8d-03a4-4fcd-a327-db4aaff262dd)
"
72,Can codellama achieve what it claims on HumanEval?,"I used codellama python 7B  to generate code for human eval dataset (164 questions). I can only get 3 or 4 problems correct, much worse than codellama claims (38%). Is there any trick to make it work? I use the default setting in example_completion.py.

One of the problem that all llama faces is it does not know when to stop. The first few line of codes seems correct. Then it generates non-related codes. 

Can someone show their codes to use codellama to reproduce the human eval results? Thank you!",jgehring,"Hi @jybbjybb, in our evaluations we truncate the generations based on custom stop words. See [the HumanEval implementation in the BigCode evaluation harness](https://github.com/bigcode-project/bigcode-evaluation-harness/blob/136b93c0aea40652b8494bb0df93125b2ea1f833/lm_eval/tasks/humaneval.py#L54) for an example of stop words for this task.

Feel free to re-open this issue if you have further questions on this."
72,Can codellama achieve what it claims on HumanEval?,"I used codellama python 7B  to generate code for human eval dataset (164 questions). I can only get 3 or 4 problems correct, much worse than codellama claims (38%). Is there any trick to make it work? I use the default setting in example_completion.py.

One of the problem that all llama faces is it does not know when to stop. The first few line of codes seems correct. Then it generates non-related codes. 

Can someone show their codes to use codellama to reproduce the human eval results? Thank you!",nicoladainese96,Hi @jgehring what about batched generations with these models? Is there an official way to deal with padding?
71,如何使用多个GPU去加载模型？,"code:  torchrun --nproc_per_node 2 example_instructions.py  --ckpt_dir CodeLlama-13b-Instruct/    --tokenizer_path CodeLlama-13b-Instruct/tokenizer.model     --max_seq_len 2048 --max_batch_size 4


checkpoint = torch.load(ckpt_path, map_location=""cpu"") --> checkpoint = torch.load(ckpt_path, map_location=""cuda"")
我将cpu改为cuda后，会报下列错误
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 1; 23.69 GiB total capacity; 22.64 GiB already allocated; 137.69 MiB free; 23.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 146425 closing signal SIGTERM",bank010,使用cpu加载时，推理速度特别慢
71,如何使用多个GPU去加载模型？,"code:  torchrun --nproc_per_node 2 example_instructions.py  --ckpt_dir CodeLlama-13b-Instruct/    --tokenizer_path CodeLlama-13b-Instruct/tokenizer.model     --max_seq_len 2048 --max_batch_size 4


checkpoint = torch.load(ckpt_path, map_location=""cpu"") --> checkpoint = torch.load(ckpt_path, map_location=""cuda"")
我将cpu改为cuda后，会报下列错误
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 1; 23.69 GiB total capacity; 22.64 GiB already allocated; 137.69 MiB free; 23.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 146425 closing signal SIGTERM",GaganHonor,"I have used google translate my answer and I hope these resources may be helpful ! 🌟


要在Codellama 2023中使用多个GPU加载模型，请按照以下步骤进行操作：

克隆代码库：首先从GitHub克隆Codellama代码库。您可以在终端中运行以下命令来完成克隆：
      git clone https://github.com/facebookresearch/codellama.git

安装依赖项：进入克隆的代码库并安装所需的依赖项。您可以使用pip来安装requirements.txt文件中列出的依赖项：
      cd codellama
   pip install -r requirements.txt

配置环境：设置环境变量以指定要使用的CUDA设备。例如，如果您有两个GPU并且想要同时使用它们，可以将CUDA_VISIBLE_DEVICES环境变量设置为""0,1""：
      export CUDA_VISIBLE_DEVICES=0,1

加载模型：在想要加载模型的脚本中，您可以使用torch.cuda.device上下文管理器来指定要使用的GPU设备。以下是一个示例：
      import torch
   from torch.cuda import device

   device_id = 0  # 指定要使用的GPU设备ID

   with device(device_id):
       model = torch.load('path/to/model.pth')
       model = model.to(device_id)

   在上面的示例中，将'path/to/model.pth'替换为实际的模型文件路径。

验证GPU使用情况：为了验证模型是否加载到指定的GPU设备上，您可以打印model对象并检查其device属性。它应该指示您指定的GPU设备ID。

按照这些步骤，您应该能够在Codellama 2023中使用多个GPU加载模型。根据您拥有的GPU数量和要使用的具体GPU设备，记得调整CUDA_VISIBLE_DEVICES环境变量和device_id的值。




https://finance.sina.com.cn/tech/roll/2023-08-25/doc-imzikthf8880146.shtml

https://zhuanlan.zhihu.com/p/621196415

https://huggingface.co/blog/llama2

"
71,如何使用多个GPU去加载模型？,"code:  torchrun --nproc_per_node 2 example_instructions.py  --ckpt_dir CodeLlama-13b-Instruct/    --tokenizer_path CodeLlama-13b-Instruct/tokenizer.model     --max_seq_len 2048 --max_batch_size 4


checkpoint = torch.load(ckpt_path, map_location=""cpu"") --> checkpoint = torch.load(ckpt_path, map_location=""cuda"")
我将cpu改为cuda后，会报下列错误
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 1; 23.69 GiB total capacity; 22.64 GiB already allocated; 137.69 MiB free; 23.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 146425 closing signal SIGTERM",DyeKuu,"Could you provide more details about your hardware information. 

- How many GPUs you are using to load the model?

Out of the box, the 13b model cannot be loaded in a GPU w/ 24G memory. You may want to try quantization to make it work: you could take a look at https://github.com/facebookresearch/codellama/issues/82."
71,如何使用多个GPU去加载模型？,"code:  torchrun --nproc_per_node 2 example_instructions.py  --ckpt_dir CodeLlama-13b-Instruct/    --tokenizer_path CodeLlama-13b-Instruct/tokenizer.model     --max_seq_len 2048 --max_batch_size 4


checkpoint = torch.load(ckpt_path, map_location=""cpu"") --> checkpoint = torch.load(ckpt_path, map_location=""cuda"")
我将cpu改为cuda后，会报下列错误
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 1; 23.69 GiB total capacity; 22.64 GiB already allocated; 137.69 MiB free; 23.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 146425 closing signal SIGTERM",bank010,"> 您能否提供有关硬件信息的更多详细信息。
> 
> * 您使用多少个 GPU 来加载模型？
> 
> 开箱即用，13b 型号无法加载到带有 24G 内存的 GPU 中。您可能想尝试量化以使其工作：您可以查看#82。

我现在有8张3090，但是当我去部署模型时，发现推理速度特别慢"
70,RuntimeError: Distributed package doesn't have NCCL built in,"When trying to run example_completion.py file in my windows laptop, I am getting below error:
![Error](https://github.com/facebookresearch/codellama/assets/52530801/0b8d0d51-be1f-4178-a089-cf4676aa6654)


I am using pytorch 2.0 version with CUDA 11.7 . On typing the command
import torch.distributed as dist

if dist.is_nccl_available():
    print(""NCCL is available and built into PyTorch."")
else:
    print(""NCCL is not available in this PyTorch installation."")
 
 I am getting the output ""NCCL is not available in this PyTorch installation.""
 What should I do ? ",GaganHonor,"Based on the error message you provided, it seems that you are encountering a ChildFailedError when trying to run the example_completion.py file in your Windows laptop. This error is related to distributed training in PyTorch.

To fix this issue, you can try the following steps:

Check PyTorch Documentation: Visit the PyTorch documentation page on elastic errors to enable traceback and get more information about the error.

Update PyTorch and CUDA Versions: Make sure you are using compatible versions of PyTorch and CUDA. It is recommended to use the latest stable versions to ensure compatibility and access to the latest"
70,RuntimeError: Distributed package doesn't have NCCL built in,"When trying to run example_completion.py file in my windows laptop, I am getting below error:
![Error](https://github.com/facebookresearch/codellama/assets/52530801/0b8d0d51-be1f-4178-a089-cf4676aa6654)


I am using pytorch 2.0 version with CUDA 11.7 . On typing the command
import torch.distributed as dist

if dist.is_nccl_available():
    print(""NCCL is available and built into PyTorch."")
else:
    print(""NCCL is not available in this PyTorch installation."")
 
 I am getting the output ""NCCL is not available in this PyTorch installation.""
 What should I do ? ",manoj21192,Everything checked but unable to resolve. I am using latest version of Pytorch and 11.7 CUDA.
70,RuntimeError: Distributed package doesn't have NCCL built in,"When trying to run example_completion.py file in my windows laptop, I am getting below error:
![Error](https://github.com/facebookresearch/codellama/assets/52530801/0b8d0d51-be1f-4178-a089-cf4676aa6654)


I am using pytorch 2.0 version with CUDA 11.7 . On typing the command
import torch.distributed as dist

if dist.is_nccl_available():
    print(""NCCL is available and built into PyTorch."")
else:
    print(""NCCL is not available in this PyTorch installation."")
 
 I am getting the output ""NCCL is not available in this PyTorch installation.""
 What should I do ? ",srinivaskumarramdas,Getting the same error on my macbook as well. 
70,RuntimeError: Distributed package doesn't have NCCL built in,"When trying to run example_completion.py file in my windows laptop, I am getting below error:
![Error](https://github.com/facebookresearch/codellama/assets/52530801/0b8d0d51-be1f-4178-a089-cf4676aa6654)


I am using pytorch 2.0 version with CUDA 11.7 . On typing the command
import torch.distributed as dist

if dist.is_nccl_available():
    print(""NCCL is available and built into PyTorch."")
else:
    print(""NCCL is not available in this PyTorch installation."")
 
 I am getting the output ""NCCL is not available in this PyTorch installation.""
 What should I do ? ",GaganHonor,"please try to explain more  and share information like system info , and all logs in txt form or link to them "
70,RuntimeError: Distributed package doesn't have NCCL built in,"When trying to run example_completion.py file in my windows laptop, I am getting below error:
![Error](https://github.com/facebookresearch/codellama/assets/52530801/0b8d0d51-be1f-4178-a089-cf4676aa6654)


I am using pytorch 2.0 version with CUDA 11.7 . On typing the command
import torch.distributed as dist

if dist.is_nccl_available():
    print(""NCCL is available and built into PyTorch."")
else:
    print(""NCCL is not available in this PyTorch installation."")
 
 I am getting the output ""NCCL is not available in this PyTorch installation.""
 What should I do ? ",manoj21192,"As of now, for 7B parameter model, its working on windows by making changes to generator.py file by using torch.distributed.init_process_group(""gloo""), instead of ""nccl"". 
Is this methodology fine if I want to use high parameter model in future?"
70,RuntimeError: Distributed package doesn't have NCCL built in,"When trying to run example_completion.py file in my windows laptop, I am getting below error:
![Error](https://github.com/facebookresearch/codellama/assets/52530801/0b8d0d51-be1f-4178-a089-cf4676aa6654)


I am using pytorch 2.0 version with CUDA 11.7 . On typing the command
import torch.distributed as dist

if dist.is_nccl_available():
    print(""NCCL is available and built into PyTorch."")
else:
    print(""NCCL is not available in this PyTorch installation."")
 
 I am getting the output ""NCCL is not available in this PyTorch installation.""
 What should I do ? ",srinivaskumarramdas,"""gloo"" worked on Apple M2 chips mac as well. had some more additional changes covered in this PR: https://github.com/facebookresearch/codellama/pull/18 ."
70,RuntimeError: Distributed package doesn't have NCCL built in,"When trying to run example_completion.py file in my windows laptop, I am getting below error:
![Error](https://github.com/facebookresearch/codellama/assets/52530801/0b8d0d51-be1f-4178-a089-cf4676aa6654)


I am using pytorch 2.0 version with CUDA 11.7 . On typing the command
import torch.distributed as dist

if dist.is_nccl_available():
    print(""NCCL is available and built into PyTorch."")
else:
    print(""NCCL is not available in this PyTorch installation."")
 
 I am getting the output ""NCCL is not available in this PyTorch installation.""
 What should I do ? ",GaganHonor,"Please close this issue if issue is Fixed 
![RunForrestPuppetGIF](https://github.com/facebookresearch/codellama/assets/141204441/e67b8bbc-706b-424f-95c3-bd4cae40dcbc)
"
70,RuntimeError: Distributed package doesn't have NCCL built in,"When trying to run example_completion.py file in my windows laptop, I am getting below error:
![Error](https://github.com/facebookresearch/codellama/assets/52530801/0b8d0d51-be1f-4178-a089-cf4676aa6654)


I am using pytorch 2.0 version with CUDA 11.7 . On typing the command
import torch.distributed as dist

if dist.is_nccl_available():
    print(""NCCL is available and built into PyTorch."")
else:
    print(""NCCL is not available in this PyTorch installation."")
 
 I am getting the output ""NCCL is not available in this PyTorch installation.""
 What should I do ? ",manoj21192,Use gloo to make it work on Windows
69,Fine tuning it with own code,"can code llama be fine tuned with my own code for code refactoring tasks? and in that case, how do we prepare the dataset ?",GaganHonor,"Yes, CodeLLama can be fine-tuned with your own code for code refactoring tasks. Fine-tuning involves training the model on your specific dataset to adapt it to your particular task.

To prepare the dataset for fine-tuning, you can follow these steps:

Collect Code Examples: Gather a collection of code examples that represent the refactoring task you want to train the model on. These examples should include both the original code and the refactored code.

Create Input-Output Pairs: For each code example, create an input-output pair where the input is the original code and the output is the refactored code. This will serve as your training data.

Preprocess the Code: Preprocess the code examples to convert them into a format suitable for training the model. This may involve tokenization, handling special characters, and applying any necessary formatting.

Split the Dataset: Split your dataset into training, validation, and testing sets. The training set will be used to train the model, the validation set will be used to tune the hyperparameters, and the testing set will be used to evaluate the model's performance.

Tokenize the Code: Tokenize the code examples into tokens or sequences of tokens. This is usually done using a tokenizer specific to the programming language you are working with.

Encode the Tokens: Encode the tokens into numerical representations that can be fed into the model. This can be done using techniques such as one-hot encoding or word embeddings.

Train the Model: Use the preprocessed dataset to train the CodeLLama model. You can use techniques like transfer learning or fine-tuning to adapt the model to your specific task.

Evaluate and Iterate: Evaluate the performance of the trained model on the testing set. If the performance is not satisfactory, you can iterate on the previous steps, such as collecting more data, adjusting the model architecture, or fine-tuning the hyperparameters.

By following these steps, you can prepare your dataset and fine-tune the CodeLLama model for code refactoring tasks using your own code examples."
69,Fine tuning it with own code,"can code llama be fine tuned with my own code for code refactoring tasks? and in that case, how do we prepare the dataset ?",Kushalamummigatti,"@GaganHonor is there any script you have for finetuning. 
"
69,Fine tuning it with own code,"can code llama be fine tuned with my own code for code refactoring tasks? and in that case, how do we prepare the dataset ?",GaganHonor,"Yes , Please search on Hugging Face community , very active 
![image](https://github.com/facebookresearch/codellama/assets/141204441/50bd7aa3-9b57-43ae-96b0-fd7986cd8768)

https://huggingface.co/mlabonne/codellama-2-7b etc "
69,Fine tuning it with own code,"can code llama be fine tuned with my own code for code refactoring tasks? and in that case, how do we prepare the dataset ?",Kushalamummigatti,"@GaganHonor The above link don't have finetuning script. 

Actually i have finetuned. But inferences are very bad so looking for other options. "
69,Fine tuning it with own code,"can code llama be fine tuned with my own code for code refactoring tasks? and in that case, how do we prepare the dataset ?",syhw,"As multiple actors did in the community (e.g. Llama-Adapter, Qlora, [tutorials like this one](https://www.mlexpert.io/machine-learning/tutorials/alpaca-fine-tuning)), you can fine-tune the model for your task, and the advice above is correct. However, this is not in the scope of this repository to support fine-tuning. "
68,Add terraform config to start EC2 instance(s),,lene,"Wrong PR target, sorry!"
67,Sponsored issue: I need it for my project,"## Priority Support

- @apmnoman is using  [Mintycode](https://mintycode.io) to fund this issue.
- You can receive ![amount](https://imageupload.io/ib/zwkd2EaAY6YpPsg_1693209112.png) if you provide priority support to @apmnoman. 
- To view the support request and terms go to  [Mintycode](https://mintycode.io/profile?action=CREATOR_SUPPORT_MODAL&requestId=34e64f65-9635-4d16-97c3-30cfbbe8e21c&owner=facebookresearch&name=codellama).
- Thank you in advance for helping.

[![mintycode](https://imageupload.io/ib/LvxMwrPxam1JW9e_1693210482.png)](https://mintycode.io)
",GaganHonor,Just read 📖 me or ask Questions 
66,how to install wget and md5sum.(Windows user),"I am running all the commands in a git bash terminal.
If you guys tell me a step by step method on how to install the model it will be very helpful

```download.sh: line 18: wget: command not found
download.sh: line 19: wget: command not found
Downloading CodeLlama-7b
download.sh: line 53: wget: command not found
download.sh: line 56: wget: command not found
download.sh: line 57: wget: command not found
download.sh: line 58: wget: command not found
Checking checksums
md5sum: checklist.chk: No such file or directory```",bank010,"https://eternallybored.org/misc/wget/1.21.4/32/wget.exe
下载后放入Git/mingw64/bin文件下就可以用wget命令"
66,how to install wget and md5sum.(Windows user),"I am running all the commands in a git bash terminal.
If you guys tell me a step by step method on how to install the model it will be very helpful

```download.sh: line 18: wget: command not found
download.sh: line 19: wget: command not found
Downloading CodeLlama-7b
download.sh: line 53: wget: command not found
download.sh: line 56: wget: command not found
download.sh: line 57: wget: command not found
download.sh: line 58: wget: command not found
Checking checksums
md5sum: checklist.chk: No such file or directory```",notTanveer,"> https://eternallybored.org/misc/wget/1.21.4/32/wget.exe
> 下载后放入Git/mingw64/bin文件下就可以用wget命令

Still doesnt work. Is there any specific path where i need to store the .exe file? 
How to install md5sums"
66,how to install wget and md5sum.(Windows user),"I am running all the commands in a git bash terminal.
If you guys tell me a step by step method on how to install the model it will be very helpful

```download.sh: line 18: wget: command not found
download.sh: line 19: wget: command not found
Downloading CodeLlama-7b
download.sh: line 53: wget: command not found
download.sh: line 56: wget: command not found
download.sh: line 57: wget: command not found
download.sh: line 58: wget: command not found
Checking checksums
md5sum: checklist.chk: No such file or directory```",syhw,You can use WSL https://learn.microsoft.com/en-us/windows/wsl/install 
66,how to install wget and md5sum.(Windows user),"I am running all the commands in a git bash terminal.
If you guys tell me a step by step method on how to install the model it will be very helpful

```download.sh: line 18: wget: command not found
download.sh: line 19: wget: command not found
Downloading CodeLlama-7b
download.sh: line 53: wget: command not found
download.sh: line 56: wget: command not found
download.sh: line 57: wget: command not found
download.sh: line 58: wget: command not found
Checking checksums
md5sum: checklist.chk: No such file or directory```",GaganHonor,"WSL ( Windows Subsystem for Linux ╰(*°▽°*)╯) is best option for Windows users ( I am on W11 ) 
I suggest to use WSL2 as it's best for [codellama](https://github.com/facebookresearch/codellama/)

Enable It ✅
To install WSL 2 on Windows 11 by PowerShell, follow these steps:

1. Open PowerShell as administrator.
2. Run the command: `wsl --install`
3. Restart your PC.
4. To verify that WSL 2 is installed, run the command: `wsl --set-default-version 2`
5. To check the version of your Linux distribution, run the command: `wsl -l -v`

For more details, you can refer to these sources:

- (How to Install WSL2 on Windows 10 and Windows 11 - https://petri.com/install-wsl2-windows/
- (Install WSL | Microsoft Learn - https://learn.microsoft.com/en-us/windows/wsl/install 


![Screenshot 2023-08-31 093642](https://github.com/facebookresearch/codellama/assets/141204441/f8555dec-86e1-41f3-ad40-b1776512d840)
![Screenshot 2023-08-31 093511](https://github.com/facebookresearch/codellama/assets/141204441/ca4e24d5-ee85-40f0-9de9-a2fd1745e9c5)


you can also enable the feature manually - ` dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart` and  `dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart`


after installation successfully 
![Screenshot 2023-08-31 104122](https://github.com/facebookresearch/codellama/assets/141204441/778397ef-f69a-4d87-970a-376f7b2e2ad9)
more help 👍  https://devblogs.microsoft.com/commandline/install-wsl-with-a-single-command-now-available-in-windows-10-version-2004-and-higher/

and yes after login use `sudo apt install coreutils` and ` sudo apt install wget`
and please ask more if need help as i myself using WSL method. 🤖





"
66,how to install wget and md5sum.(Windows user),"I am running all the commands in a git bash terminal.
If you guys tell me a step by step method on how to install the model it will be very helpful

```download.sh: line 18: wget: command not found
download.sh: line 19: wget: command not found
Downloading CodeLlama-7b
download.sh: line 53: wget: command not found
download.sh: line 56: wget: command not found
download.sh: line 57: wget: command not found
download.sh: line 58: wget: command not found
Checking checksums
md5sum: checklist.chk: No such file or directory```",broken-bytes,"Was about to mention WSL as well, got it working there but chat is rather slow. Taking around 60s to respond for 1024 max size.

i7 12700K
64GB RAM
RTX 4080

Should be way faster than that imho."
66,how to install wget and md5sum.(Windows user),"I am running all the commands in a git bash terminal.
If you guys tell me a step by step method on how to install the model it will be very helpful

```download.sh: line 18: wget: command not found
download.sh: line 19: wget: command not found
Downloading CodeLlama-7b
download.sh: line 53: wget: command not found
download.sh: line 56: wget: command not found
download.sh: line 57: wget: command not found
download.sh: line 58: wget: command not found
Checking checksums
md5sum: checklist.chk: No such file or directory```",tantravahi-kalyan,"Hello @notTanveer I'm having the same issue, can you please send a detailed note on how to download codellama in my local and run the download.sh file.

./download.sh: line 18: wget: command not found
./download.sh: line 19: wget: command not found
Downloading CodeLlama-7b
./download.sh: line 59: wget: command not found
./download.sh: line 62: wget: command not found
./download.sh: line 63: wget: command not found
./download.sh: line 64: wget: command not found
Checking checksums
md5sum: checklist.chk: No such file or directory

Thankyou
"
66,how to install wget and md5sum.(Windows user),"I am running all the commands in a git bash terminal.
If you guys tell me a step by step method on how to install the model it will be very helpful

```download.sh: line 18: wget: command not found
download.sh: line 19: wget: command not found
Downloading CodeLlama-7b
download.sh: line 53: wget: command not found
download.sh: line 56: wget: command not found
download.sh: line 57: wget: command not found
download.sh: line 58: wget: command not found
Checking checksums
md5sum: checklist.chk: No such file or directory```",notTanveer,"> Hello @notTanveer I'm having the same issue, can you please send a detailed note on how to download codellama in my local and run the download.sh file.
> 
> ./download.sh: line 18: wget: command not found
> ./download.sh: line 19: wget: command not found
> Downloading CodeLlama-7b
> ./download.sh: line 59: wget: command not found
> ./download.sh: line 62: wget: command not found
> ./download.sh: line 63: wget: command not found
> ./download.sh: line 64: wget: command not found
> Checking checksums
> md5sum: checklist.chk: No such file or directory
> 
> Thankyou
> 

Switch to WSL. "
66,how to install wget and md5sum.(Windows user),"I am running all the commands in a git bash terminal.
If you guys tell me a step by step method on how to install the model it will be very helpful

```download.sh: line 18: wget: command not found
download.sh: line 19: wget: command not found
Downloading CodeLlama-7b
download.sh: line 53: wget: command not found
download.sh: line 56: wget: command not found
download.sh: line 57: wget: command not found
download.sh: line 58: wget: command not found
Checking checksums
md5sum: checklist.chk: No such file or directory```",spikers,">I'm having the same issue, can you please send a detailed note on how to download codellama in my local and run the download.sh file.

WSL should have wget and md5. https://github.com/meta-llama/codellama/issues/66#issuecomment-1700370515 tells you how to download it. If you're on Windows and you have wsl installed, to see which ones are available for download, use this command: `wsl -l -o` You should get a list like this:
```
NAME                                   FRIENDLY NAME
Ubuntu                                 Ubuntu
Debian                                 Debian GNU/Linux
kali-linux                             Kali Linux Rolling
Ubuntu-18.04                           Ubuntu 18.04 LTS
Ubuntu-20.04                           Ubuntu 20.04 LTS
Ubuntu-22.04                           Ubuntu 22.04 LTS
```

I used this one: `wsl --install -d Ubuntu-22.04`

The username/password is kind of arbitrary, so I set up my username to be
User: myname
Pass: mypw

You should be logged in. See the end to see if you're not immediately logged in. Note: WSL will automatically open to your current directory. 

You should get a shell that looks like this, ensure it's the same directory as your llama3 repo with the download.sh. If it isn't, go find it:
`myname@COMPUTERNAME-ABC123:/mnt/c/llama3$`

In the above example, my llama3 is inside `C:/llama3`, but it shows up as `/mnt/c/llama3/`

If you found it, run: `./download.sh`
It asks for the link you got. It should be like: `https://download6.llamameta.net/*?` then other stuff. It's like 600-700 chars long (mine is 687). Paste that in and press enter. 
Then it asks for which models you want, I typed: `8B,8B-instruct`
It should start downloading at this point. Mine looked like this while it downloaded: `./Meta-Llama-3-8B/consolidated.00  68%[=======================================>                   ]  10.18G  16.7MB/s    eta 5m 24s`
Good luck!


### If you aren't immediately logged in (or if you're logged out), do this:
Run `wsl -l -v` to see which distros you have on your machine. I got this:
`Ubuntu-22.04           Stopped         2`

Run this to run WSL with the Ubuntu-22.04 distro: `wsl --distribution Ubuntu-22.04`

Then go to the llama3 folder and run `./download.sh` and follow the instructions from there. 

### Other Errors:
If you get 403 Forbidden, you need to re-request a link. You might get approved immediately. Note: I failed to download it 5x in a row and I had to re-request a link. 

If you get some error that say something like ""out of specified range"", go into download.sh and remove all the `--continue`
  *  For example: Line 19 goes from: `wget --continue ${PRESIGNED_URL/'*'/""LICENSE""} -O ${TARGET_FOLDER}""/LICENSE""` to `wget ${PRESIGNED_URL/'*'/""LICENSE""} -O ${TARGET_FOLDER}""/LICENSE""`
  *  There's 6x `--continue` in the file

If you get some weird bash errors, make sure you installed an ubuntu version.
  *  I remember I did some weird stuff that didn't work. One thing I did was change the first line to replace ""bash"" with ""sh"", then it gave a sequence error, so I changed the sequence line to be like seq -w 00 0 ${SHARD}, it looked like it was doing something, but like I said, that stuff didn't actually download for me. 
"
65,run win10  is error,"i found this  https://github.com/facebookresearch/codellama/issues/55
but  is closed,and  no  solved.

# env:
win10+conda(pytorch-gpu+python3.11)+powershell

# error:

```

(pytorch-gpu) PS F:\aiProject\codellama> torchrun --nproc_per_node 1 example_completion.py --ckpt_dir .\CodeLlama-34b-Python\ --tokenizer_path .\CodeLlama-34b-Python\tokenizer.model --max_seq_len 512 --max_batch_size 4
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
Traceback (most recent call last):
  File ""F:\aiProject\codellama\example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""F:\aiProject\codellama\example_completion.py"", line 20, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""F:\aiProject\codellama\llama\generation.py"", line 68, in build
    torch.distributed.init_process_group(""nccl"")
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\distributed_c10d.py"", line 907, in init_process_group
    default_pg = _new_process_group_helper(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\distributed_c10d.py"", line 1013, in _new_process_group_helper
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 27024) of binary: C:\Users\b\.conda\envs\pytorch-gpu\python.exe
Traceback (most recent call last):
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Scripts\torchrun-script.py"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-30_10:00:59
  host      : Administrator
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 27024)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```


# HOW:
how to use it?

**thanks**
",broken-bytes,"Same issue
"
65,run win10  is error,"i found this  https://github.com/facebookresearch/codellama/issues/55
but  is closed,and  no  solved.

# env:
win10+conda(pytorch-gpu+python3.11)+powershell

# error:

```

(pytorch-gpu) PS F:\aiProject\codellama> torchrun --nproc_per_node 1 example_completion.py --ckpt_dir .\CodeLlama-34b-Python\ --tokenizer_path .\CodeLlama-34b-Python\tokenizer.model --max_seq_len 512 --max_batch_size 4
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
Traceback (most recent call last):
  File ""F:\aiProject\codellama\example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""F:\aiProject\codellama\example_completion.py"", line 20, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""F:\aiProject\codellama\llama\generation.py"", line 68, in build
    torch.distributed.init_process_group(""nccl"")
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\distributed_c10d.py"", line 907, in init_process_group
    default_pg = _new_process_group_helper(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\distributed_c10d.py"", line 1013, in _new_process_group_helper
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 27024) of binary: C:\Users\b\.conda\envs\pytorch-gpu\python.exe
Traceback (most recent call last):
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Scripts\torchrun-script.py"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-30_10:00:59
  host      : Administrator
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 27024)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```


# HOW:
how to use it?

**thanks**
",GaganHonor,"The error message you are getting is:

```
RuntimeError: Distributed package doesn't have NCCL built in
```

This means that the PyTorch distribution you are using does not have the NCCL library built in. NCCL is a library that is used for distributed training of deep learning models. It is required for running the `torchrun` command.

To fix this error, you need to install a PyTorch distribution that has NCCL built in. You can do this by following these steps:

1. Install the `nccl` library. You can do this by running the following command:

```
pip install nccl
```

2. Install a PyTorch distribution that supports NCCL. You can find a list of PyTorch distributions that support NCCL here: https://pytorch.org/get-started/locally/

Once you have installed the `nccl` library and a PyTorch distribution that supports NCCL, you should be able to run the `torchrun` command without getting the error.

Here are some additional things you can try:

* Check that you are using the correct version of PyTorch. The `torchrun` command requires PyTorch version 1.8 or higher.
* Make sure that the `nccl` library is installed in the same location as your PyTorch distribution.
* Try running the `torchrun` command with the `--use_gloo` flag. This will use the Gloo backend instead of NCCL.

If you are still having trouble, you can ask for help here ╰(*°▽°*)╯
I hope this helps!"
65,run win10  is error,"i found this  https://github.com/facebookresearch/codellama/issues/55
but  is closed,and  no  solved.

# env:
win10+conda(pytorch-gpu+python3.11)+powershell

# error:

```

(pytorch-gpu) PS F:\aiProject\codellama> torchrun --nproc_per_node 1 example_completion.py --ckpt_dir .\CodeLlama-34b-Python\ --tokenizer_path .\CodeLlama-34b-Python\tokenizer.model --max_seq_len 512 --max_batch_size 4
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
Traceback (most recent call last):
  File ""F:\aiProject\codellama\example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""F:\aiProject\codellama\example_completion.py"", line 20, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""F:\aiProject\codellama\llama\generation.py"", line 68, in build
    torch.distributed.init_process_group(""nccl"")
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\distributed_c10d.py"", line 907, in init_process_group
    default_pg = _new_process_group_helper(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\distributed_c10d.py"", line 1013, in _new_process_group_helper
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 27024) of binary: C:\Users\b\.conda\envs\pytorch-gpu\python.exe
Traceback (most recent call last):
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Scripts\torchrun-script.py"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-30_10:00:59
  host      : Administrator
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 27024)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```


# HOW:
how to use it?

**thanks**
",broken-bytes,It seems that NCCL is not properly supported on WIndows?
65,run win10  is error,"i found this  https://github.com/facebookresearch/codellama/issues/55
but  is closed,and  no  solved.

# env:
win10+conda(pytorch-gpu+python3.11)+powershell

# error:

```

(pytorch-gpu) PS F:\aiProject\codellama> torchrun --nproc_per_node 1 example_completion.py --ckpt_dir .\CodeLlama-34b-Python\ --tokenizer_path .\CodeLlama-34b-Python\tokenizer.model --max_seq_len 512 --max_batch_size 4
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
Traceback (most recent call last):
  File ""F:\aiProject\codellama\example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""F:\aiProject\codellama\example_completion.py"", line 20, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""F:\aiProject\codellama\llama\generation.py"", line 68, in build
    torch.distributed.init_process_group(""nccl"")
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\distributed_c10d.py"", line 907, in init_process_group
    default_pg = _new_process_group_helper(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\distributed_c10d.py"", line 1013, in _new_process_group_helper
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 27024) of binary: C:\Users\b\.conda\envs\pytorch-gpu\python.exe
Traceback (most recent call last):
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Scripts\torchrun-script.py"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-30_10:00:59
  host      : Administrator
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 27024)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```


# HOW:
how to use it?

**thanks**
",GaganHonor,"Aigo ! That's  why my professor uses Linux but we have hope ╰(*°▽°*)╯

see this https://github.com/facebookresearch/codellama/issues/66#issuecomment-1700370515

@broken-bytes  what do you think 🤔 "
65,run win10  is error,"i found this  https://github.com/facebookresearch/codellama/issues/55
but  is closed,and  no  solved.

# env:
win10+conda(pytorch-gpu+python3.11)+powershell

# error:

```

(pytorch-gpu) PS F:\aiProject\codellama> torchrun --nproc_per_node 1 example_completion.py --ckpt_dir .\CodeLlama-34b-Python\ --tokenizer_path .\CodeLlama-34b-Python\tokenizer.model --max_seq_len 512 --max_batch_size 4
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
Traceback (most recent call last):
  File ""F:\aiProject\codellama\example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""F:\aiProject\codellama\example_completion.py"", line 20, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""F:\aiProject\codellama\llama\generation.py"", line 68, in build
    torch.distributed.init_process_group(""nccl"")
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\distributed_c10d.py"", line 907, in init_process_group
    default_pg = _new_process_group_helper(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\distributed_c10d.py"", line 1013, in _new_process_group_helper
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 27024) of binary: C:\Users\b\.conda\envs\pytorch-gpu\python.exe
Traceback (most recent call last):
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Scripts\torchrun-script.py"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-30_10:00:59
  host      : Administrator
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 27024)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```


# HOW:
how to use it?

**thanks**
",GaganHonor,"yeah you are Right 👉 your system is good enough ! please check this https://stackoverflow.com/questions/68972448/why-is-wsl-extremely-slow-when-compared-with-native-windows-npm-yarn-processing  , It's somewhat similar , Let's discuss more and solve it together if you wish ᓚᘏᗢ"
65,run win10  is error,"i found this  https://github.com/facebookresearch/codellama/issues/55
but  is closed,and  no  solved.

# env:
win10+conda(pytorch-gpu+python3.11)+powershell

# error:

```

(pytorch-gpu) PS F:\aiProject\codellama> torchrun --nproc_per_node 1 example_completion.py --ckpt_dir .\CodeLlama-34b-Python\ --tokenizer_path .\CodeLlama-34b-Python\tokenizer.model --max_seq_len 512 --max_batch_size 4
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
Traceback (most recent call last):
  File ""F:\aiProject\codellama\example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""F:\aiProject\codellama\example_completion.py"", line 20, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""F:\aiProject\codellama\llama\generation.py"", line 68, in build
    torch.distributed.init_process_group(""nccl"")
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\distributed_c10d.py"", line 907, in init_process_group
    default_pg = _new_process_group_helper(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\distributed_c10d.py"", line 1013, in _new_process_group_helper
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 27024) of binary: C:\Users\b\.conda\envs\pytorch-gpu\python.exe
Traceback (most recent call last):
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Scripts\torchrun-script.py"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-30_10:00:59
  host      : Administrator
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 27024)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```


# HOW:
how to use it?

**thanks**
",realhaik,"Works perfect on Windows, this is how I load it:
Also, I have benchmarked Windows vs Linux and the inference times are exactly the same.
```

temperature  = 0
top_p  = 0
max_seq_len  = 4096
max_batch_size  = 1
max_gen_len  = None
num_of_worlds = 1

torch.distributed.init_process_group(backend='gloo', init_method='tcp://localhost:23455', world_size=num_of_worlds, rank=0)


generator = Llama.build(
    
    ckpt_dir=""C:/AI/LLaMA2_Docker_FileSystem/codellama/CodeLlama-7b-Instruct"",
    tokenizer_path=""C:/AI/LLaMA2_Docker_FileSystem/codellama/CodeLlama-7b-Instruct/tokenizer.model"",
    max_seq_len=max_seq_len,
    max_batch_size=max_batch_size,
    model_parallel_size = num_of_worlds
)

```
"
65,run win10  is error,"i found this  https://github.com/facebookresearch/codellama/issues/55
but  is closed,and  no  solved.

# env:
win10+conda(pytorch-gpu+python3.11)+powershell

# error:

```

(pytorch-gpu) PS F:\aiProject\codellama> torchrun --nproc_per_node 1 example_completion.py --ckpt_dir .\CodeLlama-34b-Python\ --tokenizer_path .\CodeLlama-34b-Python\tokenizer.model --max_seq_len 512 --max_batch_size 4
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
Traceback (most recent call last):
  File ""F:\aiProject\codellama\example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""F:\aiProject\codellama\example_completion.py"", line 20, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""F:\aiProject\codellama\llama\generation.py"", line 68, in build
    torch.distributed.init_process_group(""nccl"")
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\distributed_c10d.py"", line 907, in init_process_group
    default_pg = _new_process_group_helper(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\distributed_c10d.py"", line 1013, in _new_process_group_helper
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 27024) of binary: C:\Users\b\.conda\envs\pytorch-gpu\python.exe
Traceback (most recent call last):
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Scripts\torchrun-script.py"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-30_10:00:59
  host      : Administrator
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 27024)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```


# HOW:
how to use it?

**thanks**
",GaganHonor,underrated comment by @realhaik ❇️
64,Finetuning 7B codellama: Runtime error ,"Am trying to finetune codellama with the same idea of llama2 and using the same script to finetune. 
Am not sure whether am right as the repo or blog not talking about finetune approach. 

Am facing this error. RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504


RuntimeError                              Traceback (most recent call last)
Cell In[10], line 29
     20 trainer = Trainer(
     21     model=model,
     22     args=training_args,
   (...)
     25     callbacks=[profiler_callback] if enable_profiler else [],
     26 )
     28 # Start training
---> 29 trainer.train()

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1662, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1657     self.model_wrapped = self.model
   1659 inner_training_loop = find_executable_batch_size(
   1660     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1661 )
-> 1662 return inner_training_loop(
   1663     args=args,
   1664     resume_from_checkpoint=resume_from_checkpoint,
   1665     trial=trial,
   1666     ignore_keys_for_eval=ignore_keys_for_eval,
   1667 )

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1929, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1927         tr_loss_step = self.training_step(model, inputs)
   1928 else:
-> 1929     tr_loss_step = self.training_step(model, inputs)
   1931 if (
   1932     args.logging_nan_inf_filter
   1933     and not is_torch_tpu_available()
   1934     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   1935 ):
   1936     # if loss is nan or inf simply add the average of previous logged losses
   1937     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2699, in Trainer.training_step(self, model, inputs)
   2696     return loss_mb.reduce_mean().detach().to(self.args.device)
   2698 with self.compute_loss_context_manager():
-> 2699     loss = self.compute_loss(model, inputs)
   2701 if self.args.n_gpu > 1:
   2702     loss = loss.mean()  # mean() to average on multi-gpu parallel training

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2731, in Trainer.compute_loss(self, model, inputs, return_outputs)
   2729 else:
   2730     labels = None
-> 2731 outputs = model(**inputs)
   2732 # Save past state if it exists
   2733 # TODO: this needs to be fixed and made cleaner later.
   2734 if self.args.past_index >= 0:

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/peft/peft_model.py:947, in PeftModelForCausalLM.forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)
    936             raise AssertionError(""forward in MPTForCausalLM does not support inputs_embeds"")
    937         return self.base_model(
    938             input_ids=input_ids,
    939             attention_mask=attention_mask,
   (...)
    944             **kwargs,
    945         )
--> 947     return self.base_model(
    948         input_ids=input_ids,
    949         attention_mask=attention_mask,
    950         inputs_embeds=inputs_embeds,
    951         labels=labels,
    952         output_attentions=output_attentions,
    953         output_hidden_states=output_hidden_states,
    954         return_dict=return_dict,
    955         **kwargs,
    956     )
    958 batch_size = input_ids.shape[0]
    959 if attention_mask is not None:
    960     # concat prompt attention mask

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/accelerate/hooks.py:165, in add_hook_to_module.<locals>.new_forward(*args, **kwargs)
    163         output = old_forward(*args, **kwargs)
    164 else:
--> 165     output = old_forward(*args, **kwargs)
    166 return module._hf_hook.post_forward(module, output)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:709, in LlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
    707 # Flatten the tokens
    708 loss_fct = CrossEntropyLoss()
--> 709 shift_logits = shift_logits.view(-1, self.config.vocab_size)
    710 shift_labels = shift_labels.view(-1)
    711 # Enable model parallelism

RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504

​",zhanghainan,the same issue
64,Finetuning 7B codellama: Runtime error ,"Am trying to finetune codellama with the same idea of llama2 and using the same script to finetune. 
Am not sure whether am right as the repo or blog not talking about finetune approach. 

Am facing this error. RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504


RuntimeError                              Traceback (most recent call last)
Cell In[10], line 29
     20 trainer = Trainer(
     21     model=model,
     22     args=training_args,
   (...)
     25     callbacks=[profiler_callback] if enable_profiler else [],
     26 )
     28 # Start training
---> 29 trainer.train()

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1662, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1657     self.model_wrapped = self.model
   1659 inner_training_loop = find_executable_batch_size(
   1660     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1661 )
-> 1662 return inner_training_loop(
   1663     args=args,
   1664     resume_from_checkpoint=resume_from_checkpoint,
   1665     trial=trial,
   1666     ignore_keys_for_eval=ignore_keys_for_eval,
   1667 )

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1929, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1927         tr_loss_step = self.training_step(model, inputs)
   1928 else:
-> 1929     tr_loss_step = self.training_step(model, inputs)
   1931 if (
   1932     args.logging_nan_inf_filter
   1933     and not is_torch_tpu_available()
   1934     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   1935 ):
   1936     # if loss is nan or inf simply add the average of previous logged losses
   1937     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2699, in Trainer.training_step(self, model, inputs)
   2696     return loss_mb.reduce_mean().detach().to(self.args.device)
   2698 with self.compute_loss_context_manager():
-> 2699     loss = self.compute_loss(model, inputs)
   2701 if self.args.n_gpu > 1:
   2702     loss = loss.mean()  # mean() to average on multi-gpu parallel training

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2731, in Trainer.compute_loss(self, model, inputs, return_outputs)
   2729 else:
   2730     labels = None
-> 2731 outputs = model(**inputs)
   2732 # Save past state if it exists
   2733 # TODO: this needs to be fixed and made cleaner later.
   2734 if self.args.past_index >= 0:

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/peft/peft_model.py:947, in PeftModelForCausalLM.forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)
    936             raise AssertionError(""forward in MPTForCausalLM does not support inputs_embeds"")
    937         return self.base_model(
    938             input_ids=input_ids,
    939             attention_mask=attention_mask,
   (...)
    944             **kwargs,
    945         )
--> 947     return self.base_model(
    948         input_ids=input_ids,
    949         attention_mask=attention_mask,
    950         inputs_embeds=inputs_embeds,
    951         labels=labels,
    952         output_attentions=output_attentions,
    953         output_hidden_states=output_hidden_states,
    954         return_dict=return_dict,
    955         **kwargs,
    956     )
    958 batch_size = input_ids.shape[0]
    959 if attention_mask is not None:
    960     # concat prompt attention mask

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/accelerate/hooks.py:165, in add_hook_to_module.<locals>.new_forward(*args, **kwargs)
    163         output = old_forward(*args, **kwargs)
    164 else:
--> 165     output = old_forward(*args, **kwargs)
    166 return module._hf_hook.post_forward(module, output)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:709, in LlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
    707 # Flatten the tokens
    708 loss_fct = CrossEntropyLoss()
--> 709 shift_logits = shift_logits.view(-1, self.config.vocab_size)
    710 shift_labels = shift_labels.view(-1)
    711 # Enable model parallelism

RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504

​",Kushalamummigatti,"@zhanghainan Are you using adapter weights or converted weights? Have you thought of any other way to finetune?
"
64,Finetuning 7B codellama: Runtime error ,"Am trying to finetune codellama with the same idea of llama2 and using the same script to finetune. 
Am not sure whether am right as the repo or blog not talking about finetune approach. 

Am facing this error. RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504


RuntimeError                              Traceback (most recent call last)
Cell In[10], line 29
     20 trainer = Trainer(
     21     model=model,
     22     args=training_args,
   (...)
     25     callbacks=[profiler_callback] if enable_profiler else [],
     26 )
     28 # Start training
---> 29 trainer.train()

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1662, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1657     self.model_wrapped = self.model
   1659 inner_training_loop = find_executable_batch_size(
   1660     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1661 )
-> 1662 return inner_training_loop(
   1663     args=args,
   1664     resume_from_checkpoint=resume_from_checkpoint,
   1665     trial=trial,
   1666     ignore_keys_for_eval=ignore_keys_for_eval,
   1667 )

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1929, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1927         tr_loss_step = self.training_step(model, inputs)
   1928 else:
-> 1929     tr_loss_step = self.training_step(model, inputs)
   1931 if (
   1932     args.logging_nan_inf_filter
   1933     and not is_torch_tpu_available()
   1934     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   1935 ):
   1936     # if loss is nan or inf simply add the average of previous logged losses
   1937     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2699, in Trainer.training_step(self, model, inputs)
   2696     return loss_mb.reduce_mean().detach().to(self.args.device)
   2698 with self.compute_loss_context_manager():
-> 2699     loss = self.compute_loss(model, inputs)
   2701 if self.args.n_gpu > 1:
   2702     loss = loss.mean()  # mean() to average on multi-gpu parallel training

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2731, in Trainer.compute_loss(self, model, inputs, return_outputs)
   2729 else:
   2730     labels = None
-> 2731 outputs = model(**inputs)
   2732 # Save past state if it exists
   2733 # TODO: this needs to be fixed and made cleaner later.
   2734 if self.args.past_index >= 0:

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/peft/peft_model.py:947, in PeftModelForCausalLM.forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)
    936             raise AssertionError(""forward in MPTForCausalLM does not support inputs_embeds"")
    937         return self.base_model(
    938             input_ids=input_ids,
    939             attention_mask=attention_mask,
   (...)
    944             **kwargs,
    945         )
--> 947     return self.base_model(
    948         input_ids=input_ids,
    949         attention_mask=attention_mask,
    950         inputs_embeds=inputs_embeds,
    951         labels=labels,
    952         output_attentions=output_attentions,
    953         output_hidden_states=output_hidden_states,
    954         return_dict=return_dict,
    955         **kwargs,
    956     )
    958 batch_size = input_ids.shape[0]
    959 if attention_mask is not None:
    960     # concat prompt attention mask

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/accelerate/hooks.py:165, in add_hook_to_module.<locals>.new_forward(*args, **kwargs)
    163         output = old_forward(*args, **kwargs)
    164 else:
--> 165     output = old_forward(*args, **kwargs)
    166 return module._hf_hook.post_forward(module, output)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:709, in LlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
    707 # Flatten the tokens
    708 loss_fct = CrossEntropyLoss()
--> 709 shift_logits = shift_logits.view(-1, self.config.vocab_size)
    710 shift_labels = shift_labels.view(-1)
    711 # Enable model parallelism

RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504

​",zhanghainan,I convert the weights to huggingface(https://github.com/FlagAlpha/Llama2-Chinese/tree/main/scripts/convert2hf)，and utilize the https://github.com/FlagAlpha/Llama2-Chinese/tree/main/train/sft/finetune.sh to finetune.  Then I got the same issue. 
64,Finetuning 7B codellama: Runtime error ,"Am trying to finetune codellama with the same idea of llama2 and using the same script to finetune. 
Am not sure whether am right as the repo or blog not talking about finetune approach. 

Am facing this error. RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504


RuntimeError                              Traceback (most recent call last)
Cell In[10], line 29
     20 trainer = Trainer(
     21     model=model,
     22     args=training_args,
   (...)
     25     callbacks=[profiler_callback] if enable_profiler else [],
     26 )
     28 # Start training
---> 29 trainer.train()

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1662, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1657     self.model_wrapped = self.model
   1659 inner_training_loop = find_executable_batch_size(
   1660     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1661 )
-> 1662 return inner_training_loop(
   1663     args=args,
   1664     resume_from_checkpoint=resume_from_checkpoint,
   1665     trial=trial,
   1666     ignore_keys_for_eval=ignore_keys_for_eval,
   1667 )

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1929, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1927         tr_loss_step = self.training_step(model, inputs)
   1928 else:
-> 1929     tr_loss_step = self.training_step(model, inputs)
   1931 if (
   1932     args.logging_nan_inf_filter
   1933     and not is_torch_tpu_available()
   1934     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   1935 ):
   1936     # if loss is nan or inf simply add the average of previous logged losses
   1937     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2699, in Trainer.training_step(self, model, inputs)
   2696     return loss_mb.reduce_mean().detach().to(self.args.device)
   2698 with self.compute_loss_context_manager():
-> 2699     loss = self.compute_loss(model, inputs)
   2701 if self.args.n_gpu > 1:
   2702     loss = loss.mean()  # mean() to average on multi-gpu parallel training

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2731, in Trainer.compute_loss(self, model, inputs, return_outputs)
   2729 else:
   2730     labels = None
-> 2731 outputs = model(**inputs)
   2732 # Save past state if it exists
   2733 # TODO: this needs to be fixed and made cleaner later.
   2734 if self.args.past_index >= 0:

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/peft/peft_model.py:947, in PeftModelForCausalLM.forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)
    936             raise AssertionError(""forward in MPTForCausalLM does not support inputs_embeds"")
    937         return self.base_model(
    938             input_ids=input_ids,
    939             attention_mask=attention_mask,
   (...)
    944             **kwargs,
    945         )
--> 947     return self.base_model(
    948         input_ids=input_ids,
    949         attention_mask=attention_mask,
    950         inputs_embeds=inputs_embeds,
    951         labels=labels,
    952         output_attentions=output_attentions,
    953         output_hidden_states=output_hidden_states,
    954         return_dict=return_dict,
    955         **kwargs,
    956     )
    958 batch_size = input_ids.shape[0]
    959 if attention_mask is not None:
    960     # concat prompt attention mask

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/accelerate/hooks.py:165, in add_hook_to_module.<locals>.new_forward(*args, **kwargs)
    163         output = old_forward(*args, **kwargs)
    164 else:
--> 165     output = old_forward(*args, **kwargs)
    166 return module._hf_hook.post_forward(module, output)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:709, in LlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
    707 # Flatten the tokens
    708 loss_fct = CrossEntropyLoss()
--> 709 shift_logits = shift_logits.view(-1, self.config.vocab_size)
    710 shift_labels = shift_labels.view(-1)
    711 # Enable model parallelism

RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504

​",zhanghainan,"I find that the vocab size is ""vocab_size"": 32016 rather than 32000. I set the ""vocab_size"" as 32016 and then successful."
64,Finetuning 7B codellama: Runtime error ,"Am trying to finetune codellama with the same idea of llama2 and using the same script to finetune. 
Am not sure whether am right as the repo or blog not talking about finetune approach. 

Am facing this error. RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504


RuntimeError                              Traceback (most recent call last)
Cell In[10], line 29
     20 trainer = Trainer(
     21     model=model,
     22     args=training_args,
   (...)
     25     callbacks=[profiler_callback] if enable_profiler else [],
     26 )
     28 # Start training
---> 29 trainer.train()

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1662, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1657     self.model_wrapped = self.model
   1659 inner_training_loop = find_executable_batch_size(
   1660     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1661 )
-> 1662 return inner_training_loop(
   1663     args=args,
   1664     resume_from_checkpoint=resume_from_checkpoint,
   1665     trial=trial,
   1666     ignore_keys_for_eval=ignore_keys_for_eval,
   1667 )

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1929, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1927         tr_loss_step = self.training_step(model, inputs)
   1928 else:
-> 1929     tr_loss_step = self.training_step(model, inputs)
   1931 if (
   1932     args.logging_nan_inf_filter
   1933     and not is_torch_tpu_available()
   1934     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   1935 ):
   1936     # if loss is nan or inf simply add the average of previous logged losses
   1937     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2699, in Trainer.training_step(self, model, inputs)
   2696     return loss_mb.reduce_mean().detach().to(self.args.device)
   2698 with self.compute_loss_context_manager():
-> 2699     loss = self.compute_loss(model, inputs)
   2701 if self.args.n_gpu > 1:
   2702     loss = loss.mean()  # mean() to average on multi-gpu parallel training

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2731, in Trainer.compute_loss(self, model, inputs, return_outputs)
   2729 else:
   2730     labels = None
-> 2731 outputs = model(**inputs)
   2732 # Save past state if it exists
   2733 # TODO: this needs to be fixed and made cleaner later.
   2734 if self.args.past_index >= 0:

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/peft/peft_model.py:947, in PeftModelForCausalLM.forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)
    936             raise AssertionError(""forward in MPTForCausalLM does not support inputs_embeds"")
    937         return self.base_model(
    938             input_ids=input_ids,
    939             attention_mask=attention_mask,
   (...)
    944             **kwargs,
    945         )
--> 947     return self.base_model(
    948         input_ids=input_ids,
    949         attention_mask=attention_mask,
    950         inputs_embeds=inputs_embeds,
    951         labels=labels,
    952         output_attentions=output_attentions,
    953         output_hidden_states=output_hidden_states,
    954         return_dict=return_dict,
    955         **kwargs,
    956     )
    958 batch_size = input_ids.shape[0]
    959 if attention_mask is not None:
    960     # concat prompt attention mask

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/accelerate/hooks.py:165, in add_hook_to_module.<locals>.new_forward(*args, **kwargs)
    163         output = old_forward(*args, **kwargs)
    164 else:
--> 165     output = old_forward(*args, **kwargs)
    166 return module._hf_hook.post_forward(module, output)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:709, in LlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
    707 # Flatten the tokens
    708 loss_fct = CrossEntropyLoss()
--> 709 shift_logits = shift_logits.view(-1, self.config.vocab_size)
    710 shift_labels = shift_labels.view(-1)
    711 # Enable model parallelism

RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504

​",Kushalamummigatti,"@zhanghainan Could you please 
share me code snippet to update the vocab size. "
64,Finetuning 7B codellama: Runtime error ,"Am trying to finetune codellama with the same idea of llama2 and using the same script to finetune. 
Am not sure whether am right as the repo or blog not talking about finetune approach. 

Am facing this error. RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504


RuntimeError                              Traceback (most recent call last)
Cell In[10], line 29
     20 trainer = Trainer(
     21     model=model,
     22     args=training_args,
   (...)
     25     callbacks=[profiler_callback] if enable_profiler else [],
     26 )
     28 # Start training
---> 29 trainer.train()

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1662, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1657     self.model_wrapped = self.model
   1659 inner_training_loop = find_executable_batch_size(
   1660     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1661 )
-> 1662 return inner_training_loop(
   1663     args=args,
   1664     resume_from_checkpoint=resume_from_checkpoint,
   1665     trial=trial,
   1666     ignore_keys_for_eval=ignore_keys_for_eval,
   1667 )

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1929, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1927         tr_loss_step = self.training_step(model, inputs)
   1928 else:
-> 1929     tr_loss_step = self.training_step(model, inputs)
   1931 if (
   1932     args.logging_nan_inf_filter
   1933     and not is_torch_tpu_available()
   1934     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   1935 ):
   1936     # if loss is nan or inf simply add the average of previous logged losses
   1937     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2699, in Trainer.training_step(self, model, inputs)
   2696     return loss_mb.reduce_mean().detach().to(self.args.device)
   2698 with self.compute_loss_context_manager():
-> 2699     loss = self.compute_loss(model, inputs)
   2701 if self.args.n_gpu > 1:
   2702     loss = loss.mean()  # mean() to average on multi-gpu parallel training

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2731, in Trainer.compute_loss(self, model, inputs, return_outputs)
   2729 else:
   2730     labels = None
-> 2731 outputs = model(**inputs)
   2732 # Save past state if it exists
   2733 # TODO: this needs to be fixed and made cleaner later.
   2734 if self.args.past_index >= 0:

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/peft/peft_model.py:947, in PeftModelForCausalLM.forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)
    936             raise AssertionError(""forward in MPTForCausalLM does not support inputs_embeds"")
    937         return self.base_model(
    938             input_ids=input_ids,
    939             attention_mask=attention_mask,
   (...)
    944             **kwargs,
    945         )
--> 947     return self.base_model(
    948         input_ids=input_ids,
    949         attention_mask=attention_mask,
    950         inputs_embeds=inputs_embeds,
    951         labels=labels,
    952         output_attentions=output_attentions,
    953         output_hidden_states=output_hidden_states,
    954         return_dict=return_dict,
    955         **kwargs,
    956     )
    958 batch_size = input_ids.shape[0]
    959 if attention_mask is not None:
    960     # concat prompt attention mask

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/accelerate/hooks.py:165, in add_hook_to_module.<locals>.new_forward(*args, **kwargs)
    163         output = old_forward(*args, **kwargs)
    164 else:
--> 165     output = old_forward(*args, **kwargs)
    166 return module._hf_hook.post_forward(module, output)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:709, in LlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
    707 # Flatten the tokens
    708 loss_fct = CrossEntropyLoss()
--> 709 shift_logits = shift_logits.view(-1, self.config.vocab_size)
    710 shift_labels = shift_labels.view(-1)
    711 # Enable model parallelism

RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504

​",zhanghainan,"I convert the model as hugging face model. In config.json,  I set the vocab size.
![image](https://github.com/facebookresearch/codellama/assets/7510008/93cc429c-54bb-41d0-9fff-435eb2e0b7bf)
"
64,Finetuning 7B codellama: Runtime error ,"Am trying to finetune codellama with the same idea of llama2 and using the same script to finetune. 
Am not sure whether am right as the repo or blog not talking about finetune approach. 

Am facing this error. RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504


RuntimeError                              Traceback (most recent call last)
Cell In[10], line 29
     20 trainer = Trainer(
     21     model=model,
     22     args=training_args,
   (...)
     25     callbacks=[profiler_callback] if enable_profiler else [],
     26 )
     28 # Start training
---> 29 trainer.train()

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1662, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1657     self.model_wrapped = self.model
   1659 inner_training_loop = find_executable_batch_size(
   1660     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1661 )
-> 1662 return inner_training_loop(
   1663     args=args,
   1664     resume_from_checkpoint=resume_from_checkpoint,
   1665     trial=trial,
   1666     ignore_keys_for_eval=ignore_keys_for_eval,
   1667 )

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1929, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1927         tr_loss_step = self.training_step(model, inputs)
   1928 else:
-> 1929     tr_loss_step = self.training_step(model, inputs)
   1931 if (
   1932     args.logging_nan_inf_filter
   1933     and not is_torch_tpu_available()
   1934     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   1935 ):
   1936     # if loss is nan or inf simply add the average of previous logged losses
   1937     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2699, in Trainer.training_step(self, model, inputs)
   2696     return loss_mb.reduce_mean().detach().to(self.args.device)
   2698 with self.compute_loss_context_manager():
-> 2699     loss = self.compute_loss(model, inputs)
   2701 if self.args.n_gpu > 1:
   2702     loss = loss.mean()  # mean() to average on multi-gpu parallel training

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2731, in Trainer.compute_loss(self, model, inputs, return_outputs)
   2729 else:
   2730     labels = None
-> 2731 outputs = model(**inputs)
   2732 # Save past state if it exists
   2733 # TODO: this needs to be fixed and made cleaner later.
   2734 if self.args.past_index >= 0:

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/peft/peft_model.py:947, in PeftModelForCausalLM.forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)
    936             raise AssertionError(""forward in MPTForCausalLM does not support inputs_embeds"")
    937         return self.base_model(
    938             input_ids=input_ids,
    939             attention_mask=attention_mask,
   (...)
    944             **kwargs,
    945         )
--> 947     return self.base_model(
    948         input_ids=input_ids,
    949         attention_mask=attention_mask,
    950         inputs_embeds=inputs_embeds,
    951         labels=labels,
    952         output_attentions=output_attentions,
    953         output_hidden_states=output_hidden_states,
    954         return_dict=return_dict,
    955         **kwargs,
    956     )
    958 batch_size = input_ids.shape[0]
    959 if attention_mask is not None:
    960     # concat prompt attention mask

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/accelerate/hooks.py:165, in add_hook_to_module.<locals>.new_forward(*args, **kwargs)
    163         output = old_forward(*args, **kwargs)
    164 else:
--> 165     output = old_forward(*args, **kwargs)
    166 return module._hf_hook.post_forward(module, output)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:709, in LlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
    707 # Flatten the tokens
    708 loss_fct = CrossEntropyLoss()
--> 709 shift_logits = shift_logits.view(-1, self.config.vocab_size)
    710 shift_labels = shift_labels.view(-1)
    711 # Enable model parallelism

RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504

​",Kushalamummigatti,"@zhanghainan Strange. Even i have converted the model. In config.json vocab size is as below image.
<img width=""322"" alt=""image"" src=""https://github.com/facebookresearch/codellama/assets/62338340/3249769c-4ef8-4017-9015-e5a28ba11702"">

But when i print the tokenizer its like below 
<img width=""766"" alt=""image"" src=""https://github.com/facebookresearch/codellama/assets/62338340/a8f3c724-f9b7-44f3-993e-207610a7b6c5"">
"
64,Finetuning 7B codellama: Runtime error ,"Am trying to finetune codellama with the same idea of llama2 and using the same script to finetune. 
Am not sure whether am right as the repo or blog not talking about finetune approach. 

Am facing this error. RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504


RuntimeError                              Traceback (most recent call last)
Cell In[10], line 29
     20 trainer = Trainer(
     21     model=model,
     22     args=training_args,
   (...)
     25     callbacks=[profiler_callback] if enable_profiler else [],
     26 )
     28 # Start training
---> 29 trainer.train()

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1662, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1657     self.model_wrapped = self.model
   1659 inner_training_loop = find_executable_batch_size(
   1660     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1661 )
-> 1662 return inner_training_loop(
   1663     args=args,
   1664     resume_from_checkpoint=resume_from_checkpoint,
   1665     trial=trial,
   1666     ignore_keys_for_eval=ignore_keys_for_eval,
   1667 )

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1929, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1927         tr_loss_step = self.training_step(model, inputs)
   1928 else:
-> 1929     tr_loss_step = self.training_step(model, inputs)
   1931 if (
   1932     args.logging_nan_inf_filter
   1933     and not is_torch_tpu_available()
   1934     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   1935 ):
   1936     # if loss is nan or inf simply add the average of previous logged losses
   1937     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2699, in Trainer.training_step(self, model, inputs)
   2696     return loss_mb.reduce_mean().detach().to(self.args.device)
   2698 with self.compute_loss_context_manager():
-> 2699     loss = self.compute_loss(model, inputs)
   2701 if self.args.n_gpu > 1:
   2702     loss = loss.mean()  # mean() to average on multi-gpu parallel training

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2731, in Trainer.compute_loss(self, model, inputs, return_outputs)
   2729 else:
   2730     labels = None
-> 2731 outputs = model(**inputs)
   2732 # Save past state if it exists
   2733 # TODO: this needs to be fixed and made cleaner later.
   2734 if self.args.past_index >= 0:

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/peft/peft_model.py:947, in PeftModelForCausalLM.forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)
    936             raise AssertionError(""forward in MPTForCausalLM does not support inputs_embeds"")
    937         return self.base_model(
    938             input_ids=input_ids,
    939             attention_mask=attention_mask,
   (...)
    944             **kwargs,
    945         )
--> 947     return self.base_model(
    948         input_ids=input_ids,
    949         attention_mask=attention_mask,
    950         inputs_embeds=inputs_embeds,
    951         labels=labels,
    952         output_attentions=output_attentions,
    953         output_hidden_states=output_hidden_states,
    954         return_dict=return_dict,
    955         **kwargs,
    956     )
    958 batch_size = input_ids.shape[0]
    959 if attention_mask is not None:
    960     # concat prompt attention mask

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/accelerate/hooks.py:165, in add_hook_to_module.<locals>.new_forward(*args, **kwargs)
    163         output = old_forward(*args, **kwargs)
    164 else:
--> 165     output = old_forward(*args, **kwargs)
    166 return module._hf_hook.post_forward(module, output)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:709, in LlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
    707 # Flatten the tokens
    708 loss_fct = CrossEntropyLoss()
--> 709 shift_logits = shift_logits.view(-1, self.config.vocab_size)
    710 shift_labels = shift_labels.view(-1)
    711 # Enable model parallelism

RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504

​",CarolXh,"I have the same issue and solve it in your way. However, when I do the inference, I encounter a problem as following and it can not do a simple inference correctly. Have you met the problem?
**warning info:** 
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /data/sonald/projects/codellama/codellama-sft and are newly initialized: ['model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
64,Finetuning 7B codellama: Runtime error ,"Am trying to finetune codellama with the same idea of llama2 and using the same script to finetune. 
Am not sure whether am right as the repo or blog not talking about finetune approach. 

Am facing this error. RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504


RuntimeError                              Traceback (most recent call last)
Cell In[10], line 29
     20 trainer = Trainer(
     21     model=model,
     22     args=training_args,
   (...)
     25     callbacks=[profiler_callback] if enable_profiler else [],
     26 )
     28 # Start training
---> 29 trainer.train()

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1662, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1657     self.model_wrapped = self.model
   1659 inner_training_loop = find_executable_batch_size(
   1660     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1661 )
-> 1662 return inner_training_loop(
   1663     args=args,
   1664     resume_from_checkpoint=resume_from_checkpoint,
   1665     trial=trial,
   1666     ignore_keys_for_eval=ignore_keys_for_eval,
   1667 )

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1929, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1927         tr_loss_step = self.training_step(model, inputs)
   1928 else:
-> 1929     tr_loss_step = self.training_step(model, inputs)
   1931 if (
   1932     args.logging_nan_inf_filter
   1933     and not is_torch_tpu_available()
   1934     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   1935 ):
   1936     # if loss is nan or inf simply add the average of previous logged losses
   1937     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2699, in Trainer.training_step(self, model, inputs)
   2696     return loss_mb.reduce_mean().detach().to(self.args.device)
   2698 with self.compute_loss_context_manager():
-> 2699     loss = self.compute_loss(model, inputs)
   2701 if self.args.n_gpu > 1:
   2702     loss = loss.mean()  # mean() to average on multi-gpu parallel training

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2731, in Trainer.compute_loss(self, model, inputs, return_outputs)
   2729 else:
   2730     labels = None
-> 2731 outputs = model(**inputs)
   2732 # Save past state if it exists
   2733 # TODO: this needs to be fixed and made cleaner later.
   2734 if self.args.past_index >= 0:

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/peft/peft_model.py:947, in PeftModelForCausalLM.forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)
    936             raise AssertionError(""forward in MPTForCausalLM does not support inputs_embeds"")
    937         return self.base_model(
    938             input_ids=input_ids,
    939             attention_mask=attention_mask,
   (...)
    944             **kwargs,
    945         )
--> 947     return self.base_model(
    948         input_ids=input_ids,
    949         attention_mask=attention_mask,
    950         inputs_embeds=inputs_embeds,
    951         labels=labels,
    952         output_attentions=output_attentions,
    953         output_hidden_states=output_hidden_states,
    954         return_dict=return_dict,
    955         **kwargs,
    956     )
    958 batch_size = input_ids.shape[0]
    959 if attention_mask is not None:
    960     # concat prompt attention mask

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/accelerate/hooks.py:165, in add_hook_to_module.<locals>.new_forward(*args, **kwargs)
    163         output = old_forward(*args, **kwargs)
    164 else:
--> 165     output = old_forward(*args, **kwargs)
    166 return module._hf_hook.post_forward(module, output)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:709, in LlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
    707 # Flatten the tokens
    708 loss_fct = CrossEntropyLoss()
--> 709 shift_logits = shift_logits.view(-1, self.config.vocab_size)
    710 shift_labels = shift_labels.view(-1)
    711 # Enable model parallelism

RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504

​",zhanghainan,"@CarolXh I utilize the code like bellow， and it works successfully in inference.

> import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel,PeftConfig
finetune_model_path='../../models/sft_codellama'
config = PeftConfig.from_pretrained(finetune_model_path)
base_model_name_or_path='../../models/codellama-hf'
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,use_fast=False)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,device_map='auto',torch_dtype=torch.float16,load_in_8bit=True)
model = PeftModel.from_pretrained(model, finetune_model_path, device_map={"""": 0})
model =model.eval()
input_ids = tokenizer([""<s>[INST]XXX[/INST] ""], return_tensors=""pt"",add_special_tokens=False).input_ids.to('cuda')        
generate_input = {
    ""input_ids"":input_ids,
    ""max_new_tokens"":512,
    ""do_sample"":True,
    ""top_k"":50,
    ""top_p"":0.95,
    ""temperature"":0.3,
    ""repetition_penalty"":1.3,
    ""eos_token_id"":tokenizer.eos_token_id,
    ""bos_token_id"":tokenizer.bos_token_id,
    ""pad_token_id"":tokenizer.pad_token_id
}
generate_ids  = model.generate(**generate_input)
text = tokenizer.decode(generate_ids[0])
>print(text)"
64,Finetuning 7B codellama: Runtime error ,"Am trying to finetune codellama with the same idea of llama2 and using the same script to finetune. 
Am not sure whether am right as the repo or blog not talking about finetune approach. 

Am facing this error. RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504


RuntimeError                              Traceback (most recent call last)
Cell In[10], line 29
     20 trainer = Trainer(
     21     model=model,
     22     args=training_args,
   (...)
     25     callbacks=[profiler_callback] if enable_profiler else [],
     26 )
     28 # Start training
---> 29 trainer.train()

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1662, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1657     self.model_wrapped = self.model
   1659 inner_training_loop = find_executable_batch_size(
   1660     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1661 )
-> 1662 return inner_training_loop(
   1663     args=args,
   1664     resume_from_checkpoint=resume_from_checkpoint,
   1665     trial=trial,
   1666     ignore_keys_for_eval=ignore_keys_for_eval,
   1667 )

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1929, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1927         tr_loss_step = self.training_step(model, inputs)
   1928 else:
-> 1929     tr_loss_step = self.training_step(model, inputs)
   1931 if (
   1932     args.logging_nan_inf_filter
   1933     and not is_torch_tpu_available()
   1934     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   1935 ):
   1936     # if loss is nan or inf simply add the average of previous logged losses
   1937     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2699, in Trainer.training_step(self, model, inputs)
   2696     return loss_mb.reduce_mean().detach().to(self.args.device)
   2698 with self.compute_loss_context_manager():
-> 2699     loss = self.compute_loss(model, inputs)
   2701 if self.args.n_gpu > 1:
   2702     loss = loss.mean()  # mean() to average on multi-gpu parallel training

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2731, in Trainer.compute_loss(self, model, inputs, return_outputs)
   2729 else:
   2730     labels = None
-> 2731 outputs = model(**inputs)
   2732 # Save past state if it exists
   2733 # TODO: this needs to be fixed and made cleaner later.
   2734 if self.args.past_index >= 0:

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/peft/peft_model.py:947, in PeftModelForCausalLM.forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)
    936             raise AssertionError(""forward in MPTForCausalLM does not support inputs_embeds"")
    937         return self.base_model(
    938             input_ids=input_ids,
    939             attention_mask=attention_mask,
   (...)
    944             **kwargs,
    945         )
--> 947     return self.base_model(
    948         input_ids=input_ids,
    949         attention_mask=attention_mask,
    950         inputs_embeds=inputs_embeds,
    951         labels=labels,
    952         output_attentions=output_attentions,
    953         output_hidden_states=output_hidden_states,
    954         return_dict=return_dict,
    955         **kwargs,
    956     )
    958 batch_size = input_ids.shape[0]
    959 if attention_mask is not None:
    960     # concat prompt attention mask

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/accelerate/hooks.py:165, in add_hook_to_module.<locals>.new_forward(*args, **kwargs)
    163         output = old_forward(*args, **kwargs)
    164 else:
--> 165     output = old_forward(*args, **kwargs)
    166 return module._hf_hook.post_forward(module, output)

File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:709, in LlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
    707 # Flatten the tokens
    708 loss_fct = CrossEntropyLoss()
--> 709 shift_logits = shift_logits.view(-1, self.config.vocab_size)
    710 shift_labels = shift_labels.view(-1)
    711 # Enable model parallelism

RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504

​",CarolXh,"@zhanghainan Thanks for your help. My inference code is almost similar to yours. I think the problem may be that codellama is not quite sensitive to questions related to SQL or DSL, I will do that work further."
63,"Unable to run this morning, yesterday it ran fine","Morning!  I need help getting the models to run a _second_ time, on a new instance.

Yesterday, I registered for and downloaded the models onto an AWS sagemaker instance.  Everything worked fine and I was able to run 

`pip install -e .`

And from there experiment with the models.  I shut down the instance and this morning started it again.  I reran the pip installation, but now, everything hangs at this step:

```
sh-4.2$ torchrun --nproc_per_node 4 example_instructions.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 4
> initializing ddp with size 1
> initializing pipeline with size 1
```

This same code would finish loading the model after 8 seconds or so and be good to go.  I've tried this with the 7b instruct model, the 13b instruct, and the 34b instruct; all worked fine yesterday, none work today.  

How can I make this work?  Did I forget some crucial step?

For the rest of this bug report, it's basically how I arrived at the conclusion that

`checkpoint = torch.load(ckpt_path, map_location=""cpu"")`

Is not working, and I'm not sure why.  Once I get to that point, the RAM usage rises from 1.8GB to 28.9GB, so it looks like it's at least found the first file in the checkpoint.  This instance g5.12xlarge has 196GB and 4 24GB GPUs (and everything worked yesterday).

To figure this all out, I went into generation.py in the llama directory, and I added in some line number inspections.  I added in lines like:

```
from inspect import getframeinfo, currentframe

print(f""Got to {getframeinfo(currentframe()).lineno}"")
```

The code in generation now looks like: 

```
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        assert len(checkpoints) > 0, f""no checkpoint files found in {ckpt_dir}""
        assert model_parallel_size == len(
            checkpoints
        ), f""Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}""
        
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        ckpt_path = checkpoints[get_model_parallel_rank()]
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        checkpoint = torch.load(ckpt_path, map_location=""cpu"")
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        with open(Path(ckpt_dir) / ""params.json"", ""r"") as f:
            params = json.loads(f.read())
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        model_args: ModelArgs = ModelArgs(
            max_seq_len=max_seq_len,
            max_batch_size=max_batch_size,
            **params,
        )
        tokenizer = Tokenizer(model_path=tokenizer_path)
        model_args.vocab_size = tokenizer.n_words
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
```

and the run output looks like:

```
sh-4.2$ torchrun --nproc_per_node 4 example_instructions.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 4
> initializing ddp with size 1
> initializing pipeline with size 1
Got to 86
Got to 92
Got to 94
```
which is `checkpoint = torch.load(ckpt_path, map_location=""cpu"")`

My `pip freeze`:
```
sh-4.2$ pip freeze
aiobotocore @ file:///home/conda/feedstock_root/build_artifacts/aiobotocore_1691451276487/work
aiofiles==22.1.0
aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1689804989077/work
aioitertools @ file:///home/conda/feedstock_root/build_artifacts/aioitertools_1663521246073/work
aiosignal @ file:///home/conda/feedstock_root/build_artifacts/aiosignal_1667935791922/work
aiosqlite==0.19.0
anyio @ file:///home/conda/feedstock_root/build_artifacts/anyio_1688651106312/work/dist
argon2-cffi @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi_1640817743617/work
argon2-cffi-bindings @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi-bindings_1666850768662/work
arrow @ file:///home/conda/feedstock_root/build_artifacts/arrow_1662382474514/work
astroid==2.15.6
asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1670263926556/work
async-timeout @ file:///home/conda/feedstock_root/build_artifacts/async-timeout_1691763562544/work
attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1683424013410/work
autopep8==2.0.2
autovizwidget @ file:///home/conda/feedstock_root/build_artifacts/autovizwidget_1680800327357/work
awscli==1.29.28
Babel==2.12.1
backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work
backports.functools-lru-cache @ file:///home/conda/feedstock_root/build_artifacts/backports.functools_lru_cache_1687772187254/work
beautifulsoup4 @ file:///home/conda/feedstock_root/build_artifacts/beautifulsoup4_1680888073205/work
bleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1674535352125/work
boto3==1.28.28
botocore==1.31.28
brotlipy @ file:///home/conda/feedstock_root/build_artifacts/brotlipy_1666764671472/work
cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work
certifi==2023.7.22
cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1671179353105/work
charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1688813409104/work
cloudpickle==2.2.1
cmake==3.27.2
-e git+ssh://git@github.com/facebookresearch/codellama.git@cb51c14ec761370ba2e2bc351374a79265d0465e#egg=codellama
colorama==0.4.4
comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1691044910542/work
contextlib2==21.6.0
cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography-split_1672672382195/work
debugpy @ file:///home/conda/feedstock_root/build_artifacts/debugpy_1691021228385/work
decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work
defusedxml @ file:///home/conda/feedstock_root/build_artifacts/defusedxml_1615232257335/work
dill==0.3.7
docker==6.1.3
docstring-to-markdown==0.12
docutils==0.16
entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work
environment-kernels==1.2.0
exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1692026125334/work
executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1667317341051/work
fairscale==0.4.13
fastjsonschema @ file:///home/conda/feedstock_root/build_artifacts/python-fastjsonschema_1690055433477/work/dist
filelock==3.12.3
fire==0.5.0
flit_core @ file:///home/conda/feedstock_root/build_artifacts/flit-core_1684084314667/work/source/flit_core
fqdn @ file:///home/conda/feedstock_root/build_artifacts/fqdn_1638810296540/work/dist
frozenlist @ file:///home/conda/feedstock_root/build_artifacts/frozenlist_1689244399117/work
fsspec @ file:///home/conda/feedstock_root/build_artifacts/fsspec_1626188337504/work
gitdb==4.0.10
GitPython==3.1.32
google-pasta==0.2.0
hdijupyterutils @ file:///home/conda/feedstock_root/build_artifacts/hdijupyterutils_1680800332182/work
idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work
importlib-metadata==6.8.0
importlib-resources @ file:///home/conda/feedstock_root/build_artifacts/importlib_resources_1691408075105/work
ipykernel==5.5.6
ipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1685727741709/work
ipython-genutils==0.2.0
ipywidgets @ file:///home/conda/feedstock_root/build_artifacts/ipywidgets_1690877070294/work
isoduration @ file:///home/conda/feedstock_root/build_artifacts/isoduration_1638811571363/work/dist
isort==5.12.0
jedi==0.18.2
Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1654302431367/work
jmespath @ file:///home/conda/feedstock_root/build_artifacts/jmespath_1655568249366/work
json5==0.9.14
jsonpointer==2.0
jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-meta_1691761378595/work
jsonschema-specifications @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-specifications_1689701150890/work
jupyter @ file:///home/conda/feedstock_root/build_artifacts/jupyter_1670249595582/work
jupyter-console @ file:///home/conda/feedstock_root/build_artifacts/jupyter_console_1678118109161/work
jupyter-events @ file:///home/conda/feedstock_root/build_artifacts/jupyter_events_1691505939576/work
jupyter-lsp==2.2.0
jupyter-server-mathjax==0.2.6
jupyter-server-proxy @ git+https://github.com/jupyterhub/jupyter-server-proxy@2d7dd346bb595106b417476de870a348943f3c70
jupyter-ydoc==0.2.5
jupyter_client==7.4.9
jupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1686775611663/work
jupyter_server @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_1692108700252/work
jupyter_server_fileid==0.9.0
jupyter_server_terminals @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_terminals_1673491454549/work
jupyter_server_ydoc==0.8.0
jupyterlab==3.6.5
jupyterlab-git==0.41.0
jupyterlab-lsp==4.2.0
jupyterlab-pygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1649936611996/work
jupyterlab-widgets @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_widgets_1688489450369/work
jupyterlab_server==2.24.0
lazy-object-proxy==1.9.0
lit==16.0.6
MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1685769049201/work
matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work
mccabe==0.7.0
mistune @ file:///home/conda/feedstock_root/build_artifacts/mistune_1675771498296/work
mock @ file:///home/conda/feedstock_root/build_artifacts/mock_1689092066756/work
mpmath==1.3.0
multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1672339403932/work
multiprocess==0.70.15
nb-conda @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_1654442778977/work
nb-conda-kernels @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_kernels_1667060632461/work
nbclassic @ file:///home/conda/feedstock_root/build_artifacts/nbclassic_1675369808718/work
nbclient @ file:///home/conda/feedstock_root/build_artifacts/nbclient_1684790896106/work
nbconvert @ file:///home/conda/feedstock_root/build_artifacts/nbconvert-meta_1674590374792/work
nbdime==3.2.1
nbexamples @ file:///opt/nbexamples
nbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1690814868471/work
nest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1664684991461/work
networkx==3.1
nose @ file:///home/conda/feedstock_root/build_artifacts/nose_1602434998960/work
notebook @ file:///home/conda/feedstock_root/build_artifacts/notebook_1691436218243/work
notebook_shim @ file:///home/conda/feedstock_root/build_artifacts/notebook-shim_1682360583588/work
numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1691056231492/work
nvidia-cublas-cu11==11.10.3.66
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cudnn-cu11==8.5.0.96
nvidia-cufft-cu11==10.9.0.58
nvidia-curand-cu11==10.2.10.91
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusparse-cu11==11.7.4.91
nvidia-nccl-cu11==2.14.3
nvidia-nvtx-cu11==11.7.91
overrides @ file:///home/conda/feedstock_root/build_artifacts/overrides_1691338815398/work
packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1681337016113/work
pandas @ file:///home/conda/feedstock_root/build_artifacts/pandas_1688740542634/work
pandocfilters @ file:///home/conda/feedstock_root/build_artifacts/pandocfilters_1631603243851/work
parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work
pathos==0.3.1
pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1667297516076/work
pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work
pid==3.0.4
pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1633981968097/work
platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1690813113769/work
plotly @ file:///home/conda/feedstock_root/build_artifacts/plotly_1692220561510/work
pluggy==1.2.0
pox==0.3.3
ppft==1.7.6.7
prometheus-client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1689032443210/work
prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1688565951714/work
protobuf==4.23.4
psutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1681775027942/work
psycopg2 @ file:///home/conda/feedstock_root/build_artifacts/psycopg2-split_1667025517155/work
ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl
pure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work
py4j==0.10.9.5
pyasn1==0.5.0
pycodestyle==2.10.0
pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work
pydocstyle==6.3.0
pyflakes==3.0.1
pygal==3.0.0
Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1691408637400/work
pykerberos @ file:///home/conda/feedstock_root/build_artifacts/pykerberos_1671204518513/work
pylint==2.17.5
pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1685514481738/work
PyQt5==5.12.3
PyQt5_sip==4.19.18
PyQtChart==5.12
PyQtWebEngine==5.12.1
PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1661604839144/work
pyspark==3.3.0
python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work
python-json-logger @ file:///home/conda/feedstock_root/build_artifacts/python-json-logger_1677079630776/work
python-lsp-jsonrpc==1.0.0
python-lsp-server==1.7.4
pytoolconfig==1.2.5
pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work
PyYAML @ file:///home/conda/feedstock_root/build_artifacts/pyyaml_1666772395347/work
pyzmq @ file:///home/conda/feedstock_root/build_artifacts/pyzmq_1666828497229/work
qtconsole @ file:///home/conda/feedstock_root/build_artifacts/qtconsole-base_1683329453903/work
QtPy @ file:///home/conda/feedstock_root/build_artifacts/qtpy_1680148448366/work
referencing @ file:///home/conda/feedstock_root/build_artifacts/referencing_1691337268233/work
requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1684774241324/work
requests-kerberos @ file:///home/conda/feedstock_root/build_artifacts/requests-kerberos_1667464887610/work
rfc3339-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3339-validator_1638811747357/work
rfc3986-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3986-validator_1598024191506/work
rope==1.9.0
rpds-py @ file:///home/conda/feedstock_root/build_artifacts/rpds-py_1689705060450/work
rsa==4.7.2
s3fs @ file:///home/conda/feedstock_root/build_artifacts/s3fs_1626193591467/work
s3transfer @ file:///home/conda/feedstock_root/build_artifacts/s3transfer_1692149178344/work
sagemaker==2.177.1
sagemaker-experiments==0.1.45
sagemaker-nbi-agent @ file:///opt/sagemaker_nbi_agent
sagemaker-pyspark==1.4.5
schema==0.7.5
Send2Trash @ file:///home/conda/feedstock_root/build_artifacts/send2trash_1682601222253/work
sentencepiece==0.1.99
simpervisor==1.0.0
six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work
smdebug-rulesconfig==1.0.1
smmap==5.0.0
sniffio @ file:///home/conda/feedstock_root/build_artifacts/sniffio_1662051266223/work
snowballstemmer==2.2.0
soupsieve @ file:///home/conda/feedstock_root/build_artifacts/soupsieve_1658207591808/work
sparkmagic @ file:///home/conda/feedstock_root/build_artifacts/sparkmagic_1680849855330/work/sparkmagic
stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work
sympy==1.12
tblib==1.7.0
tenacity @ file:///home/conda/feedstock_root/build_artifacts/tenacity_1692026804430/work
termcolor==2.3.0
terminado @ file:///home/conda/feedstock_root/build_artifacts/terminado_1670253674810/work
tinycss2 @ file:///home/conda/feedstock_root/build_artifacts/tinycss2_1666100256010/work
tomli==2.0.1
tomlkit==0.12.1
torch==2.0.1
tornado @ file:///home/conda/feedstock_root/build_artifacts/tornado_1684150054582/work
traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1675110562325/work
triton==2.0.0
typing-utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1622899189314/work
typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1688315532570/work
tzdata @ file:///home/conda/feedstock_root/build_artifacts/python-tzdata_1680081134351/work
ujson==5.8.0
uri-template @ file:///home/conda/feedstock_root/build_artifacts/uri-template_1688655812972/work/dist
urllib3==1.26.14
wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1673864653149/work
webcolors @ file:///home/conda/feedstock_root/build_artifacts/webcolors_1679900785843/work
webencodings==0.5.1
websocket-client @ file:///home/conda/feedstock_root/build_artifacts/websocket-client_1687789148259/work
widgetsnbextension @ file:///home/conda/feedstock_root/build_artifacts/widgetsnbextension_1688504439014/work
wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1677485519705/work
y-py==0.6.0
yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1685191749966/work
ypy-websocket==0.8.4
zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1689374466814/work
```",GaganHonor,"╰(*°▽°*)╯ohhhhhhhhhhhhh
The error message you are getting is:

```
Got to 86
Got to 92
Got to 94
```

This means that the code is reaching line 86, 92, and 94 of the `example_instructions.py` file. These lines are responsible for loading the model checkpoint, initializing the model parallel, and initializing the pipeline.

The reason why the code is hanging at this step is because the model checkpoint is too large to fit on the CPU. The `CodeLlama-34b-Instruct` model has a size of 1.6GB, which is larger than the 1.8GB of RAM that is available on the CPU.

To fix this error, you need to move the model checkpoint to the GPU. You can do this by running the following command:

```
cp CodeLlama-34b-Instruct.ckpt /tmp/CodeLlama-34b-Instruct.ckpt
```

Once you have moved the model checkpoint to the GPU, you need to update the `example_instructions.py` file to load the model checkpoint from the GPU. You can do this by changing the `map_location` argument to `""cuda""`.

The updated code should look like this:

```
checkpoint = torch.load(""/tmp/CodeLlama-34b-Instruct.ckpt"", map_location=""cuda"")
```

Once you have made these changes, you should be able to run the `example_instructions.py` file without any errors.

Here are some additional things you can try:

* Make sure that you are using the correct version of PyTorch. The `torchrun` command requires PyTorch version 1.8 or higher.
* Make sure that the `nccl` library is installed in the same location as your PyTorch distribution.
* Try running the `torchrun` command with the `--use_gloo` flag. This will use the Gloo backend instead of NCCL.

If you are still having trouble, you can ask for help here ❇️❇️
I hope this helps!
"
63,"Unable to run this morning, yesterday it ran fine","Morning!  I need help getting the models to run a _second_ time, on a new instance.

Yesterday, I registered for and downloaded the models onto an AWS sagemaker instance.  Everything worked fine and I was able to run 

`pip install -e .`

And from there experiment with the models.  I shut down the instance and this morning started it again.  I reran the pip installation, but now, everything hangs at this step:

```
sh-4.2$ torchrun --nproc_per_node 4 example_instructions.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 4
> initializing ddp with size 1
> initializing pipeline with size 1
```

This same code would finish loading the model after 8 seconds or so and be good to go.  I've tried this with the 7b instruct model, the 13b instruct, and the 34b instruct; all worked fine yesterday, none work today.  

How can I make this work?  Did I forget some crucial step?

For the rest of this bug report, it's basically how I arrived at the conclusion that

`checkpoint = torch.load(ckpt_path, map_location=""cpu"")`

Is not working, and I'm not sure why.  Once I get to that point, the RAM usage rises from 1.8GB to 28.9GB, so it looks like it's at least found the first file in the checkpoint.  This instance g5.12xlarge has 196GB and 4 24GB GPUs (and everything worked yesterday).

To figure this all out, I went into generation.py in the llama directory, and I added in some line number inspections.  I added in lines like:

```
from inspect import getframeinfo, currentframe

print(f""Got to {getframeinfo(currentframe()).lineno}"")
```

The code in generation now looks like: 

```
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        assert len(checkpoints) > 0, f""no checkpoint files found in {ckpt_dir}""
        assert model_parallel_size == len(
            checkpoints
        ), f""Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}""
        
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        ckpt_path = checkpoints[get_model_parallel_rank()]
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        checkpoint = torch.load(ckpt_path, map_location=""cpu"")
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        with open(Path(ckpt_dir) / ""params.json"", ""r"") as f:
            params = json.loads(f.read())
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        model_args: ModelArgs = ModelArgs(
            max_seq_len=max_seq_len,
            max_batch_size=max_batch_size,
            **params,
        )
        tokenizer = Tokenizer(model_path=tokenizer_path)
        model_args.vocab_size = tokenizer.n_words
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
```

and the run output looks like:

```
sh-4.2$ torchrun --nproc_per_node 4 example_instructions.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 4
> initializing ddp with size 1
> initializing pipeline with size 1
Got to 86
Got to 92
Got to 94
```
which is `checkpoint = torch.load(ckpt_path, map_location=""cpu"")`

My `pip freeze`:
```
sh-4.2$ pip freeze
aiobotocore @ file:///home/conda/feedstock_root/build_artifacts/aiobotocore_1691451276487/work
aiofiles==22.1.0
aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1689804989077/work
aioitertools @ file:///home/conda/feedstock_root/build_artifacts/aioitertools_1663521246073/work
aiosignal @ file:///home/conda/feedstock_root/build_artifacts/aiosignal_1667935791922/work
aiosqlite==0.19.0
anyio @ file:///home/conda/feedstock_root/build_artifacts/anyio_1688651106312/work/dist
argon2-cffi @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi_1640817743617/work
argon2-cffi-bindings @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi-bindings_1666850768662/work
arrow @ file:///home/conda/feedstock_root/build_artifacts/arrow_1662382474514/work
astroid==2.15.6
asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1670263926556/work
async-timeout @ file:///home/conda/feedstock_root/build_artifacts/async-timeout_1691763562544/work
attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1683424013410/work
autopep8==2.0.2
autovizwidget @ file:///home/conda/feedstock_root/build_artifacts/autovizwidget_1680800327357/work
awscli==1.29.28
Babel==2.12.1
backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work
backports.functools-lru-cache @ file:///home/conda/feedstock_root/build_artifacts/backports.functools_lru_cache_1687772187254/work
beautifulsoup4 @ file:///home/conda/feedstock_root/build_artifacts/beautifulsoup4_1680888073205/work
bleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1674535352125/work
boto3==1.28.28
botocore==1.31.28
brotlipy @ file:///home/conda/feedstock_root/build_artifacts/brotlipy_1666764671472/work
cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work
certifi==2023.7.22
cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1671179353105/work
charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1688813409104/work
cloudpickle==2.2.1
cmake==3.27.2
-e git+ssh://git@github.com/facebookresearch/codellama.git@cb51c14ec761370ba2e2bc351374a79265d0465e#egg=codellama
colorama==0.4.4
comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1691044910542/work
contextlib2==21.6.0
cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography-split_1672672382195/work
debugpy @ file:///home/conda/feedstock_root/build_artifacts/debugpy_1691021228385/work
decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work
defusedxml @ file:///home/conda/feedstock_root/build_artifacts/defusedxml_1615232257335/work
dill==0.3.7
docker==6.1.3
docstring-to-markdown==0.12
docutils==0.16
entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work
environment-kernels==1.2.0
exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1692026125334/work
executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1667317341051/work
fairscale==0.4.13
fastjsonschema @ file:///home/conda/feedstock_root/build_artifacts/python-fastjsonschema_1690055433477/work/dist
filelock==3.12.3
fire==0.5.0
flit_core @ file:///home/conda/feedstock_root/build_artifacts/flit-core_1684084314667/work/source/flit_core
fqdn @ file:///home/conda/feedstock_root/build_artifacts/fqdn_1638810296540/work/dist
frozenlist @ file:///home/conda/feedstock_root/build_artifacts/frozenlist_1689244399117/work
fsspec @ file:///home/conda/feedstock_root/build_artifacts/fsspec_1626188337504/work
gitdb==4.0.10
GitPython==3.1.32
google-pasta==0.2.0
hdijupyterutils @ file:///home/conda/feedstock_root/build_artifacts/hdijupyterutils_1680800332182/work
idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work
importlib-metadata==6.8.0
importlib-resources @ file:///home/conda/feedstock_root/build_artifacts/importlib_resources_1691408075105/work
ipykernel==5.5.6
ipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1685727741709/work
ipython-genutils==0.2.0
ipywidgets @ file:///home/conda/feedstock_root/build_artifacts/ipywidgets_1690877070294/work
isoduration @ file:///home/conda/feedstock_root/build_artifacts/isoduration_1638811571363/work/dist
isort==5.12.0
jedi==0.18.2
Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1654302431367/work
jmespath @ file:///home/conda/feedstock_root/build_artifacts/jmespath_1655568249366/work
json5==0.9.14
jsonpointer==2.0
jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-meta_1691761378595/work
jsonschema-specifications @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-specifications_1689701150890/work
jupyter @ file:///home/conda/feedstock_root/build_artifacts/jupyter_1670249595582/work
jupyter-console @ file:///home/conda/feedstock_root/build_artifacts/jupyter_console_1678118109161/work
jupyter-events @ file:///home/conda/feedstock_root/build_artifacts/jupyter_events_1691505939576/work
jupyter-lsp==2.2.0
jupyter-server-mathjax==0.2.6
jupyter-server-proxy @ git+https://github.com/jupyterhub/jupyter-server-proxy@2d7dd346bb595106b417476de870a348943f3c70
jupyter-ydoc==0.2.5
jupyter_client==7.4.9
jupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1686775611663/work
jupyter_server @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_1692108700252/work
jupyter_server_fileid==0.9.0
jupyter_server_terminals @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_terminals_1673491454549/work
jupyter_server_ydoc==0.8.0
jupyterlab==3.6.5
jupyterlab-git==0.41.0
jupyterlab-lsp==4.2.0
jupyterlab-pygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1649936611996/work
jupyterlab-widgets @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_widgets_1688489450369/work
jupyterlab_server==2.24.0
lazy-object-proxy==1.9.0
lit==16.0.6
MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1685769049201/work
matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work
mccabe==0.7.0
mistune @ file:///home/conda/feedstock_root/build_artifacts/mistune_1675771498296/work
mock @ file:///home/conda/feedstock_root/build_artifacts/mock_1689092066756/work
mpmath==1.3.0
multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1672339403932/work
multiprocess==0.70.15
nb-conda @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_1654442778977/work
nb-conda-kernels @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_kernels_1667060632461/work
nbclassic @ file:///home/conda/feedstock_root/build_artifacts/nbclassic_1675369808718/work
nbclient @ file:///home/conda/feedstock_root/build_artifacts/nbclient_1684790896106/work
nbconvert @ file:///home/conda/feedstock_root/build_artifacts/nbconvert-meta_1674590374792/work
nbdime==3.2.1
nbexamples @ file:///opt/nbexamples
nbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1690814868471/work
nest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1664684991461/work
networkx==3.1
nose @ file:///home/conda/feedstock_root/build_artifacts/nose_1602434998960/work
notebook @ file:///home/conda/feedstock_root/build_artifacts/notebook_1691436218243/work
notebook_shim @ file:///home/conda/feedstock_root/build_artifacts/notebook-shim_1682360583588/work
numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1691056231492/work
nvidia-cublas-cu11==11.10.3.66
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cudnn-cu11==8.5.0.96
nvidia-cufft-cu11==10.9.0.58
nvidia-curand-cu11==10.2.10.91
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusparse-cu11==11.7.4.91
nvidia-nccl-cu11==2.14.3
nvidia-nvtx-cu11==11.7.91
overrides @ file:///home/conda/feedstock_root/build_artifacts/overrides_1691338815398/work
packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1681337016113/work
pandas @ file:///home/conda/feedstock_root/build_artifacts/pandas_1688740542634/work
pandocfilters @ file:///home/conda/feedstock_root/build_artifacts/pandocfilters_1631603243851/work
parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work
pathos==0.3.1
pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1667297516076/work
pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work
pid==3.0.4
pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1633981968097/work
platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1690813113769/work
plotly @ file:///home/conda/feedstock_root/build_artifacts/plotly_1692220561510/work
pluggy==1.2.0
pox==0.3.3
ppft==1.7.6.7
prometheus-client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1689032443210/work
prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1688565951714/work
protobuf==4.23.4
psutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1681775027942/work
psycopg2 @ file:///home/conda/feedstock_root/build_artifacts/psycopg2-split_1667025517155/work
ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl
pure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work
py4j==0.10.9.5
pyasn1==0.5.0
pycodestyle==2.10.0
pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work
pydocstyle==6.3.0
pyflakes==3.0.1
pygal==3.0.0
Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1691408637400/work
pykerberos @ file:///home/conda/feedstock_root/build_artifacts/pykerberos_1671204518513/work
pylint==2.17.5
pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1685514481738/work
PyQt5==5.12.3
PyQt5_sip==4.19.18
PyQtChart==5.12
PyQtWebEngine==5.12.1
PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1661604839144/work
pyspark==3.3.0
python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work
python-json-logger @ file:///home/conda/feedstock_root/build_artifacts/python-json-logger_1677079630776/work
python-lsp-jsonrpc==1.0.0
python-lsp-server==1.7.4
pytoolconfig==1.2.5
pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work
PyYAML @ file:///home/conda/feedstock_root/build_artifacts/pyyaml_1666772395347/work
pyzmq @ file:///home/conda/feedstock_root/build_artifacts/pyzmq_1666828497229/work
qtconsole @ file:///home/conda/feedstock_root/build_artifacts/qtconsole-base_1683329453903/work
QtPy @ file:///home/conda/feedstock_root/build_artifacts/qtpy_1680148448366/work
referencing @ file:///home/conda/feedstock_root/build_artifacts/referencing_1691337268233/work
requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1684774241324/work
requests-kerberos @ file:///home/conda/feedstock_root/build_artifacts/requests-kerberos_1667464887610/work
rfc3339-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3339-validator_1638811747357/work
rfc3986-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3986-validator_1598024191506/work
rope==1.9.0
rpds-py @ file:///home/conda/feedstock_root/build_artifacts/rpds-py_1689705060450/work
rsa==4.7.2
s3fs @ file:///home/conda/feedstock_root/build_artifacts/s3fs_1626193591467/work
s3transfer @ file:///home/conda/feedstock_root/build_artifacts/s3transfer_1692149178344/work
sagemaker==2.177.1
sagemaker-experiments==0.1.45
sagemaker-nbi-agent @ file:///opt/sagemaker_nbi_agent
sagemaker-pyspark==1.4.5
schema==0.7.5
Send2Trash @ file:///home/conda/feedstock_root/build_artifacts/send2trash_1682601222253/work
sentencepiece==0.1.99
simpervisor==1.0.0
six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work
smdebug-rulesconfig==1.0.1
smmap==5.0.0
sniffio @ file:///home/conda/feedstock_root/build_artifacts/sniffio_1662051266223/work
snowballstemmer==2.2.0
soupsieve @ file:///home/conda/feedstock_root/build_artifacts/soupsieve_1658207591808/work
sparkmagic @ file:///home/conda/feedstock_root/build_artifacts/sparkmagic_1680849855330/work/sparkmagic
stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work
sympy==1.12
tblib==1.7.0
tenacity @ file:///home/conda/feedstock_root/build_artifacts/tenacity_1692026804430/work
termcolor==2.3.0
terminado @ file:///home/conda/feedstock_root/build_artifacts/terminado_1670253674810/work
tinycss2 @ file:///home/conda/feedstock_root/build_artifacts/tinycss2_1666100256010/work
tomli==2.0.1
tomlkit==0.12.1
torch==2.0.1
tornado @ file:///home/conda/feedstock_root/build_artifacts/tornado_1684150054582/work
traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1675110562325/work
triton==2.0.0
typing-utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1622899189314/work
typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1688315532570/work
tzdata @ file:///home/conda/feedstock_root/build_artifacts/python-tzdata_1680081134351/work
ujson==5.8.0
uri-template @ file:///home/conda/feedstock_root/build_artifacts/uri-template_1688655812972/work/dist
urllib3==1.26.14
wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1673864653149/work
webcolors @ file:///home/conda/feedstock_root/build_artifacts/webcolors_1679900785843/work
webencodings==0.5.1
websocket-client @ file:///home/conda/feedstock_root/build_artifacts/websocket-client_1687789148259/work
widgetsnbextension @ file:///home/conda/feedstock_root/build_artifacts/widgetsnbextension_1688504439014/work
wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1677485519705/work
y-py==0.6.0
yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1685191749966/work
ypy-websocket==0.8.4
zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1689374466814/work
```",mmr-crexi,"Admit it @GaganHonor, you just took my bug report and plugged it into a generative AI for an answer.

What makes me think that?
1. You got the name of the file wrong where I inserted line numbers, and reused the explanation of where the line numbers were but for that wrong file.  The line numbers were in `generation.py`, not `example_instructions.py`
2. The RAM amounts you listed are ludicrously low.  g5.12xlarges have more than enough RAM to load the 34b parameter model into RAM, and in fact did so on the preceding day, not ""1.8GB of RAM"".  Looking up the instance type would tell you that, if the amount that I put in the report seemed ludicrous to you.
3. The very first thing I did when resetting up the environment is to run `pip install -e .`, which you would know if you had read the report.  That would make sure that all dependencies are installed properly, and indeed, those dependencies were installed the first time.  Perhaps they do not install a second time?

It very well may be that the torch load should not be to CPU first, but that's what the `generation.py` script provided in this repo does, and it's worked at least once.  I strongly suspect that there's either some kind of unnamed dependency not being installed, or some implicit assumption that the machine running the models will be persisted from run to run, rather than shut down and restarted with a blank slate.

Were you able to reproduce the issue?  It may also just be that, for whatever reason, the Sagemaker instance I was using changed in some fundamental way, or that issues like [pytorch #99625 ](https://github.com/pytorch/pytorch/issues/99625) were somehow manifesting one day and not the other.  If so, I would really like to know what happened and how you were able to actually solve things."
63,"Unable to run this morning, yesterday it ran fine","Morning!  I need help getting the models to run a _second_ time, on a new instance.

Yesterday, I registered for and downloaded the models onto an AWS sagemaker instance.  Everything worked fine and I was able to run 

`pip install -e .`

And from there experiment with the models.  I shut down the instance and this morning started it again.  I reran the pip installation, but now, everything hangs at this step:

```
sh-4.2$ torchrun --nproc_per_node 4 example_instructions.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 4
> initializing ddp with size 1
> initializing pipeline with size 1
```

This same code would finish loading the model after 8 seconds or so and be good to go.  I've tried this with the 7b instruct model, the 13b instruct, and the 34b instruct; all worked fine yesterday, none work today.  

How can I make this work?  Did I forget some crucial step?

For the rest of this bug report, it's basically how I arrived at the conclusion that

`checkpoint = torch.load(ckpt_path, map_location=""cpu"")`

Is not working, and I'm not sure why.  Once I get to that point, the RAM usage rises from 1.8GB to 28.9GB, so it looks like it's at least found the first file in the checkpoint.  This instance g5.12xlarge has 196GB and 4 24GB GPUs (and everything worked yesterday).

To figure this all out, I went into generation.py in the llama directory, and I added in some line number inspections.  I added in lines like:

```
from inspect import getframeinfo, currentframe

print(f""Got to {getframeinfo(currentframe()).lineno}"")
```

The code in generation now looks like: 

```
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        assert len(checkpoints) > 0, f""no checkpoint files found in {ckpt_dir}""
        assert model_parallel_size == len(
            checkpoints
        ), f""Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}""
        
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        ckpt_path = checkpoints[get_model_parallel_rank()]
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        checkpoint = torch.load(ckpt_path, map_location=""cpu"")
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        with open(Path(ckpt_dir) / ""params.json"", ""r"") as f:
            params = json.loads(f.read())
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        model_args: ModelArgs = ModelArgs(
            max_seq_len=max_seq_len,
            max_batch_size=max_batch_size,
            **params,
        )
        tokenizer = Tokenizer(model_path=tokenizer_path)
        model_args.vocab_size = tokenizer.n_words
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
```

and the run output looks like:

```
sh-4.2$ torchrun --nproc_per_node 4 example_instructions.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 4
> initializing ddp with size 1
> initializing pipeline with size 1
Got to 86
Got to 92
Got to 94
```
which is `checkpoint = torch.load(ckpt_path, map_location=""cpu"")`

My `pip freeze`:
```
sh-4.2$ pip freeze
aiobotocore @ file:///home/conda/feedstock_root/build_artifacts/aiobotocore_1691451276487/work
aiofiles==22.1.0
aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1689804989077/work
aioitertools @ file:///home/conda/feedstock_root/build_artifacts/aioitertools_1663521246073/work
aiosignal @ file:///home/conda/feedstock_root/build_artifacts/aiosignal_1667935791922/work
aiosqlite==0.19.0
anyio @ file:///home/conda/feedstock_root/build_artifacts/anyio_1688651106312/work/dist
argon2-cffi @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi_1640817743617/work
argon2-cffi-bindings @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi-bindings_1666850768662/work
arrow @ file:///home/conda/feedstock_root/build_artifacts/arrow_1662382474514/work
astroid==2.15.6
asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1670263926556/work
async-timeout @ file:///home/conda/feedstock_root/build_artifacts/async-timeout_1691763562544/work
attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1683424013410/work
autopep8==2.0.2
autovizwidget @ file:///home/conda/feedstock_root/build_artifacts/autovizwidget_1680800327357/work
awscli==1.29.28
Babel==2.12.1
backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work
backports.functools-lru-cache @ file:///home/conda/feedstock_root/build_artifacts/backports.functools_lru_cache_1687772187254/work
beautifulsoup4 @ file:///home/conda/feedstock_root/build_artifacts/beautifulsoup4_1680888073205/work
bleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1674535352125/work
boto3==1.28.28
botocore==1.31.28
brotlipy @ file:///home/conda/feedstock_root/build_artifacts/brotlipy_1666764671472/work
cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work
certifi==2023.7.22
cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1671179353105/work
charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1688813409104/work
cloudpickle==2.2.1
cmake==3.27.2
-e git+ssh://git@github.com/facebookresearch/codellama.git@cb51c14ec761370ba2e2bc351374a79265d0465e#egg=codellama
colorama==0.4.4
comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1691044910542/work
contextlib2==21.6.0
cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography-split_1672672382195/work
debugpy @ file:///home/conda/feedstock_root/build_artifacts/debugpy_1691021228385/work
decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work
defusedxml @ file:///home/conda/feedstock_root/build_artifacts/defusedxml_1615232257335/work
dill==0.3.7
docker==6.1.3
docstring-to-markdown==0.12
docutils==0.16
entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work
environment-kernels==1.2.0
exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1692026125334/work
executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1667317341051/work
fairscale==0.4.13
fastjsonschema @ file:///home/conda/feedstock_root/build_artifacts/python-fastjsonschema_1690055433477/work/dist
filelock==3.12.3
fire==0.5.0
flit_core @ file:///home/conda/feedstock_root/build_artifacts/flit-core_1684084314667/work/source/flit_core
fqdn @ file:///home/conda/feedstock_root/build_artifacts/fqdn_1638810296540/work/dist
frozenlist @ file:///home/conda/feedstock_root/build_artifacts/frozenlist_1689244399117/work
fsspec @ file:///home/conda/feedstock_root/build_artifacts/fsspec_1626188337504/work
gitdb==4.0.10
GitPython==3.1.32
google-pasta==0.2.0
hdijupyterutils @ file:///home/conda/feedstock_root/build_artifacts/hdijupyterutils_1680800332182/work
idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work
importlib-metadata==6.8.0
importlib-resources @ file:///home/conda/feedstock_root/build_artifacts/importlib_resources_1691408075105/work
ipykernel==5.5.6
ipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1685727741709/work
ipython-genutils==0.2.0
ipywidgets @ file:///home/conda/feedstock_root/build_artifacts/ipywidgets_1690877070294/work
isoduration @ file:///home/conda/feedstock_root/build_artifacts/isoduration_1638811571363/work/dist
isort==5.12.0
jedi==0.18.2
Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1654302431367/work
jmespath @ file:///home/conda/feedstock_root/build_artifacts/jmespath_1655568249366/work
json5==0.9.14
jsonpointer==2.0
jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-meta_1691761378595/work
jsonschema-specifications @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-specifications_1689701150890/work
jupyter @ file:///home/conda/feedstock_root/build_artifacts/jupyter_1670249595582/work
jupyter-console @ file:///home/conda/feedstock_root/build_artifacts/jupyter_console_1678118109161/work
jupyter-events @ file:///home/conda/feedstock_root/build_artifacts/jupyter_events_1691505939576/work
jupyter-lsp==2.2.0
jupyter-server-mathjax==0.2.6
jupyter-server-proxy @ git+https://github.com/jupyterhub/jupyter-server-proxy@2d7dd346bb595106b417476de870a348943f3c70
jupyter-ydoc==0.2.5
jupyter_client==7.4.9
jupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1686775611663/work
jupyter_server @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_1692108700252/work
jupyter_server_fileid==0.9.0
jupyter_server_terminals @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_terminals_1673491454549/work
jupyter_server_ydoc==0.8.0
jupyterlab==3.6.5
jupyterlab-git==0.41.0
jupyterlab-lsp==4.2.0
jupyterlab-pygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1649936611996/work
jupyterlab-widgets @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_widgets_1688489450369/work
jupyterlab_server==2.24.0
lazy-object-proxy==1.9.0
lit==16.0.6
MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1685769049201/work
matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work
mccabe==0.7.0
mistune @ file:///home/conda/feedstock_root/build_artifacts/mistune_1675771498296/work
mock @ file:///home/conda/feedstock_root/build_artifacts/mock_1689092066756/work
mpmath==1.3.0
multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1672339403932/work
multiprocess==0.70.15
nb-conda @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_1654442778977/work
nb-conda-kernels @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_kernels_1667060632461/work
nbclassic @ file:///home/conda/feedstock_root/build_artifacts/nbclassic_1675369808718/work
nbclient @ file:///home/conda/feedstock_root/build_artifacts/nbclient_1684790896106/work
nbconvert @ file:///home/conda/feedstock_root/build_artifacts/nbconvert-meta_1674590374792/work
nbdime==3.2.1
nbexamples @ file:///opt/nbexamples
nbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1690814868471/work
nest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1664684991461/work
networkx==3.1
nose @ file:///home/conda/feedstock_root/build_artifacts/nose_1602434998960/work
notebook @ file:///home/conda/feedstock_root/build_artifacts/notebook_1691436218243/work
notebook_shim @ file:///home/conda/feedstock_root/build_artifacts/notebook-shim_1682360583588/work
numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1691056231492/work
nvidia-cublas-cu11==11.10.3.66
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cudnn-cu11==8.5.0.96
nvidia-cufft-cu11==10.9.0.58
nvidia-curand-cu11==10.2.10.91
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusparse-cu11==11.7.4.91
nvidia-nccl-cu11==2.14.3
nvidia-nvtx-cu11==11.7.91
overrides @ file:///home/conda/feedstock_root/build_artifacts/overrides_1691338815398/work
packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1681337016113/work
pandas @ file:///home/conda/feedstock_root/build_artifacts/pandas_1688740542634/work
pandocfilters @ file:///home/conda/feedstock_root/build_artifacts/pandocfilters_1631603243851/work
parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work
pathos==0.3.1
pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1667297516076/work
pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work
pid==3.0.4
pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1633981968097/work
platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1690813113769/work
plotly @ file:///home/conda/feedstock_root/build_artifacts/plotly_1692220561510/work
pluggy==1.2.0
pox==0.3.3
ppft==1.7.6.7
prometheus-client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1689032443210/work
prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1688565951714/work
protobuf==4.23.4
psutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1681775027942/work
psycopg2 @ file:///home/conda/feedstock_root/build_artifacts/psycopg2-split_1667025517155/work
ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl
pure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work
py4j==0.10.9.5
pyasn1==0.5.0
pycodestyle==2.10.0
pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work
pydocstyle==6.3.0
pyflakes==3.0.1
pygal==3.0.0
Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1691408637400/work
pykerberos @ file:///home/conda/feedstock_root/build_artifacts/pykerberos_1671204518513/work
pylint==2.17.5
pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1685514481738/work
PyQt5==5.12.3
PyQt5_sip==4.19.18
PyQtChart==5.12
PyQtWebEngine==5.12.1
PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1661604839144/work
pyspark==3.3.0
python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work
python-json-logger @ file:///home/conda/feedstock_root/build_artifacts/python-json-logger_1677079630776/work
python-lsp-jsonrpc==1.0.0
python-lsp-server==1.7.4
pytoolconfig==1.2.5
pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work
PyYAML @ file:///home/conda/feedstock_root/build_artifacts/pyyaml_1666772395347/work
pyzmq @ file:///home/conda/feedstock_root/build_artifacts/pyzmq_1666828497229/work
qtconsole @ file:///home/conda/feedstock_root/build_artifacts/qtconsole-base_1683329453903/work
QtPy @ file:///home/conda/feedstock_root/build_artifacts/qtpy_1680148448366/work
referencing @ file:///home/conda/feedstock_root/build_artifacts/referencing_1691337268233/work
requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1684774241324/work
requests-kerberos @ file:///home/conda/feedstock_root/build_artifacts/requests-kerberos_1667464887610/work
rfc3339-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3339-validator_1638811747357/work
rfc3986-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3986-validator_1598024191506/work
rope==1.9.0
rpds-py @ file:///home/conda/feedstock_root/build_artifacts/rpds-py_1689705060450/work
rsa==4.7.2
s3fs @ file:///home/conda/feedstock_root/build_artifacts/s3fs_1626193591467/work
s3transfer @ file:///home/conda/feedstock_root/build_artifacts/s3transfer_1692149178344/work
sagemaker==2.177.1
sagemaker-experiments==0.1.45
sagemaker-nbi-agent @ file:///opt/sagemaker_nbi_agent
sagemaker-pyspark==1.4.5
schema==0.7.5
Send2Trash @ file:///home/conda/feedstock_root/build_artifacts/send2trash_1682601222253/work
sentencepiece==0.1.99
simpervisor==1.0.0
six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work
smdebug-rulesconfig==1.0.1
smmap==5.0.0
sniffio @ file:///home/conda/feedstock_root/build_artifacts/sniffio_1662051266223/work
snowballstemmer==2.2.0
soupsieve @ file:///home/conda/feedstock_root/build_artifacts/soupsieve_1658207591808/work
sparkmagic @ file:///home/conda/feedstock_root/build_artifacts/sparkmagic_1680849855330/work/sparkmagic
stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work
sympy==1.12
tblib==1.7.0
tenacity @ file:///home/conda/feedstock_root/build_artifacts/tenacity_1692026804430/work
termcolor==2.3.0
terminado @ file:///home/conda/feedstock_root/build_artifacts/terminado_1670253674810/work
tinycss2 @ file:///home/conda/feedstock_root/build_artifacts/tinycss2_1666100256010/work
tomli==2.0.1
tomlkit==0.12.1
torch==2.0.1
tornado @ file:///home/conda/feedstock_root/build_artifacts/tornado_1684150054582/work
traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1675110562325/work
triton==2.0.0
typing-utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1622899189314/work
typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1688315532570/work
tzdata @ file:///home/conda/feedstock_root/build_artifacts/python-tzdata_1680081134351/work
ujson==5.8.0
uri-template @ file:///home/conda/feedstock_root/build_artifacts/uri-template_1688655812972/work/dist
urllib3==1.26.14
wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1673864653149/work
webcolors @ file:///home/conda/feedstock_root/build_artifacts/webcolors_1679900785843/work
webencodings==0.5.1
websocket-client @ file:///home/conda/feedstock_root/build_artifacts/websocket-client_1687789148259/work
widgetsnbextension @ file:///home/conda/feedstock_root/build_artifacts/widgetsnbextension_1688504439014/work
wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1677485519705/work
y-py==0.6.0
yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1685191749966/work
ypy-websocket==0.8.4
zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1689374466814/work
```",GaganHonor,"why will I not admit ? I used codellama 34B model , Along with some HF Plugins  💀 My intention was to help you @mmr-crexi 

still sorry 
![CallOfDutyGhostGIF](https://github.com/facebookresearch/codellama/assets/141204441/74490fe0-8a93-4b86-bb2a-7277d7fa16e1)

"
63,"Unable to run this morning, yesterday it ran fine","Morning!  I need help getting the models to run a _second_ time, on a new instance.

Yesterday, I registered for and downloaded the models onto an AWS sagemaker instance.  Everything worked fine and I was able to run 

`pip install -e .`

And from there experiment with the models.  I shut down the instance and this morning started it again.  I reran the pip installation, but now, everything hangs at this step:

```
sh-4.2$ torchrun --nproc_per_node 4 example_instructions.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 4
> initializing ddp with size 1
> initializing pipeline with size 1
```

This same code would finish loading the model after 8 seconds or so and be good to go.  I've tried this with the 7b instruct model, the 13b instruct, and the 34b instruct; all worked fine yesterday, none work today.  

How can I make this work?  Did I forget some crucial step?

For the rest of this bug report, it's basically how I arrived at the conclusion that

`checkpoint = torch.load(ckpt_path, map_location=""cpu"")`

Is not working, and I'm not sure why.  Once I get to that point, the RAM usage rises from 1.8GB to 28.9GB, so it looks like it's at least found the first file in the checkpoint.  This instance g5.12xlarge has 196GB and 4 24GB GPUs (and everything worked yesterday).

To figure this all out, I went into generation.py in the llama directory, and I added in some line number inspections.  I added in lines like:

```
from inspect import getframeinfo, currentframe

print(f""Got to {getframeinfo(currentframe()).lineno}"")
```

The code in generation now looks like: 

```
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        assert len(checkpoints) > 0, f""no checkpoint files found in {ckpt_dir}""
        assert model_parallel_size == len(
            checkpoints
        ), f""Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}""
        
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        ckpt_path = checkpoints[get_model_parallel_rank()]
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        checkpoint = torch.load(ckpt_path, map_location=""cpu"")
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        with open(Path(ckpt_dir) / ""params.json"", ""r"") as f:
            params = json.loads(f.read())
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        model_args: ModelArgs = ModelArgs(
            max_seq_len=max_seq_len,
            max_batch_size=max_batch_size,
            **params,
        )
        tokenizer = Tokenizer(model_path=tokenizer_path)
        model_args.vocab_size = tokenizer.n_words
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
```

and the run output looks like:

```
sh-4.2$ torchrun --nproc_per_node 4 example_instructions.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 4
> initializing ddp with size 1
> initializing pipeline with size 1
Got to 86
Got to 92
Got to 94
```
which is `checkpoint = torch.load(ckpt_path, map_location=""cpu"")`

My `pip freeze`:
```
sh-4.2$ pip freeze
aiobotocore @ file:///home/conda/feedstock_root/build_artifacts/aiobotocore_1691451276487/work
aiofiles==22.1.0
aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1689804989077/work
aioitertools @ file:///home/conda/feedstock_root/build_artifacts/aioitertools_1663521246073/work
aiosignal @ file:///home/conda/feedstock_root/build_artifacts/aiosignal_1667935791922/work
aiosqlite==0.19.0
anyio @ file:///home/conda/feedstock_root/build_artifacts/anyio_1688651106312/work/dist
argon2-cffi @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi_1640817743617/work
argon2-cffi-bindings @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi-bindings_1666850768662/work
arrow @ file:///home/conda/feedstock_root/build_artifacts/arrow_1662382474514/work
astroid==2.15.6
asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1670263926556/work
async-timeout @ file:///home/conda/feedstock_root/build_artifacts/async-timeout_1691763562544/work
attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1683424013410/work
autopep8==2.0.2
autovizwidget @ file:///home/conda/feedstock_root/build_artifacts/autovizwidget_1680800327357/work
awscli==1.29.28
Babel==2.12.1
backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work
backports.functools-lru-cache @ file:///home/conda/feedstock_root/build_artifacts/backports.functools_lru_cache_1687772187254/work
beautifulsoup4 @ file:///home/conda/feedstock_root/build_artifacts/beautifulsoup4_1680888073205/work
bleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1674535352125/work
boto3==1.28.28
botocore==1.31.28
brotlipy @ file:///home/conda/feedstock_root/build_artifacts/brotlipy_1666764671472/work
cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work
certifi==2023.7.22
cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1671179353105/work
charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1688813409104/work
cloudpickle==2.2.1
cmake==3.27.2
-e git+ssh://git@github.com/facebookresearch/codellama.git@cb51c14ec761370ba2e2bc351374a79265d0465e#egg=codellama
colorama==0.4.4
comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1691044910542/work
contextlib2==21.6.0
cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography-split_1672672382195/work
debugpy @ file:///home/conda/feedstock_root/build_artifacts/debugpy_1691021228385/work
decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work
defusedxml @ file:///home/conda/feedstock_root/build_artifacts/defusedxml_1615232257335/work
dill==0.3.7
docker==6.1.3
docstring-to-markdown==0.12
docutils==0.16
entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work
environment-kernels==1.2.0
exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1692026125334/work
executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1667317341051/work
fairscale==0.4.13
fastjsonschema @ file:///home/conda/feedstock_root/build_artifacts/python-fastjsonschema_1690055433477/work/dist
filelock==3.12.3
fire==0.5.0
flit_core @ file:///home/conda/feedstock_root/build_artifacts/flit-core_1684084314667/work/source/flit_core
fqdn @ file:///home/conda/feedstock_root/build_artifacts/fqdn_1638810296540/work/dist
frozenlist @ file:///home/conda/feedstock_root/build_artifacts/frozenlist_1689244399117/work
fsspec @ file:///home/conda/feedstock_root/build_artifacts/fsspec_1626188337504/work
gitdb==4.0.10
GitPython==3.1.32
google-pasta==0.2.0
hdijupyterutils @ file:///home/conda/feedstock_root/build_artifacts/hdijupyterutils_1680800332182/work
idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work
importlib-metadata==6.8.0
importlib-resources @ file:///home/conda/feedstock_root/build_artifacts/importlib_resources_1691408075105/work
ipykernel==5.5.6
ipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1685727741709/work
ipython-genutils==0.2.0
ipywidgets @ file:///home/conda/feedstock_root/build_artifacts/ipywidgets_1690877070294/work
isoduration @ file:///home/conda/feedstock_root/build_artifacts/isoduration_1638811571363/work/dist
isort==5.12.0
jedi==0.18.2
Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1654302431367/work
jmespath @ file:///home/conda/feedstock_root/build_artifacts/jmespath_1655568249366/work
json5==0.9.14
jsonpointer==2.0
jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-meta_1691761378595/work
jsonschema-specifications @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-specifications_1689701150890/work
jupyter @ file:///home/conda/feedstock_root/build_artifacts/jupyter_1670249595582/work
jupyter-console @ file:///home/conda/feedstock_root/build_artifacts/jupyter_console_1678118109161/work
jupyter-events @ file:///home/conda/feedstock_root/build_artifacts/jupyter_events_1691505939576/work
jupyter-lsp==2.2.0
jupyter-server-mathjax==0.2.6
jupyter-server-proxy @ git+https://github.com/jupyterhub/jupyter-server-proxy@2d7dd346bb595106b417476de870a348943f3c70
jupyter-ydoc==0.2.5
jupyter_client==7.4.9
jupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1686775611663/work
jupyter_server @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_1692108700252/work
jupyter_server_fileid==0.9.0
jupyter_server_terminals @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_terminals_1673491454549/work
jupyter_server_ydoc==0.8.0
jupyterlab==3.6.5
jupyterlab-git==0.41.0
jupyterlab-lsp==4.2.0
jupyterlab-pygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1649936611996/work
jupyterlab-widgets @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_widgets_1688489450369/work
jupyterlab_server==2.24.0
lazy-object-proxy==1.9.0
lit==16.0.6
MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1685769049201/work
matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work
mccabe==0.7.0
mistune @ file:///home/conda/feedstock_root/build_artifacts/mistune_1675771498296/work
mock @ file:///home/conda/feedstock_root/build_artifacts/mock_1689092066756/work
mpmath==1.3.0
multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1672339403932/work
multiprocess==0.70.15
nb-conda @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_1654442778977/work
nb-conda-kernels @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_kernels_1667060632461/work
nbclassic @ file:///home/conda/feedstock_root/build_artifacts/nbclassic_1675369808718/work
nbclient @ file:///home/conda/feedstock_root/build_artifacts/nbclient_1684790896106/work
nbconvert @ file:///home/conda/feedstock_root/build_artifacts/nbconvert-meta_1674590374792/work
nbdime==3.2.1
nbexamples @ file:///opt/nbexamples
nbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1690814868471/work
nest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1664684991461/work
networkx==3.1
nose @ file:///home/conda/feedstock_root/build_artifacts/nose_1602434998960/work
notebook @ file:///home/conda/feedstock_root/build_artifacts/notebook_1691436218243/work
notebook_shim @ file:///home/conda/feedstock_root/build_artifacts/notebook-shim_1682360583588/work
numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1691056231492/work
nvidia-cublas-cu11==11.10.3.66
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cudnn-cu11==8.5.0.96
nvidia-cufft-cu11==10.9.0.58
nvidia-curand-cu11==10.2.10.91
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusparse-cu11==11.7.4.91
nvidia-nccl-cu11==2.14.3
nvidia-nvtx-cu11==11.7.91
overrides @ file:///home/conda/feedstock_root/build_artifacts/overrides_1691338815398/work
packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1681337016113/work
pandas @ file:///home/conda/feedstock_root/build_artifacts/pandas_1688740542634/work
pandocfilters @ file:///home/conda/feedstock_root/build_artifacts/pandocfilters_1631603243851/work
parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work
pathos==0.3.1
pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1667297516076/work
pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work
pid==3.0.4
pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1633981968097/work
platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1690813113769/work
plotly @ file:///home/conda/feedstock_root/build_artifacts/plotly_1692220561510/work
pluggy==1.2.0
pox==0.3.3
ppft==1.7.6.7
prometheus-client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1689032443210/work
prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1688565951714/work
protobuf==4.23.4
psutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1681775027942/work
psycopg2 @ file:///home/conda/feedstock_root/build_artifacts/psycopg2-split_1667025517155/work
ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl
pure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work
py4j==0.10.9.5
pyasn1==0.5.0
pycodestyle==2.10.0
pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work
pydocstyle==6.3.0
pyflakes==3.0.1
pygal==3.0.0
Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1691408637400/work
pykerberos @ file:///home/conda/feedstock_root/build_artifacts/pykerberos_1671204518513/work
pylint==2.17.5
pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1685514481738/work
PyQt5==5.12.3
PyQt5_sip==4.19.18
PyQtChart==5.12
PyQtWebEngine==5.12.1
PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1661604839144/work
pyspark==3.3.0
python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work
python-json-logger @ file:///home/conda/feedstock_root/build_artifacts/python-json-logger_1677079630776/work
python-lsp-jsonrpc==1.0.0
python-lsp-server==1.7.4
pytoolconfig==1.2.5
pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work
PyYAML @ file:///home/conda/feedstock_root/build_artifacts/pyyaml_1666772395347/work
pyzmq @ file:///home/conda/feedstock_root/build_artifacts/pyzmq_1666828497229/work
qtconsole @ file:///home/conda/feedstock_root/build_artifacts/qtconsole-base_1683329453903/work
QtPy @ file:///home/conda/feedstock_root/build_artifacts/qtpy_1680148448366/work
referencing @ file:///home/conda/feedstock_root/build_artifacts/referencing_1691337268233/work
requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1684774241324/work
requests-kerberos @ file:///home/conda/feedstock_root/build_artifacts/requests-kerberos_1667464887610/work
rfc3339-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3339-validator_1638811747357/work
rfc3986-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3986-validator_1598024191506/work
rope==1.9.0
rpds-py @ file:///home/conda/feedstock_root/build_artifacts/rpds-py_1689705060450/work
rsa==4.7.2
s3fs @ file:///home/conda/feedstock_root/build_artifacts/s3fs_1626193591467/work
s3transfer @ file:///home/conda/feedstock_root/build_artifacts/s3transfer_1692149178344/work
sagemaker==2.177.1
sagemaker-experiments==0.1.45
sagemaker-nbi-agent @ file:///opt/sagemaker_nbi_agent
sagemaker-pyspark==1.4.5
schema==0.7.5
Send2Trash @ file:///home/conda/feedstock_root/build_artifacts/send2trash_1682601222253/work
sentencepiece==0.1.99
simpervisor==1.0.0
six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work
smdebug-rulesconfig==1.0.1
smmap==5.0.0
sniffio @ file:///home/conda/feedstock_root/build_artifacts/sniffio_1662051266223/work
snowballstemmer==2.2.0
soupsieve @ file:///home/conda/feedstock_root/build_artifacts/soupsieve_1658207591808/work
sparkmagic @ file:///home/conda/feedstock_root/build_artifacts/sparkmagic_1680849855330/work/sparkmagic
stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work
sympy==1.12
tblib==1.7.0
tenacity @ file:///home/conda/feedstock_root/build_artifacts/tenacity_1692026804430/work
termcolor==2.3.0
terminado @ file:///home/conda/feedstock_root/build_artifacts/terminado_1670253674810/work
tinycss2 @ file:///home/conda/feedstock_root/build_artifacts/tinycss2_1666100256010/work
tomli==2.0.1
tomlkit==0.12.1
torch==2.0.1
tornado @ file:///home/conda/feedstock_root/build_artifacts/tornado_1684150054582/work
traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1675110562325/work
triton==2.0.0
typing-utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1622899189314/work
typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1688315532570/work
tzdata @ file:///home/conda/feedstock_root/build_artifacts/python-tzdata_1680081134351/work
ujson==5.8.0
uri-template @ file:///home/conda/feedstock_root/build_artifacts/uri-template_1688655812972/work/dist
urllib3==1.26.14
wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1673864653149/work
webcolors @ file:///home/conda/feedstock_root/build_artifacts/webcolors_1679900785843/work
webencodings==0.5.1
websocket-client @ file:///home/conda/feedstock_root/build_artifacts/websocket-client_1687789148259/work
widgetsnbextension @ file:///home/conda/feedstock_root/build_artifacts/widgetsnbextension_1688504439014/work
wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1677485519705/work
y-py==0.6.0
yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1685191749966/work
ypy-websocket==0.8.4
zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1689374466814/work
```",mmr-crexi,"Well, you've done a great job demonstrating its limitations :)

In all seriousness, I may be dealing with just some heisenbug in my setup, and that would be unfortunate, but I would not be devastated if no one had an answer for my corner case.  "
63,"Unable to run this morning, yesterday it ran fine","Morning!  I need help getting the models to run a _second_ time, on a new instance.

Yesterday, I registered for and downloaded the models onto an AWS sagemaker instance.  Everything worked fine and I was able to run 

`pip install -e .`

And from there experiment with the models.  I shut down the instance and this morning started it again.  I reran the pip installation, but now, everything hangs at this step:

```
sh-4.2$ torchrun --nproc_per_node 4 example_instructions.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 4
> initializing ddp with size 1
> initializing pipeline with size 1
```

This same code would finish loading the model after 8 seconds or so and be good to go.  I've tried this with the 7b instruct model, the 13b instruct, and the 34b instruct; all worked fine yesterday, none work today.  

How can I make this work?  Did I forget some crucial step?

For the rest of this bug report, it's basically how I arrived at the conclusion that

`checkpoint = torch.load(ckpt_path, map_location=""cpu"")`

Is not working, and I'm not sure why.  Once I get to that point, the RAM usage rises from 1.8GB to 28.9GB, so it looks like it's at least found the first file in the checkpoint.  This instance g5.12xlarge has 196GB and 4 24GB GPUs (and everything worked yesterday).

To figure this all out, I went into generation.py in the llama directory, and I added in some line number inspections.  I added in lines like:

```
from inspect import getframeinfo, currentframe

print(f""Got to {getframeinfo(currentframe()).lineno}"")
```

The code in generation now looks like: 

```
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        assert len(checkpoints) > 0, f""no checkpoint files found in {ckpt_dir}""
        assert model_parallel_size == len(
            checkpoints
        ), f""Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}""
        
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        ckpt_path = checkpoints[get_model_parallel_rank()]
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        checkpoint = torch.load(ckpt_path, map_location=""cpu"")
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        with open(Path(ckpt_dir) / ""params.json"", ""r"") as f:
            params = json.loads(f.read())
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        model_args: ModelArgs = ModelArgs(
            max_seq_len=max_seq_len,
            max_batch_size=max_batch_size,
            **params,
        )
        tokenizer = Tokenizer(model_path=tokenizer_path)
        model_args.vocab_size = tokenizer.n_words
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
```

and the run output looks like:

```
sh-4.2$ torchrun --nproc_per_node 4 example_instructions.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 4
> initializing ddp with size 1
> initializing pipeline with size 1
Got to 86
Got to 92
Got to 94
```
which is `checkpoint = torch.load(ckpt_path, map_location=""cpu"")`

My `pip freeze`:
```
sh-4.2$ pip freeze
aiobotocore @ file:///home/conda/feedstock_root/build_artifacts/aiobotocore_1691451276487/work
aiofiles==22.1.0
aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1689804989077/work
aioitertools @ file:///home/conda/feedstock_root/build_artifacts/aioitertools_1663521246073/work
aiosignal @ file:///home/conda/feedstock_root/build_artifacts/aiosignal_1667935791922/work
aiosqlite==0.19.0
anyio @ file:///home/conda/feedstock_root/build_artifacts/anyio_1688651106312/work/dist
argon2-cffi @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi_1640817743617/work
argon2-cffi-bindings @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi-bindings_1666850768662/work
arrow @ file:///home/conda/feedstock_root/build_artifacts/arrow_1662382474514/work
astroid==2.15.6
asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1670263926556/work
async-timeout @ file:///home/conda/feedstock_root/build_artifacts/async-timeout_1691763562544/work
attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1683424013410/work
autopep8==2.0.2
autovizwidget @ file:///home/conda/feedstock_root/build_artifacts/autovizwidget_1680800327357/work
awscli==1.29.28
Babel==2.12.1
backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work
backports.functools-lru-cache @ file:///home/conda/feedstock_root/build_artifacts/backports.functools_lru_cache_1687772187254/work
beautifulsoup4 @ file:///home/conda/feedstock_root/build_artifacts/beautifulsoup4_1680888073205/work
bleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1674535352125/work
boto3==1.28.28
botocore==1.31.28
brotlipy @ file:///home/conda/feedstock_root/build_artifacts/brotlipy_1666764671472/work
cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work
certifi==2023.7.22
cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1671179353105/work
charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1688813409104/work
cloudpickle==2.2.1
cmake==3.27.2
-e git+ssh://git@github.com/facebookresearch/codellama.git@cb51c14ec761370ba2e2bc351374a79265d0465e#egg=codellama
colorama==0.4.4
comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1691044910542/work
contextlib2==21.6.0
cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography-split_1672672382195/work
debugpy @ file:///home/conda/feedstock_root/build_artifacts/debugpy_1691021228385/work
decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work
defusedxml @ file:///home/conda/feedstock_root/build_artifacts/defusedxml_1615232257335/work
dill==0.3.7
docker==6.1.3
docstring-to-markdown==0.12
docutils==0.16
entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work
environment-kernels==1.2.0
exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1692026125334/work
executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1667317341051/work
fairscale==0.4.13
fastjsonschema @ file:///home/conda/feedstock_root/build_artifacts/python-fastjsonschema_1690055433477/work/dist
filelock==3.12.3
fire==0.5.0
flit_core @ file:///home/conda/feedstock_root/build_artifacts/flit-core_1684084314667/work/source/flit_core
fqdn @ file:///home/conda/feedstock_root/build_artifacts/fqdn_1638810296540/work/dist
frozenlist @ file:///home/conda/feedstock_root/build_artifacts/frozenlist_1689244399117/work
fsspec @ file:///home/conda/feedstock_root/build_artifacts/fsspec_1626188337504/work
gitdb==4.0.10
GitPython==3.1.32
google-pasta==0.2.0
hdijupyterutils @ file:///home/conda/feedstock_root/build_artifacts/hdijupyterutils_1680800332182/work
idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work
importlib-metadata==6.8.0
importlib-resources @ file:///home/conda/feedstock_root/build_artifacts/importlib_resources_1691408075105/work
ipykernel==5.5.6
ipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1685727741709/work
ipython-genutils==0.2.0
ipywidgets @ file:///home/conda/feedstock_root/build_artifacts/ipywidgets_1690877070294/work
isoduration @ file:///home/conda/feedstock_root/build_artifacts/isoduration_1638811571363/work/dist
isort==5.12.0
jedi==0.18.2
Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1654302431367/work
jmespath @ file:///home/conda/feedstock_root/build_artifacts/jmespath_1655568249366/work
json5==0.9.14
jsonpointer==2.0
jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-meta_1691761378595/work
jsonschema-specifications @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-specifications_1689701150890/work
jupyter @ file:///home/conda/feedstock_root/build_artifacts/jupyter_1670249595582/work
jupyter-console @ file:///home/conda/feedstock_root/build_artifacts/jupyter_console_1678118109161/work
jupyter-events @ file:///home/conda/feedstock_root/build_artifacts/jupyter_events_1691505939576/work
jupyter-lsp==2.2.0
jupyter-server-mathjax==0.2.6
jupyter-server-proxy @ git+https://github.com/jupyterhub/jupyter-server-proxy@2d7dd346bb595106b417476de870a348943f3c70
jupyter-ydoc==0.2.5
jupyter_client==7.4.9
jupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1686775611663/work
jupyter_server @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_1692108700252/work
jupyter_server_fileid==0.9.0
jupyter_server_terminals @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_terminals_1673491454549/work
jupyter_server_ydoc==0.8.0
jupyterlab==3.6.5
jupyterlab-git==0.41.0
jupyterlab-lsp==4.2.0
jupyterlab-pygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1649936611996/work
jupyterlab-widgets @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_widgets_1688489450369/work
jupyterlab_server==2.24.0
lazy-object-proxy==1.9.0
lit==16.0.6
MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1685769049201/work
matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work
mccabe==0.7.0
mistune @ file:///home/conda/feedstock_root/build_artifacts/mistune_1675771498296/work
mock @ file:///home/conda/feedstock_root/build_artifacts/mock_1689092066756/work
mpmath==1.3.0
multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1672339403932/work
multiprocess==0.70.15
nb-conda @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_1654442778977/work
nb-conda-kernels @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_kernels_1667060632461/work
nbclassic @ file:///home/conda/feedstock_root/build_artifacts/nbclassic_1675369808718/work
nbclient @ file:///home/conda/feedstock_root/build_artifacts/nbclient_1684790896106/work
nbconvert @ file:///home/conda/feedstock_root/build_artifacts/nbconvert-meta_1674590374792/work
nbdime==3.2.1
nbexamples @ file:///opt/nbexamples
nbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1690814868471/work
nest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1664684991461/work
networkx==3.1
nose @ file:///home/conda/feedstock_root/build_artifacts/nose_1602434998960/work
notebook @ file:///home/conda/feedstock_root/build_artifacts/notebook_1691436218243/work
notebook_shim @ file:///home/conda/feedstock_root/build_artifacts/notebook-shim_1682360583588/work
numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1691056231492/work
nvidia-cublas-cu11==11.10.3.66
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cudnn-cu11==8.5.0.96
nvidia-cufft-cu11==10.9.0.58
nvidia-curand-cu11==10.2.10.91
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusparse-cu11==11.7.4.91
nvidia-nccl-cu11==2.14.3
nvidia-nvtx-cu11==11.7.91
overrides @ file:///home/conda/feedstock_root/build_artifacts/overrides_1691338815398/work
packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1681337016113/work
pandas @ file:///home/conda/feedstock_root/build_artifacts/pandas_1688740542634/work
pandocfilters @ file:///home/conda/feedstock_root/build_artifacts/pandocfilters_1631603243851/work
parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work
pathos==0.3.1
pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1667297516076/work
pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work
pid==3.0.4
pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1633981968097/work
platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1690813113769/work
plotly @ file:///home/conda/feedstock_root/build_artifacts/plotly_1692220561510/work
pluggy==1.2.0
pox==0.3.3
ppft==1.7.6.7
prometheus-client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1689032443210/work
prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1688565951714/work
protobuf==4.23.4
psutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1681775027942/work
psycopg2 @ file:///home/conda/feedstock_root/build_artifacts/psycopg2-split_1667025517155/work
ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl
pure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work
py4j==0.10.9.5
pyasn1==0.5.0
pycodestyle==2.10.0
pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work
pydocstyle==6.3.0
pyflakes==3.0.1
pygal==3.0.0
Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1691408637400/work
pykerberos @ file:///home/conda/feedstock_root/build_artifacts/pykerberos_1671204518513/work
pylint==2.17.5
pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1685514481738/work
PyQt5==5.12.3
PyQt5_sip==4.19.18
PyQtChart==5.12
PyQtWebEngine==5.12.1
PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1661604839144/work
pyspark==3.3.0
python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work
python-json-logger @ file:///home/conda/feedstock_root/build_artifacts/python-json-logger_1677079630776/work
python-lsp-jsonrpc==1.0.0
python-lsp-server==1.7.4
pytoolconfig==1.2.5
pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work
PyYAML @ file:///home/conda/feedstock_root/build_artifacts/pyyaml_1666772395347/work
pyzmq @ file:///home/conda/feedstock_root/build_artifacts/pyzmq_1666828497229/work
qtconsole @ file:///home/conda/feedstock_root/build_artifacts/qtconsole-base_1683329453903/work
QtPy @ file:///home/conda/feedstock_root/build_artifacts/qtpy_1680148448366/work
referencing @ file:///home/conda/feedstock_root/build_artifacts/referencing_1691337268233/work
requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1684774241324/work
requests-kerberos @ file:///home/conda/feedstock_root/build_artifacts/requests-kerberos_1667464887610/work
rfc3339-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3339-validator_1638811747357/work
rfc3986-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3986-validator_1598024191506/work
rope==1.9.0
rpds-py @ file:///home/conda/feedstock_root/build_artifacts/rpds-py_1689705060450/work
rsa==4.7.2
s3fs @ file:///home/conda/feedstock_root/build_artifacts/s3fs_1626193591467/work
s3transfer @ file:///home/conda/feedstock_root/build_artifacts/s3transfer_1692149178344/work
sagemaker==2.177.1
sagemaker-experiments==0.1.45
sagemaker-nbi-agent @ file:///opt/sagemaker_nbi_agent
sagemaker-pyspark==1.4.5
schema==0.7.5
Send2Trash @ file:///home/conda/feedstock_root/build_artifacts/send2trash_1682601222253/work
sentencepiece==0.1.99
simpervisor==1.0.0
six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work
smdebug-rulesconfig==1.0.1
smmap==5.0.0
sniffio @ file:///home/conda/feedstock_root/build_artifacts/sniffio_1662051266223/work
snowballstemmer==2.2.0
soupsieve @ file:///home/conda/feedstock_root/build_artifacts/soupsieve_1658207591808/work
sparkmagic @ file:///home/conda/feedstock_root/build_artifacts/sparkmagic_1680849855330/work/sparkmagic
stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work
sympy==1.12
tblib==1.7.0
tenacity @ file:///home/conda/feedstock_root/build_artifacts/tenacity_1692026804430/work
termcolor==2.3.0
terminado @ file:///home/conda/feedstock_root/build_artifacts/terminado_1670253674810/work
tinycss2 @ file:///home/conda/feedstock_root/build_artifacts/tinycss2_1666100256010/work
tomli==2.0.1
tomlkit==0.12.1
torch==2.0.1
tornado @ file:///home/conda/feedstock_root/build_artifacts/tornado_1684150054582/work
traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1675110562325/work
triton==2.0.0
typing-utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1622899189314/work
typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1688315532570/work
tzdata @ file:///home/conda/feedstock_root/build_artifacts/python-tzdata_1680081134351/work
ujson==5.8.0
uri-template @ file:///home/conda/feedstock_root/build_artifacts/uri-template_1688655812972/work/dist
urllib3==1.26.14
wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1673864653149/work
webcolors @ file:///home/conda/feedstock_root/build_artifacts/webcolors_1679900785843/work
webencodings==0.5.1
websocket-client @ file:///home/conda/feedstock_root/build_artifacts/websocket-client_1687789148259/work
widgetsnbextension @ file:///home/conda/feedstock_root/build_artifacts/widgetsnbextension_1688504439014/work
wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1677485519705/work
y-py==0.6.0
yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1685191749966/work
ypy-websocket==0.8.4
zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1689374466814/work
```",GaganHonor,"well one thing i found is my model is answering far better since first build , It's  far better thn Claude or GPT 3.5 turbo currently , I am fixing"
63,"Unable to run this morning, yesterday it ran fine","Morning!  I need help getting the models to run a _second_ time, on a new instance.

Yesterday, I registered for and downloaded the models onto an AWS sagemaker instance.  Everything worked fine and I was able to run 

`pip install -e .`

And from there experiment with the models.  I shut down the instance and this morning started it again.  I reran the pip installation, but now, everything hangs at this step:

```
sh-4.2$ torchrun --nproc_per_node 4 example_instructions.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 4
> initializing ddp with size 1
> initializing pipeline with size 1
```

This same code would finish loading the model after 8 seconds or so and be good to go.  I've tried this with the 7b instruct model, the 13b instruct, and the 34b instruct; all worked fine yesterday, none work today.  

How can I make this work?  Did I forget some crucial step?

For the rest of this bug report, it's basically how I arrived at the conclusion that

`checkpoint = torch.load(ckpt_path, map_location=""cpu"")`

Is not working, and I'm not sure why.  Once I get to that point, the RAM usage rises from 1.8GB to 28.9GB, so it looks like it's at least found the first file in the checkpoint.  This instance g5.12xlarge has 196GB and 4 24GB GPUs (and everything worked yesterday).

To figure this all out, I went into generation.py in the llama directory, and I added in some line number inspections.  I added in lines like:

```
from inspect import getframeinfo, currentframe

print(f""Got to {getframeinfo(currentframe()).lineno}"")
```

The code in generation now looks like: 

```
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        assert len(checkpoints) > 0, f""no checkpoint files found in {ckpt_dir}""
        assert model_parallel_size == len(
            checkpoints
        ), f""Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}""
        
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        ckpt_path = checkpoints[get_model_parallel_rank()]
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        checkpoint = torch.load(ckpt_path, map_location=""cpu"")
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        with open(Path(ckpt_dir) / ""params.json"", ""r"") as f:
            params = json.loads(f.read())
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        model_args: ModelArgs = ModelArgs(
            max_seq_len=max_seq_len,
            max_batch_size=max_batch_size,
            **params,
        )
        tokenizer = Tokenizer(model_path=tokenizer_path)
        model_args.vocab_size = tokenizer.n_words
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
```

and the run output looks like:

```
sh-4.2$ torchrun --nproc_per_node 4 example_instructions.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 4
> initializing ddp with size 1
> initializing pipeline with size 1
Got to 86
Got to 92
Got to 94
```
which is `checkpoint = torch.load(ckpt_path, map_location=""cpu"")`

My `pip freeze`:
```
sh-4.2$ pip freeze
aiobotocore @ file:///home/conda/feedstock_root/build_artifacts/aiobotocore_1691451276487/work
aiofiles==22.1.0
aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1689804989077/work
aioitertools @ file:///home/conda/feedstock_root/build_artifacts/aioitertools_1663521246073/work
aiosignal @ file:///home/conda/feedstock_root/build_artifacts/aiosignal_1667935791922/work
aiosqlite==0.19.0
anyio @ file:///home/conda/feedstock_root/build_artifacts/anyio_1688651106312/work/dist
argon2-cffi @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi_1640817743617/work
argon2-cffi-bindings @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi-bindings_1666850768662/work
arrow @ file:///home/conda/feedstock_root/build_artifacts/arrow_1662382474514/work
astroid==2.15.6
asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1670263926556/work
async-timeout @ file:///home/conda/feedstock_root/build_artifacts/async-timeout_1691763562544/work
attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1683424013410/work
autopep8==2.0.2
autovizwidget @ file:///home/conda/feedstock_root/build_artifacts/autovizwidget_1680800327357/work
awscli==1.29.28
Babel==2.12.1
backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work
backports.functools-lru-cache @ file:///home/conda/feedstock_root/build_artifacts/backports.functools_lru_cache_1687772187254/work
beautifulsoup4 @ file:///home/conda/feedstock_root/build_artifacts/beautifulsoup4_1680888073205/work
bleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1674535352125/work
boto3==1.28.28
botocore==1.31.28
brotlipy @ file:///home/conda/feedstock_root/build_artifacts/brotlipy_1666764671472/work
cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work
certifi==2023.7.22
cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1671179353105/work
charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1688813409104/work
cloudpickle==2.2.1
cmake==3.27.2
-e git+ssh://git@github.com/facebookresearch/codellama.git@cb51c14ec761370ba2e2bc351374a79265d0465e#egg=codellama
colorama==0.4.4
comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1691044910542/work
contextlib2==21.6.0
cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography-split_1672672382195/work
debugpy @ file:///home/conda/feedstock_root/build_artifacts/debugpy_1691021228385/work
decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work
defusedxml @ file:///home/conda/feedstock_root/build_artifacts/defusedxml_1615232257335/work
dill==0.3.7
docker==6.1.3
docstring-to-markdown==0.12
docutils==0.16
entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work
environment-kernels==1.2.0
exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1692026125334/work
executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1667317341051/work
fairscale==0.4.13
fastjsonschema @ file:///home/conda/feedstock_root/build_artifacts/python-fastjsonschema_1690055433477/work/dist
filelock==3.12.3
fire==0.5.0
flit_core @ file:///home/conda/feedstock_root/build_artifacts/flit-core_1684084314667/work/source/flit_core
fqdn @ file:///home/conda/feedstock_root/build_artifacts/fqdn_1638810296540/work/dist
frozenlist @ file:///home/conda/feedstock_root/build_artifacts/frozenlist_1689244399117/work
fsspec @ file:///home/conda/feedstock_root/build_artifacts/fsspec_1626188337504/work
gitdb==4.0.10
GitPython==3.1.32
google-pasta==0.2.0
hdijupyterutils @ file:///home/conda/feedstock_root/build_artifacts/hdijupyterutils_1680800332182/work
idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work
importlib-metadata==6.8.0
importlib-resources @ file:///home/conda/feedstock_root/build_artifacts/importlib_resources_1691408075105/work
ipykernel==5.5.6
ipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1685727741709/work
ipython-genutils==0.2.0
ipywidgets @ file:///home/conda/feedstock_root/build_artifacts/ipywidgets_1690877070294/work
isoduration @ file:///home/conda/feedstock_root/build_artifacts/isoduration_1638811571363/work/dist
isort==5.12.0
jedi==0.18.2
Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1654302431367/work
jmespath @ file:///home/conda/feedstock_root/build_artifacts/jmespath_1655568249366/work
json5==0.9.14
jsonpointer==2.0
jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-meta_1691761378595/work
jsonschema-specifications @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-specifications_1689701150890/work
jupyter @ file:///home/conda/feedstock_root/build_artifacts/jupyter_1670249595582/work
jupyter-console @ file:///home/conda/feedstock_root/build_artifacts/jupyter_console_1678118109161/work
jupyter-events @ file:///home/conda/feedstock_root/build_artifacts/jupyter_events_1691505939576/work
jupyter-lsp==2.2.0
jupyter-server-mathjax==0.2.6
jupyter-server-proxy @ git+https://github.com/jupyterhub/jupyter-server-proxy@2d7dd346bb595106b417476de870a348943f3c70
jupyter-ydoc==0.2.5
jupyter_client==7.4.9
jupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1686775611663/work
jupyter_server @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_1692108700252/work
jupyter_server_fileid==0.9.0
jupyter_server_terminals @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_terminals_1673491454549/work
jupyter_server_ydoc==0.8.0
jupyterlab==3.6.5
jupyterlab-git==0.41.0
jupyterlab-lsp==4.2.0
jupyterlab-pygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1649936611996/work
jupyterlab-widgets @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_widgets_1688489450369/work
jupyterlab_server==2.24.0
lazy-object-proxy==1.9.0
lit==16.0.6
MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1685769049201/work
matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work
mccabe==0.7.0
mistune @ file:///home/conda/feedstock_root/build_artifacts/mistune_1675771498296/work
mock @ file:///home/conda/feedstock_root/build_artifacts/mock_1689092066756/work
mpmath==1.3.0
multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1672339403932/work
multiprocess==0.70.15
nb-conda @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_1654442778977/work
nb-conda-kernels @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_kernels_1667060632461/work
nbclassic @ file:///home/conda/feedstock_root/build_artifacts/nbclassic_1675369808718/work
nbclient @ file:///home/conda/feedstock_root/build_artifacts/nbclient_1684790896106/work
nbconvert @ file:///home/conda/feedstock_root/build_artifacts/nbconvert-meta_1674590374792/work
nbdime==3.2.1
nbexamples @ file:///opt/nbexamples
nbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1690814868471/work
nest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1664684991461/work
networkx==3.1
nose @ file:///home/conda/feedstock_root/build_artifacts/nose_1602434998960/work
notebook @ file:///home/conda/feedstock_root/build_artifacts/notebook_1691436218243/work
notebook_shim @ file:///home/conda/feedstock_root/build_artifacts/notebook-shim_1682360583588/work
numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1691056231492/work
nvidia-cublas-cu11==11.10.3.66
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cudnn-cu11==8.5.0.96
nvidia-cufft-cu11==10.9.0.58
nvidia-curand-cu11==10.2.10.91
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusparse-cu11==11.7.4.91
nvidia-nccl-cu11==2.14.3
nvidia-nvtx-cu11==11.7.91
overrides @ file:///home/conda/feedstock_root/build_artifacts/overrides_1691338815398/work
packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1681337016113/work
pandas @ file:///home/conda/feedstock_root/build_artifacts/pandas_1688740542634/work
pandocfilters @ file:///home/conda/feedstock_root/build_artifacts/pandocfilters_1631603243851/work
parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work
pathos==0.3.1
pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1667297516076/work
pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work
pid==3.0.4
pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1633981968097/work
platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1690813113769/work
plotly @ file:///home/conda/feedstock_root/build_artifacts/plotly_1692220561510/work
pluggy==1.2.0
pox==0.3.3
ppft==1.7.6.7
prometheus-client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1689032443210/work
prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1688565951714/work
protobuf==4.23.4
psutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1681775027942/work
psycopg2 @ file:///home/conda/feedstock_root/build_artifacts/psycopg2-split_1667025517155/work
ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl
pure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work
py4j==0.10.9.5
pyasn1==0.5.0
pycodestyle==2.10.0
pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work
pydocstyle==6.3.0
pyflakes==3.0.1
pygal==3.0.0
Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1691408637400/work
pykerberos @ file:///home/conda/feedstock_root/build_artifacts/pykerberos_1671204518513/work
pylint==2.17.5
pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1685514481738/work
PyQt5==5.12.3
PyQt5_sip==4.19.18
PyQtChart==5.12
PyQtWebEngine==5.12.1
PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1661604839144/work
pyspark==3.3.0
python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work
python-json-logger @ file:///home/conda/feedstock_root/build_artifacts/python-json-logger_1677079630776/work
python-lsp-jsonrpc==1.0.0
python-lsp-server==1.7.4
pytoolconfig==1.2.5
pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work
PyYAML @ file:///home/conda/feedstock_root/build_artifacts/pyyaml_1666772395347/work
pyzmq @ file:///home/conda/feedstock_root/build_artifacts/pyzmq_1666828497229/work
qtconsole @ file:///home/conda/feedstock_root/build_artifacts/qtconsole-base_1683329453903/work
QtPy @ file:///home/conda/feedstock_root/build_artifacts/qtpy_1680148448366/work
referencing @ file:///home/conda/feedstock_root/build_artifacts/referencing_1691337268233/work
requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1684774241324/work
requests-kerberos @ file:///home/conda/feedstock_root/build_artifacts/requests-kerberos_1667464887610/work
rfc3339-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3339-validator_1638811747357/work
rfc3986-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3986-validator_1598024191506/work
rope==1.9.0
rpds-py @ file:///home/conda/feedstock_root/build_artifacts/rpds-py_1689705060450/work
rsa==4.7.2
s3fs @ file:///home/conda/feedstock_root/build_artifacts/s3fs_1626193591467/work
s3transfer @ file:///home/conda/feedstock_root/build_artifacts/s3transfer_1692149178344/work
sagemaker==2.177.1
sagemaker-experiments==0.1.45
sagemaker-nbi-agent @ file:///opt/sagemaker_nbi_agent
sagemaker-pyspark==1.4.5
schema==0.7.5
Send2Trash @ file:///home/conda/feedstock_root/build_artifacts/send2trash_1682601222253/work
sentencepiece==0.1.99
simpervisor==1.0.0
six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work
smdebug-rulesconfig==1.0.1
smmap==5.0.0
sniffio @ file:///home/conda/feedstock_root/build_artifacts/sniffio_1662051266223/work
snowballstemmer==2.2.0
soupsieve @ file:///home/conda/feedstock_root/build_artifacts/soupsieve_1658207591808/work
sparkmagic @ file:///home/conda/feedstock_root/build_artifacts/sparkmagic_1680849855330/work/sparkmagic
stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work
sympy==1.12
tblib==1.7.0
tenacity @ file:///home/conda/feedstock_root/build_artifacts/tenacity_1692026804430/work
termcolor==2.3.0
terminado @ file:///home/conda/feedstock_root/build_artifacts/terminado_1670253674810/work
tinycss2 @ file:///home/conda/feedstock_root/build_artifacts/tinycss2_1666100256010/work
tomli==2.0.1
tomlkit==0.12.1
torch==2.0.1
tornado @ file:///home/conda/feedstock_root/build_artifacts/tornado_1684150054582/work
traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1675110562325/work
triton==2.0.0
typing-utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1622899189314/work
typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1688315532570/work
tzdata @ file:///home/conda/feedstock_root/build_artifacts/python-tzdata_1680081134351/work
ujson==5.8.0
uri-template @ file:///home/conda/feedstock_root/build_artifacts/uri-template_1688655812972/work/dist
urllib3==1.26.14
wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1673864653149/work
webcolors @ file:///home/conda/feedstock_root/build_artifacts/webcolors_1679900785843/work
webencodings==0.5.1
websocket-client @ file:///home/conda/feedstock_root/build_artifacts/websocket-client_1687789148259/work
widgetsnbextension @ file:///home/conda/feedstock_root/build_artifacts/widgetsnbextension_1688504439014/work
wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1677485519705/work
y-py==0.6.0
yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1685191749966/work
ypy-websocket==0.8.4
zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1689374466814/work
```",mmr-crexi,"Maybe?  But it's still not adding much to the conversation.  

These models work well when you can understand what they're saying and change their output into whatever's appropriate for the situation.  The response you gave to this bug, for instance, was so off the mark that it seemed like you just cut and pasted the response without thinking about what was actually being said, whether or not it was actually a help.  That's just noise, that's not insight."
63,"Unable to run this morning, yesterday it ran fine","Morning!  I need help getting the models to run a _second_ time, on a new instance.

Yesterday, I registered for and downloaded the models onto an AWS sagemaker instance.  Everything worked fine and I was able to run 

`pip install -e .`

And from there experiment with the models.  I shut down the instance and this morning started it again.  I reran the pip installation, but now, everything hangs at this step:

```
sh-4.2$ torchrun --nproc_per_node 4 example_instructions.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 4
> initializing ddp with size 1
> initializing pipeline with size 1
```

This same code would finish loading the model after 8 seconds or so and be good to go.  I've tried this with the 7b instruct model, the 13b instruct, and the 34b instruct; all worked fine yesterday, none work today.  

How can I make this work?  Did I forget some crucial step?

For the rest of this bug report, it's basically how I arrived at the conclusion that

`checkpoint = torch.load(ckpt_path, map_location=""cpu"")`

Is not working, and I'm not sure why.  Once I get to that point, the RAM usage rises from 1.8GB to 28.9GB, so it looks like it's at least found the first file in the checkpoint.  This instance g5.12xlarge has 196GB and 4 24GB GPUs (and everything worked yesterday).

To figure this all out, I went into generation.py in the llama directory, and I added in some line number inspections.  I added in lines like:

```
from inspect import getframeinfo, currentframe

print(f""Got to {getframeinfo(currentframe()).lineno}"")
```

The code in generation now looks like: 

```
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        assert len(checkpoints) > 0, f""no checkpoint files found in {ckpt_dir}""
        assert model_parallel_size == len(
            checkpoints
        ), f""Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}""
        
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        ckpt_path = checkpoints[get_model_parallel_rank()]
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        checkpoint = torch.load(ckpt_path, map_location=""cpu"")
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        with open(Path(ckpt_dir) / ""params.json"", ""r"") as f:
            params = json.loads(f.read())
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        model_args: ModelArgs = ModelArgs(
            max_seq_len=max_seq_len,
            max_batch_size=max_batch_size,
            **params,
        )
        tokenizer = Tokenizer(model_path=tokenizer_path)
        model_args.vocab_size = tokenizer.n_words
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
```

and the run output looks like:

```
sh-4.2$ torchrun --nproc_per_node 4 example_instructions.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 4
> initializing ddp with size 1
> initializing pipeline with size 1
Got to 86
Got to 92
Got to 94
```
which is `checkpoint = torch.load(ckpt_path, map_location=""cpu"")`

My `pip freeze`:
```
sh-4.2$ pip freeze
aiobotocore @ file:///home/conda/feedstock_root/build_artifacts/aiobotocore_1691451276487/work
aiofiles==22.1.0
aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1689804989077/work
aioitertools @ file:///home/conda/feedstock_root/build_artifacts/aioitertools_1663521246073/work
aiosignal @ file:///home/conda/feedstock_root/build_artifacts/aiosignal_1667935791922/work
aiosqlite==0.19.0
anyio @ file:///home/conda/feedstock_root/build_artifacts/anyio_1688651106312/work/dist
argon2-cffi @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi_1640817743617/work
argon2-cffi-bindings @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi-bindings_1666850768662/work
arrow @ file:///home/conda/feedstock_root/build_artifacts/arrow_1662382474514/work
astroid==2.15.6
asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1670263926556/work
async-timeout @ file:///home/conda/feedstock_root/build_artifacts/async-timeout_1691763562544/work
attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1683424013410/work
autopep8==2.0.2
autovizwidget @ file:///home/conda/feedstock_root/build_artifacts/autovizwidget_1680800327357/work
awscli==1.29.28
Babel==2.12.1
backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work
backports.functools-lru-cache @ file:///home/conda/feedstock_root/build_artifacts/backports.functools_lru_cache_1687772187254/work
beautifulsoup4 @ file:///home/conda/feedstock_root/build_artifacts/beautifulsoup4_1680888073205/work
bleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1674535352125/work
boto3==1.28.28
botocore==1.31.28
brotlipy @ file:///home/conda/feedstock_root/build_artifacts/brotlipy_1666764671472/work
cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work
certifi==2023.7.22
cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1671179353105/work
charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1688813409104/work
cloudpickle==2.2.1
cmake==3.27.2
-e git+ssh://git@github.com/facebookresearch/codellama.git@cb51c14ec761370ba2e2bc351374a79265d0465e#egg=codellama
colorama==0.4.4
comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1691044910542/work
contextlib2==21.6.0
cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography-split_1672672382195/work
debugpy @ file:///home/conda/feedstock_root/build_artifacts/debugpy_1691021228385/work
decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work
defusedxml @ file:///home/conda/feedstock_root/build_artifacts/defusedxml_1615232257335/work
dill==0.3.7
docker==6.1.3
docstring-to-markdown==0.12
docutils==0.16
entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work
environment-kernels==1.2.0
exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1692026125334/work
executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1667317341051/work
fairscale==0.4.13
fastjsonschema @ file:///home/conda/feedstock_root/build_artifacts/python-fastjsonschema_1690055433477/work/dist
filelock==3.12.3
fire==0.5.0
flit_core @ file:///home/conda/feedstock_root/build_artifacts/flit-core_1684084314667/work/source/flit_core
fqdn @ file:///home/conda/feedstock_root/build_artifacts/fqdn_1638810296540/work/dist
frozenlist @ file:///home/conda/feedstock_root/build_artifacts/frozenlist_1689244399117/work
fsspec @ file:///home/conda/feedstock_root/build_artifacts/fsspec_1626188337504/work
gitdb==4.0.10
GitPython==3.1.32
google-pasta==0.2.0
hdijupyterutils @ file:///home/conda/feedstock_root/build_artifacts/hdijupyterutils_1680800332182/work
idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work
importlib-metadata==6.8.0
importlib-resources @ file:///home/conda/feedstock_root/build_artifacts/importlib_resources_1691408075105/work
ipykernel==5.5.6
ipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1685727741709/work
ipython-genutils==0.2.0
ipywidgets @ file:///home/conda/feedstock_root/build_artifacts/ipywidgets_1690877070294/work
isoduration @ file:///home/conda/feedstock_root/build_artifacts/isoduration_1638811571363/work/dist
isort==5.12.0
jedi==0.18.2
Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1654302431367/work
jmespath @ file:///home/conda/feedstock_root/build_artifacts/jmespath_1655568249366/work
json5==0.9.14
jsonpointer==2.0
jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-meta_1691761378595/work
jsonschema-specifications @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-specifications_1689701150890/work
jupyter @ file:///home/conda/feedstock_root/build_artifacts/jupyter_1670249595582/work
jupyter-console @ file:///home/conda/feedstock_root/build_artifacts/jupyter_console_1678118109161/work
jupyter-events @ file:///home/conda/feedstock_root/build_artifacts/jupyter_events_1691505939576/work
jupyter-lsp==2.2.0
jupyter-server-mathjax==0.2.6
jupyter-server-proxy @ git+https://github.com/jupyterhub/jupyter-server-proxy@2d7dd346bb595106b417476de870a348943f3c70
jupyter-ydoc==0.2.5
jupyter_client==7.4.9
jupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1686775611663/work
jupyter_server @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_1692108700252/work
jupyter_server_fileid==0.9.0
jupyter_server_terminals @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_terminals_1673491454549/work
jupyter_server_ydoc==0.8.0
jupyterlab==3.6.5
jupyterlab-git==0.41.0
jupyterlab-lsp==4.2.0
jupyterlab-pygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1649936611996/work
jupyterlab-widgets @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_widgets_1688489450369/work
jupyterlab_server==2.24.0
lazy-object-proxy==1.9.0
lit==16.0.6
MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1685769049201/work
matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work
mccabe==0.7.0
mistune @ file:///home/conda/feedstock_root/build_artifacts/mistune_1675771498296/work
mock @ file:///home/conda/feedstock_root/build_artifacts/mock_1689092066756/work
mpmath==1.3.0
multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1672339403932/work
multiprocess==0.70.15
nb-conda @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_1654442778977/work
nb-conda-kernels @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_kernels_1667060632461/work
nbclassic @ file:///home/conda/feedstock_root/build_artifacts/nbclassic_1675369808718/work
nbclient @ file:///home/conda/feedstock_root/build_artifacts/nbclient_1684790896106/work
nbconvert @ file:///home/conda/feedstock_root/build_artifacts/nbconvert-meta_1674590374792/work
nbdime==3.2.1
nbexamples @ file:///opt/nbexamples
nbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1690814868471/work
nest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1664684991461/work
networkx==3.1
nose @ file:///home/conda/feedstock_root/build_artifacts/nose_1602434998960/work
notebook @ file:///home/conda/feedstock_root/build_artifacts/notebook_1691436218243/work
notebook_shim @ file:///home/conda/feedstock_root/build_artifacts/notebook-shim_1682360583588/work
numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1691056231492/work
nvidia-cublas-cu11==11.10.3.66
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cudnn-cu11==8.5.0.96
nvidia-cufft-cu11==10.9.0.58
nvidia-curand-cu11==10.2.10.91
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusparse-cu11==11.7.4.91
nvidia-nccl-cu11==2.14.3
nvidia-nvtx-cu11==11.7.91
overrides @ file:///home/conda/feedstock_root/build_artifacts/overrides_1691338815398/work
packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1681337016113/work
pandas @ file:///home/conda/feedstock_root/build_artifacts/pandas_1688740542634/work
pandocfilters @ file:///home/conda/feedstock_root/build_artifacts/pandocfilters_1631603243851/work
parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work
pathos==0.3.1
pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1667297516076/work
pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work
pid==3.0.4
pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1633981968097/work
platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1690813113769/work
plotly @ file:///home/conda/feedstock_root/build_artifacts/plotly_1692220561510/work
pluggy==1.2.0
pox==0.3.3
ppft==1.7.6.7
prometheus-client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1689032443210/work
prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1688565951714/work
protobuf==4.23.4
psutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1681775027942/work
psycopg2 @ file:///home/conda/feedstock_root/build_artifacts/psycopg2-split_1667025517155/work
ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl
pure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work
py4j==0.10.9.5
pyasn1==0.5.0
pycodestyle==2.10.0
pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work
pydocstyle==6.3.0
pyflakes==3.0.1
pygal==3.0.0
Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1691408637400/work
pykerberos @ file:///home/conda/feedstock_root/build_artifacts/pykerberos_1671204518513/work
pylint==2.17.5
pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1685514481738/work
PyQt5==5.12.3
PyQt5_sip==4.19.18
PyQtChart==5.12
PyQtWebEngine==5.12.1
PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1661604839144/work
pyspark==3.3.0
python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work
python-json-logger @ file:///home/conda/feedstock_root/build_artifacts/python-json-logger_1677079630776/work
python-lsp-jsonrpc==1.0.0
python-lsp-server==1.7.4
pytoolconfig==1.2.5
pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work
PyYAML @ file:///home/conda/feedstock_root/build_artifacts/pyyaml_1666772395347/work
pyzmq @ file:///home/conda/feedstock_root/build_artifacts/pyzmq_1666828497229/work
qtconsole @ file:///home/conda/feedstock_root/build_artifacts/qtconsole-base_1683329453903/work
QtPy @ file:///home/conda/feedstock_root/build_artifacts/qtpy_1680148448366/work
referencing @ file:///home/conda/feedstock_root/build_artifacts/referencing_1691337268233/work
requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1684774241324/work
requests-kerberos @ file:///home/conda/feedstock_root/build_artifacts/requests-kerberos_1667464887610/work
rfc3339-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3339-validator_1638811747357/work
rfc3986-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3986-validator_1598024191506/work
rope==1.9.0
rpds-py @ file:///home/conda/feedstock_root/build_artifacts/rpds-py_1689705060450/work
rsa==4.7.2
s3fs @ file:///home/conda/feedstock_root/build_artifacts/s3fs_1626193591467/work
s3transfer @ file:///home/conda/feedstock_root/build_artifacts/s3transfer_1692149178344/work
sagemaker==2.177.1
sagemaker-experiments==0.1.45
sagemaker-nbi-agent @ file:///opt/sagemaker_nbi_agent
sagemaker-pyspark==1.4.5
schema==0.7.5
Send2Trash @ file:///home/conda/feedstock_root/build_artifacts/send2trash_1682601222253/work
sentencepiece==0.1.99
simpervisor==1.0.0
six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work
smdebug-rulesconfig==1.0.1
smmap==5.0.0
sniffio @ file:///home/conda/feedstock_root/build_artifacts/sniffio_1662051266223/work
snowballstemmer==2.2.0
soupsieve @ file:///home/conda/feedstock_root/build_artifacts/soupsieve_1658207591808/work
sparkmagic @ file:///home/conda/feedstock_root/build_artifacts/sparkmagic_1680849855330/work/sparkmagic
stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work
sympy==1.12
tblib==1.7.0
tenacity @ file:///home/conda/feedstock_root/build_artifacts/tenacity_1692026804430/work
termcolor==2.3.0
terminado @ file:///home/conda/feedstock_root/build_artifacts/terminado_1670253674810/work
tinycss2 @ file:///home/conda/feedstock_root/build_artifacts/tinycss2_1666100256010/work
tomli==2.0.1
tomlkit==0.12.1
torch==2.0.1
tornado @ file:///home/conda/feedstock_root/build_artifacts/tornado_1684150054582/work
traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1675110562325/work
triton==2.0.0
typing-utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1622899189314/work
typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1688315532570/work
tzdata @ file:///home/conda/feedstock_root/build_artifacts/python-tzdata_1680081134351/work
ujson==5.8.0
uri-template @ file:///home/conda/feedstock_root/build_artifacts/uri-template_1688655812972/work/dist
urllib3==1.26.14
wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1673864653149/work
webcolors @ file:///home/conda/feedstock_root/build_artifacts/webcolors_1679900785843/work
webencodings==0.5.1
websocket-client @ file:///home/conda/feedstock_root/build_artifacts/websocket-client_1687789148259/work
widgetsnbextension @ file:///home/conda/feedstock_root/build_artifacts/widgetsnbextension_1688504439014/work
wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1677485519705/work
y-py==0.6.0
yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1685191749966/work
ypy-websocket==0.8.4
zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1689374466814/work
```",GaganHonor,I'm sorry but I prefer not to continue this conversation. I'm still learning so I appreciate your understanding and patience.🙏
62,Remembering the previous context,"Hi everyone! Is it possible to make long dialogue with the Instruct model? In other words, to make the model remember a previous context.

For now, I have an idea to paste the previous prompt and the model's response into the new request. Are there more concise and easier ways?",GaganHonor,"Yes, it is possible to have long dialogues with the Instruct model in Codellama 2023 and make the model remember a previous context. While the approach you mentioned of pasting the previous prompt and the model's response into the new request can work, there are more concise and easier ways to achieve this. Here are a few suggestions:

Conversation History: Instead of manually copying and pasting the previous prompt and model's response, you can maintain a conversation history. In Codellama 2023, you can achieve this by providing a list of messages as your input. Each message in the list consists of a 'role' (e.g., 'system', 'user', 'assistant') and 'content' (the text of the message). By appending the previous prompts and model responses to the conversation history, you can maintain context and have longer dialogues.

Token-based Approach: Another approach is to use a token-based strategy. In this approach, you assign specific tokens to represent different parts of the dialogue, such as user prompts, model responses, and conversation history. By using these tokens, you can easily structure and separate the different elements of the dialogue in a concise manner.

Context Managers: Some libraries or frameworks may provide context managers or helper functions to handle conversations and context automatically. These tools can simplify the process of maintaining and updating the dialogue context.

Remember that when working with long dialogues, you need to be mindful of the model's token limit. If the dialogue becomes too long, you may need to truncate or omit parts of the conversation to fit within the model's maximum token limit.

Overall, maintaining a conversation history, using a token-based approach, or leveraging context managers can help you handle long dialogues more efficiently in Codellama 2023. Experiment with these techniques and choose the one that best suits your needs and coding style.
"
62,Remembering the previous context,"Hi everyone! Is it possible to make long dialogue with the Instruct model? In other words, to make the model remember a previous context.

For now, I have an idea to paste the previous prompt and the model's response into the new request. Are there more concise and easier ways?",univanxx,"Thank you ChatGPT but I asked for quality advice from a real person ;)

Anyways, I found another way to make a dialogue: update an [instructions](https://github.com/facebookresearch/codellama/blob/cb51c14ec761370ba2e2bc351374a79265d0465e/example_instructions.py#L27) array like that:
```
instructions = [
        [
            {""role"": ""user"", ""content"": ""Message #1""},
            {
                ""role"": ""assistant"",
                ""content"": ""Answer #1"",
            },
            {""role"": ""user"", ""Message #2""},
           ...
        ]
    ]
```

Depending on your maximum context length, model can successfully chat for some time."
62,Remembering the previous context,"Hi everyone! Is it possible to make long dialogue with the Instruct model? In other words, to make the model remember a previous context.

For now, I have an idea to paste the previous prompt and the model's response into the new request. Are there more concise and easier ways?",sidaw,"The self-answer by @univanxx above is right. If you look at the code, this structured prompt gets concatenated before being fed into the model.

The model support this particular format because it was fine-tuned on such data. To better understand what kind of format and function the model supports, check out how llama is tuned for this (section 3.3): https://arxiv.org/pdf/2307.09288.pdf"
61,unable to download the model weights," I was unable to download the authorized model within the 24-hour timeframe specified in the authorization email.

--2023-08-29 22:06:38--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 7020 (6.9K) [binary/octet-stream]
Saving to: './LICENSE'

./LICENSE                                           100%[===================================================================================================================>]   6.86K  --.-KB/s    in 0s      

2023-08-29 22:06:39 (163 MB/s) - './LICENSE' saved [7020/7020]

--2023-08-29 22:06:39--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4790 (4.7K) [text/markdown]
Saving to: './USE_POLICY.md'

./USE_POLICY.md                                     100%[===================================================================================================================>]   4.68K  --.-KB/s    in 0s      

2023-08-29 22:06:39 (139 MB/s) - './USE_POLICY.md' saved [4790/4790]

Downloading tokenizer
--2023-08-29 22:06:39--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-29 22:06:41 ERROR 403: Forbidden.

--2023-08-29 22:06:41--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Reusing existing connection to download2.llamameta.net:443.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-29 22:06:41 ERROR 403: Forbidden.",GaganHonor,"Did you used the Download URL within 24 hours ?
"
61,unable to download the model weights," I was unable to download the authorized model within the 24-hour timeframe specified in the authorization email.

--2023-08-29 22:06:38--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 7020 (6.9K) [binary/octet-stream]
Saving to: './LICENSE'

./LICENSE                                           100%[===================================================================================================================>]   6.86K  --.-KB/s    in 0s      

2023-08-29 22:06:39 (163 MB/s) - './LICENSE' saved [7020/7020]

--2023-08-29 22:06:39--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4790 (4.7K) [text/markdown]
Saving to: './USE_POLICY.md'

./USE_POLICY.md                                     100%[===================================================================================================================>]   4.68K  --.-KB/s    in 0s      

2023-08-29 22:06:39 (139 MB/s) - './USE_POLICY.md' saved [4790/4790]

Downloading tokenizer
--2023-08-29 22:06:39--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-29 22:06:41 ERROR 403: Forbidden.

--2023-08-29 22:06:41--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Reusing existing connection to download2.llamameta.net:443.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-29 22:06:41 ERROR 403: Forbidden.",freesky-edward,"I got the same message, I am sure it within 24 hours, are there any other restrictions?"
61,unable to download the model weights," I was unable to download the authorized model within the 24-hour timeframe specified in the authorization email.

--2023-08-29 22:06:38--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 7020 (6.9K) [binary/octet-stream]
Saving to: './LICENSE'

./LICENSE                                           100%[===================================================================================================================>]   6.86K  --.-KB/s    in 0s      

2023-08-29 22:06:39 (163 MB/s) - './LICENSE' saved [7020/7020]

--2023-08-29 22:06:39--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4790 (4.7K) [text/markdown]
Saving to: './USE_POLICY.md'

./USE_POLICY.md                                     100%[===================================================================================================================>]   4.68K  --.-KB/s    in 0s      

2023-08-29 22:06:39 (139 MB/s) - './USE_POLICY.md' saved [4790/4790]

Downloading tokenizer
--2023-08-29 22:06:39--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-29 22:06:41 ERROR 403: Forbidden.

--2023-08-29 22:06:41--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Reusing existing connection to download2.llamameta.net:443.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-29 22:06:41 ERROR 403: Forbidden.",freesky-edward,Everything works fine after applying again
61,unable to download the model weights," I was unable to download the authorized model within the 24-hour timeframe specified in the authorization email.

--2023-08-29 22:06:38--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 7020 (6.9K) [binary/octet-stream]
Saving to: './LICENSE'

./LICENSE                                           100%[===================================================================================================================>]   6.86K  --.-KB/s    in 0s      

2023-08-29 22:06:39 (163 MB/s) - './LICENSE' saved [7020/7020]

--2023-08-29 22:06:39--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4790 (4.7K) [text/markdown]
Saving to: './USE_POLICY.md'

./USE_POLICY.md                                     100%[===================================================================================================================>]   4.68K  --.-KB/s    in 0s      

2023-08-29 22:06:39 (139 MB/s) - './USE_POLICY.md' saved [4790/4790]

Downloading tokenizer
--2023-08-29 22:06:39--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-29 22:06:41 ERROR 403: Forbidden.

--2023-08-29 22:06:41--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Reusing existing connection to download2.llamameta.net:443.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-29 22:06:41 ERROR 403: Forbidden.",jspisak,Can we close this issue? Seems like @samuelselvan we should document this somewhere?
61,unable to download the model weights," I was unable to download the authorized model within the 24-hour timeframe specified in the authorization email.

--2023-08-29 22:06:38--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 7020 (6.9K) [binary/octet-stream]
Saving to: './LICENSE'

./LICENSE                                           100%[===================================================================================================================>]   6.86K  --.-KB/s    in 0s      

2023-08-29 22:06:39 (163 MB/s) - './LICENSE' saved [7020/7020]

--2023-08-29 22:06:39--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4790 (4.7K) [text/markdown]
Saving to: './USE_POLICY.md'

./USE_POLICY.md                                     100%[===================================================================================================================>]   4.68K  --.-KB/s    in 0s      

2023-08-29 22:06:39 (139 MB/s) - './USE_POLICY.md' saved [4790/4790]

Downloading tokenizer
--2023-08-29 22:06:39--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-29 22:06:41 ERROR 403: Forbidden.

--2023-08-29 22:06:41--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Reusing existing connection to download2.llamameta.net:443.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-29 22:06:41 ERROR 403: Forbidden.",GaganHonor,"yes close this issue :) 
![CommitPoliceCommitGIF](https://github.com/facebookresearch/codellama/assets/141204441/58496352-01fe-4bc8-a60a-aa9005e5addc)
"
61,unable to download the model weights," I was unable to download the authorized model within the 24-hour timeframe specified in the authorization email.

--2023-08-29 22:06:38--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 7020 (6.9K) [binary/octet-stream]
Saving to: './LICENSE'

./LICENSE                                           100%[===================================================================================================================>]   6.86K  --.-KB/s    in 0s      

2023-08-29 22:06:39 (163 MB/s) - './LICENSE' saved [7020/7020]

--2023-08-29 22:06:39--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4790 (4.7K) [text/markdown]
Saving to: './USE_POLICY.md'

./USE_POLICY.md                                     100%[===================================================================================================================>]   4.68K  --.-KB/s    in 0s      

2023-08-29 22:06:39 (139 MB/s) - './USE_POLICY.md' saved [4790/4790]

Downloading tokenizer
--2023-08-29 22:06:39--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-29 22:06:41 ERROR 403: Forbidden.

--2023-08-29 22:06:41--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Reusing existing connection to download2.llamameta.net:443.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-29 22:06:41 ERROR 403: Forbidden.",jspisak,roger that!
61,unable to download the model weights," I was unable to download the authorized model within the 24-hour timeframe specified in the authorization email.

--2023-08-29 22:06:38--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 7020 (6.9K) [binary/octet-stream]
Saving to: './LICENSE'

./LICENSE                                           100%[===================================================================================================================>]   6.86K  --.-KB/s    in 0s      

2023-08-29 22:06:39 (163 MB/s) - './LICENSE' saved [7020/7020]

--2023-08-29 22:06:39--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4790 (4.7K) [text/markdown]
Saving to: './USE_POLICY.md'

./USE_POLICY.md                                     100%[===================================================================================================================>]   4.68K  --.-KB/s    in 0s      

2023-08-29 22:06:39 (139 MB/s) - './USE_POLICY.md' saved [4790/4790]

Downloading tokenizer
--2023-08-29 22:06:39--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-29 22:06:41 ERROR 403: Forbidden.

--2023-08-29 22:06:41--  https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
Reusing existing connection to download2.llamameta.net:443.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-29 22:06:41 ERROR 403: Forbidden.",Soumendraprasad,"@jspisak , @freesky-edward , @vshanyiao  , I am also facing the same . I am trying to download it just after getting the permission mail , but the same is showing . Could you guide to how to resolve this .  This issue is exactly same as @vshanyiao ones .


Connecting to download2.llamameta.net (download2.llamameta.net)|18.67.195.80|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-25 06:27:16 ERROR 403: Forbidden.
"
60,torchrun --nproc_per_node 2 example_instructions.py --ckpt_dir CodeLlama-13b-Instruct/ --tokenizer_path CodeLlama-13b-Instruct/tokenizer.model --max_seq_len 8192 --max_batch_size 4,"WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 2
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/home/azureuser/codellama/example_instructions.py"", line 68, in <module>
    fire.Fire(main)
  File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/azureuser/codellama/example_instructions.py"", line 20, in main
    generator = Llama.build(
  File ""/home/azureuser/codellama/llama/generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
  File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
_pickle.UnpicklingError: invalid load key, '<'.
Traceback (most recent call last):
  File ""/home/azureuser/codellama/example_instructions.py"", line 68, in <module>
    fire.Fire(main)
  File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/azureuser/codellama/example_instructions.py"", line 20, in main
    generator = Llama.build(
  File ""/home/azureuser/codellama/llama/generation.py"", line 75, in build
    torch.cuda.set_device(local_rank)
  File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/cuda/__init__.py"", line 350, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 14881) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/home/azureuser/.local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example_instructions.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-08-29_13:34:23
  host      : llm.internal.cloudapp.net
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 14882)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-29_13:34:23
  host      : llm.internal.cloudapp.net
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 14881)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
",mhamra,"I have the same problem. I reported it in issue #55.

There's something wrong with loading a file ...

```
File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/serialization.py"", line 1033, in _legacy_load
magic_number = pickle_module.load(f, **pickle_load_args)
_pickle.UnpicklingError: invalid load key, '<'.
```
"
60,torchrun --nproc_per_node 2 example_instructions.py --ckpt_dir CodeLlama-13b-Instruct/ --tokenizer_path CodeLlama-13b-Instruct/tokenizer.model --max_seq_len 8192 --max_batch_size 4,"WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 2
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/home/azureuser/codellama/example_instructions.py"", line 68, in <module>
    fire.Fire(main)
  File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/azureuser/codellama/example_instructions.py"", line 20, in main
    generator = Llama.build(
  File ""/home/azureuser/codellama/llama/generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
  File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
_pickle.UnpicklingError: invalid load key, '<'.
Traceback (most recent call last):
  File ""/home/azureuser/codellama/example_instructions.py"", line 68, in <module>
    fire.Fire(main)
  File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/azureuser/codellama/example_instructions.py"", line 20, in main
    generator = Llama.build(
  File ""/home/azureuser/codellama/llama/generation.py"", line 75, in build
    torch.cuda.set_device(local_rank)
  File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/cuda/__init__.py"", line 350, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 14881) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/home/azureuser/.local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example_instructions.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-08-29_13:34:23
  host      : llm.internal.cloudapp.net
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 14882)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-29_13:34:23
  host      : llm.internal.cloudapp.net
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 14881)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
",GaganHonor,"It seems that you are encountering some errors related to downloading and running code. Here are a few suggestions to help you troubleshoot the issue:

OMP_NUM_THREADS environment variable: The warning message indicates that the OMP_NUM_THREADS environment variable is being set to 1. This is done to avoid overloading your system. However, you might need to further tune this variable for optimal performance in your application. You can try adjusting the value of OMP_NUM_THREADS to see if it resolves the issue.

Invalid load key error: The traceback shows an error related to loading a checkpoint file. The error message suggests that there is an invalid load key ('<') in the checkpoint file. This could indicate that the file is corrupted or incompatible with the version of the code you are using. You may need to ensure that the checkpoint file is valid and compatible.

CUDA error: invalid device ordinal: Another error in the traceback suggests a CUDA error related to an invalid device ordinal. This error typically occurs when the code is trying to access a CUDA device that doesn't exist. Make sure that you have the correct CUDA device configured and that it is accessible.

Compile with TORCH_USE_CUDA_DSA: The last error message mentions compiling with TORCH_USE_CUDA_DSA to enable device-side assertions. This is an advanced option related to CUDA programming. If you are not familiar with CUDA programming or don't specifically require device-side assertions, you can ignore this error message.

In summary, try adjusting the OMP_NUM_THREADS environment variable, ensuring the checkpoint file is valid, checking your CUDA device configuration, and ignoring the TORCH_USE_CUDA_DSA error if you are not working with CUDA programming. If the issue persists, you may need to provide more specific details or seek assistance from the code's documentation or support channels."
59,Invalid load key error,"Cmd line:
`torchrun --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b-Instruct/ --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model --max_seq_len 512 -- max_batch_size 4`

Error Raised
```
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/home/fran/codellama/example_infilling.py"", line 79, in <module>
    fire.Fire(main)
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/fran/codellama/example_infilling.py"", line 18, in main
    generator = Llama.build(
  File ""/home/fran/codellama/llama/generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
_pickle.UnpicklingError: invalid load key, '<'.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 463834) of binary: /home/fran/miniconda3/envs/cs-gpt/bin/python
Traceback (most recent call last):
  File ""/home/fran/miniconda3/envs/cs-gpt/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_infilling.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-29_20:39:20
  host      : fran.rtzr.ai
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 463834)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",mhamra,I reported a similar problem here: #55 
59,Invalid load key error,"Cmd line:
`torchrun --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b-Instruct/ --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model --max_seq_len 512 -- max_batch_size 4`

Error Raised
```
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/home/fran/codellama/example_infilling.py"", line 79, in <module>
    fire.Fire(main)
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/fran/codellama/example_infilling.py"", line 18, in main
    generator = Llama.build(
  File ""/home/fran/codellama/llama/generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
_pickle.UnpicklingError: invalid load key, '<'.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 463834) of binary: /home/fran/miniconda3/envs/cs-gpt/bin/python
Traceback (most recent call last):
  File ""/home/fran/miniconda3/envs/cs-gpt/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_infilling.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-29_20:39:20
  host      : fran.rtzr.ai
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 463834)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",GaganHonor,"The error message you are getting is:

```
_pickle.UnpicklingError: invalid load key, '<'.
```

This means that the `torch.load()` function was unable to load the checkpoint file. The `<` character is not a valid load key.

The most likely reason for this error is that the checkpoint file is corrupt. You can try to verify the integrity of the checkpoint file by running the following command:

```
sha256sum CodeLlama-7b-Instruct.ckpt
```

If the checksum of the checkpoint file does not match the checksum that is provided in the CodeLlama documentation, then the checkpoint file is corrupt.

If the checkpoint file is not corrupt, then the problem might be with the `torch.load()` function. You can try to update the `torch` library to the latest version.

Here are some additional things you can try:

* Make sure that you are using the correct version of Python. The `torch.load()` function requires Python version 3.6 or higher.
* Make sure that the checkpoint file is in the same location as the `example_infilling.py` file.
* Try running the `torch.load()` function with the `allow_pickle=False` argument. This will prevent the `torch.load()` function from trying to load the checkpoint file as a pickle file.

(❁´◡`❁) Helpful ? Y / N : ASK MOREEEEEE
![CommitPoliceCommitGIF](https://github.com/facebookresearch/codellama/assets/141204441/399c677a-5053-43b4-bdc0-dc0d65658185)
"
58,"Installed md5sum, but there is an error","I am using Ubuntu 22.04 LTS. I followed the instructions in the readme.md to run download.sh and selected all the models. The installation went smoothly at the beginning until it reached CodeLlama-34b/checklist.chk, where I encountered an md5sum error: md5sum: checklist.chk: No such file or directory, but when I checked with md5sum --version, it was indeed installed. Can you please tell me what steps I might have missed?",ZiyadMoraished,"I downloaded it by running the following 

`brew install md5sha1sum
`
"
58,"Installed md5sum, but there is an error","I am using Ubuntu 22.04 LTS. I followed the instructions in the readme.md to run download.sh and selected all the models. The installation went smoothly at the beginning until it reached CodeLlama-34b/checklist.chk, where I encountered an md5sum error: md5sum: checklist.chk: No such file or directory, but when I checked with md5sum --version, it was indeed installed. Can you please tell me what steps I might have missed?",adiyoss,"Hi @tsaitaoguei,
Did you manage to download the 34B model eventually? "
58,"Installed md5sum, but there is an error","I am using Ubuntu 22.04 LTS. I followed the instructions in the readme.md to run download.sh and selected all the models. The installation went smoothly at the beginning until it reached CodeLlama-34b/checklist.chk, where I encountered an md5sum error: md5sum: checklist.chk: No such file or directory, but when I checked with md5sum --version, it was indeed installed. Can you please tell me what steps I might have missed?",itbaduc,"I have same problem when it show error when try download

> parseopts.c:76: setup_check: fopen 'checklist.chk': No such file or directory"
58,"Installed md5sum, but there is an error","I am using Ubuntu 22.04 LTS. I followed the instructions in the readme.md to run download.sh and selected all the models. The installation went smoothly at the beginning until it reached CodeLlama-34b/checklist.chk, where I encountered an md5sum error: md5sum: checklist.chk: No such file or directory, but when I checked with md5sum --version, it was indeed installed. Can you please tell me what steps I might have missed?",seandogg,"same issue
"
55,Can't run examples on Windows 10,"Hi,
I've tried to run the examples, but I received this error.

```
(CodeLlama) PS C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama> python -m torch.distributed.run --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b-Python --tokenizer_path ./CodeLlama-7b-Python/tokenizer.model
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 79, in <module>
    fire.Fire(main)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 18, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\llama\generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_pickle.UnpicklingError: invalid load key, '<'.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 18284) of binary: C:\ProgramData\anaconda3\envs\CodeLlama\python.exe
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 798, in <module>
    main()
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_infilling.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-28_12:39:51
  host      : DESKTOP-THP4I5R
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 18284)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs
```",mhamra,"### UPDATE
I've made a mistake running the download.sh script. I've passed my email instead of the URL received from FB."
55,Can't run examples on Windows 10,"Hi,
I've tried to run the examples, but I received this error.

```
(CodeLlama) PS C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama> python -m torch.distributed.run --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b-Python --tokenizer_path ./CodeLlama-7b-Python/tokenizer.model
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 79, in <module>
    fire.Fire(main)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 18, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\llama\generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_pickle.UnpicklingError: invalid load key, '<'.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 18284) of binary: C:\ProgramData\anaconda3\envs\CodeLlama\python.exe
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 798, in <module>
    main()
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_infilling.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-28_12:39:51
  host      : DESKTOP-THP4I5R
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 18284)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs
```",manoj21192,"Did your issue resolved? I am unable to run on windows 10 as well. I am getting ""Distributed package doesnt have NCCL built-in error"""
55,Can't run examples on Windows 10,"Hi,
I've tried to run the examples, but I received this error.

```
(CodeLlama) PS C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama> python -m torch.distributed.run --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b-Python --tokenizer_path ./CodeLlama-7b-Python/tokenizer.model
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 79, in <module>
    fire.Fire(main)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 18, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\llama\generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_pickle.UnpicklingError: invalid load key, '<'.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 18284) of binary: C:\ProgramData\anaconda3\envs\CodeLlama\python.exe
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 798, in <module>
    main()
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_infilling.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-28_12:39:51
  host      : DESKTOP-THP4I5R
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 18284)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs
```",realhaik,"> 

@manoj21192 This will work on windows

```

temperature  = 0
top_p  = 0
max_seq_len  = 4096
max_batch_size  = 1
max_gen_len  = None
num_of_worlds = 1

torch.distributed.init_process_group(backend='gloo', init_method='tcp://localhost:23455', world_size=num_of_worlds, rank=0)


generator = Llama.build(
    
    ckpt_dir=""C:/AI/LLaMA2_Docker_FileSystem/codellama/CodeLlama-7b-Instruct"",
    tokenizer_path=""C:/AI/LLaMA2_Docker_FileSystem/codellama/CodeLlama-7b-Instruct/tokenizer.model"",
    max_seq_len=max_seq_len,
    max_batch_size=max_batch_size,
    model_parallel_size = num_of_worlds
)
```"
55,Can't run examples on Windows 10,"Hi,
I've tried to run the examples, but I received this error.

```
(CodeLlama) PS C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama> python -m torch.distributed.run --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b-Python --tokenizer_path ./CodeLlama-7b-Python/tokenizer.model
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 79, in <module>
    fire.Fire(main)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 18, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\llama\generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_pickle.UnpicklingError: invalid load key, '<'.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 18284) of binary: C:\ProgramData\anaconda3\envs\CodeLlama\python.exe
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 798, in <module>
    main()
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_infilling.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-28_12:39:51
  host      : DESKTOP-THP4I5R
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 18284)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs
```",99991,"> ### UPDATE
> 
> I've made a mistake running the download.sh script. I've passed my email instead of the URL received from FB.

Thank you! I can reproduce this. I at first entered my email and then noticed my error and entered the correct URL when running `download.sh`, but loading was still not possible.

I cloned the repository again, entered the correct URL on first try and then it worked."
55,Can't run examples on Windows 10,"Hi,
I've tried to run the examples, but I received this error.

```
(CodeLlama) PS C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama> python -m torch.distributed.run --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b-Python --tokenizer_path ./CodeLlama-7b-Python/tokenizer.model
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 79, in <module>
    fire.Fire(main)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 18, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\llama\generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_pickle.UnpicklingError: invalid load key, '<'.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 18284) of binary: C:\ProgramData\anaconda3\envs\CodeLlama\python.exe
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 798, in <module>
    main()
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_infilling.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-28_12:39:51
  host      : DESKTOP-THP4I5R
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 18284)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs
```",bronzwikgk,"What mistake am  I making here? 
from typing import Optional

import fire

from llama import Llama


def main(
    ckpt_dir: ""D:\pathto\codellama\CodeLlama-7b"",
    tokenizer_path: ""D:\pathto\codellama\CodeLlama-7b\tokenizer.model"",
    temperature: float = 0.2,
    top_p: float = 0.9,
    max_seq_len: int = 256,
    max_batch_size: int = 4,
    max_gen_len: Optional[int] = None,
):
    generator = Llama.build(
        ckpt_dir=ckpt_dir,
        tokenizer_path=tokenizer_path,
        max_seq_len=max_seq_len,
        max_batch_size=max_batch_size,
    )
""

I Am getting this error: ""

D:\path2\codellama>python example_completion.py
ERROR: The function received no value for the required argument: ckpt_dir
Usage: example_completion.py CKPT_DIR TOKENIZER_PATH <flags>
  optional flags:        --temperature | --top_p | --max_seq_len |
                         --max_batch_size | --max_gen_len

For detailed information on this command, run:
  example_completion.py --help """
55,Can't run examples on Windows 10,"Hi,
I've tried to run the examples, but I received this error.

```
(CodeLlama) PS C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama> python -m torch.distributed.run --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b-Python --tokenizer_path ./CodeLlama-7b-Python/tokenizer.model
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 79, in <module>
    fire.Fire(main)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 18, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\llama\generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_pickle.UnpicklingError: invalid load key, '<'.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 18284) of binary: C:\ProgramData\anaconda3\envs\CodeLlama\python.exe
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 798, in <module>
    main()
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_infilling.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-28_12:39:51
  host      : DESKTOP-THP4I5R
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 18284)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs
```",realhaik,"> What mistake am I making here? from typing import Optional
> 
> import fire
> 
> from llama import Llama
> 
> def main( ckpt_dir: ""D:\pathto\codellama\CodeLlama-7b"", tokenizer_path: ""D:\pathto\codellama\CodeLlama-7b\tokenizer.model"", temperature: float = 0.2, top_p: float = 0.9, max_seq_len: int = 256, max_batch_size: int = 4, max_gen_len: Optional[int] = None, ): generator = Llama.build( ckpt_dir=ckpt_dir, tokenizer_path=tokenizer_path, max_seq_len=max_seq_len, max_batch_size=max_batch_size, ) ""
> 
> I Am getting this error: ""
> 
> D:\path2\codellama>python example_completion.py ERROR: The function received no value for the required argument: ckpt_dir Usage: example_completion.py CKPT_DIR TOKENIZER_PATH optional flags: --temperature | --top_p | --max_seq_len | --max_batch_size | --max_gen_len
> 
> For detailed information on this command, run: example_completion.py --help ""

@bronzwikgk 

Based on the code and error message you've provided, here are some issues I've identified:

1. The type hints in the function arguments are actually string literals, which is incorrect syntax for Python.
2. The paths should be properly escaped or defined as raw strings.

Here's a revised version of the code:

```python
from typing import Optional
import fire
from llama import Llama

def main(
    ckpt_dir: str = r""D:\pathto\codellama\CodeLlama-7b"",
    tokenizer_path: str = r""D:\pathto\codellama\CodeLlama-7b\tokenizer.model"",
    temperature: float = 0.2,
    top_p: float = 0.9,
    max_seq_len: int = 256,
    max_batch_size: int = 4,
    max_gen_len: Optional[int] = None,
):
    generator = Llama.build(
        ckpt_dir=ckpt_dir,
        tokenizer_path=tokenizer_path,
        max_seq_len=max_seq_len,
        max_batch_size=max_batch_size,
    )
    
if __name__ == ""__main__"":
    fire.Fire(main)
```

1. Fixed the type hints for `ckpt_dir` and `tokenizer_path` to be `str`.
2. Used raw string literals for the Windows paths (by prefixing the string with an `r`), which allow for backslashes to be interpreted correctly.
3. Added `if __name__ == ""__main__"": fire.Fire(main)` to run the function when the script is executed.

Try running the updated code and see if the error persists."
55,Can't run examples on Windows 10,"Hi,
I've tried to run the examples, but I received this error.

```
(CodeLlama) PS C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama> python -m torch.distributed.run --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b-Python --tokenizer_path ./CodeLlama-7b-Python/tokenizer.model
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 79, in <module>
    fire.Fire(main)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 18, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\llama\generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_pickle.UnpicklingError: invalid load key, '<'.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 18284) of binary: C:\ProgramData\anaconda3\envs\CodeLlama\python.exe
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 798, in <module>
    main()
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_infilling.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-28_12:39:51
  host      : DESKTOP-THP4I5R
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 18284)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs
```",bronzwikgk,"Thanks, Moved One step ahead.
Getting this error now: {{ 
Traceback (most recent call last):
  File ""D:\shunyadotek\codellama\example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""C:\Users\shunya-desk-01\AppData\Roaming\Python\Python311\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\shunya-desk-01\AppData\Roaming\Python\Python311\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\shunya-desk-01\AppData\Roaming\Python\Python311\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\shunyadotek\codellama\example_completion.py"", line 20, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""D:\shunyadotek\codellama\llama\generation.py"", line 68, in build
    torch.distributed.init_process_group(""nccl"")
  File ""C:\Users\shunya-desk-01\AppData\Roaming\Python\Python311\site-packages\torch\distributed\distributed_c10d.py"", line 900, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\shunya-desk-01\AppData\Roaming\Python\Python311\site-packages\torch\distributed\rendezvous.py"", line 235, in _env_rendezvous_handler
    rank = int(_get_env_or_raise(""RANK""))
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\shunya-desk-01\AppData\Roaming\Python\Python311\site-packages\torch\distributed\rendezvous.py"", line 220, in _get_env_or_raise
    raise _env_error(env_var)
ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set
}}"
55,Can't run examples on Windows 10,"Hi,
I've tried to run the examples, but I received this error.

```
(CodeLlama) PS C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama> python -m torch.distributed.run --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b-Python --tokenizer_path ./CodeLlama-7b-Python/tokenizer.model
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 79, in <module>
    fire.Fire(main)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 18, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\llama\generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_pickle.UnpicklingError: invalid load key, '<'.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 18284) of binary: C:\ProgramData\anaconda3\envs\CodeLlama\python.exe
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 798, in <module>
    main()
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_infilling.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-28_12:39:51
  host      : DESKTOP-THP4I5R
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 18284)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs
```",realhaik,"> torch.distributed.init_process_group(backend='gloo', init_method='tcp://localhost:23455', world_size=num_of_worlds, rank=0)

@bronzwikgk 
I don't see this line in your code : torch.distributed.init_process_group(backend='gloo', init_method='tcp://localhost:23455', world_size=num_of_worlds, rank=0)

Are you sure you have it in your code?
See my answer with the full code with this line, few answers above."
55,Can't run examples on Windows 10,"Hi,
I've tried to run the examples, but I received this error.

```
(CodeLlama) PS C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama> python -m torch.distributed.run --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b-Python --tokenizer_path ./CodeLlama-7b-Python/tokenizer.model
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 79, in <module>
    fire.Fire(main)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 18, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\llama\generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_pickle.UnpicklingError: invalid load key, '<'.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 18284) of binary: C:\ProgramData\anaconda3\envs\CodeLlama\python.exe
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 798, in <module>
    main()
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_infilling.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-28_12:39:51
  host      : DESKTOP-THP4I5R
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 18284)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs
```",realhaik,"@bronzwikgk Right, I see that you are using torch.distributed.init_process_group(""nccl"")
nccl is for linux only, use my example above."
54,how to use this model? Is it same as hf version?,"How to use this model? Is it same as hf version?

From README, I know I must use torchrun to run example, is it possible to run by python? how to write an example that can be run by python not torchrun? Recently all my searched results are to use hf version, is it possible to use original download model?",bronzwikgk,Am wondering and juggling around the same question.
53,Programming Languages Support,Is there any chart or breakdown of % programming languages used in training data for the base/instruct models?,syhw,"There is no exhaustive list, and you should try your favorite language. There should at least be those languages: ""assembly"", ""batchfile"", ""c++"", ""c"", ""c-sharp"", ""cmake"", ""css"", ""dockerfile"", ""fortran"", ""go"", ""haskell"", ""html"", ""java"",
""javascript"", ""julia"", ""lua"", ""makefile"", ""markdown"", ""perl"", ""php"", ""powershell"", ""python"", ""ruby"", ""rust"", 
""scala"", ""shell"", ""sql"", ""tex"", ""visual-basic""."
52,enhanced the download.sh script for better usability,Enhancement on file: download.sh #51,facebook-github-bot,"Hi @rjarun8! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%2352). Thanks!"
52,enhanced the download.sh script for better usability,Enhancement on file: download.sh #51,rjarun8,"CLA Signed

```
Thank you!
Thank you for taking the time to register as an open source contributor. You may print the submission below for your records. You can resubmit this form to update your name or email. If you would like to make any other changes, please email [cla@meta.com](mailto:cla@meta.com) with your details.

We will now be able to accept any pull requests that you make to projects on our [GitHub account](https://github.com/facebook). If you already submitted a pull request before signing the CLA, then we will add a confirmation comment on that pull request (this might take several minutes). If you haven't submitted a pull request yet, then we look forward to your future contributions!
```"
52,enhanced the download.sh script for better usability,Enhancement on file: download.sh #51,facebook-github-bot,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!
50,Can't run any inference,"I'm trying to use the exemple inference on windows 10 with python 10, like that:

`(py310) d:\git\codellama>torchrun --nproc_per_node 1 example_instructions.py --ckpt_dir CodeLlama-7b-Instruct/ --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model --max_seq_len 512 --max_batch_size 4`

But it seems trying to connect to something in docker that I'm not using. That may be related to NCCL even if I only have one gpu... I don't understand.
This is the output I get:

```
NOTE: Redirects are currently not supported in Windows or MacOs.
[W ..\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - LÆadresse demandÚe nÆest pas valide dans son contexte.).
[W ..\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - LÆadresse demandÚe nÆest pas valide dans son contexte.).
[W ..\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - LÆadresse demandÚe nÆest pas valide dans son contexte.).
[W ..\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - LÆadresse demandÚe nÆest pas valide dans son contexte.).
Traceback (most recent call last):
  File ""d:\git\codellama\example_instructions.py"", line 68, in <module>
    fire.Fire(main)
  File ""D:\anaconda3\envs\py310\lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""D:\anaconda3\envs\py310\lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""D:\anaconda3\envs\py310\lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""d:\git\codellama\example_instructions.py"", line 20, in main
    generator = Llama.build(
  File ""d:\git\codellama\llama\generation.py"", line 68, in build
    torch.distributed.init_process_group(""nccl"")
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\distributed_c10d.py"", line 907, in init_process_group
    default_pg = _new_process_group_helper(
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\distributed_c10d.py"", line 1013, in _new_process_group_helper
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 13132) of binary: D:\anaconda3\envs\py310\python.exe
Traceback (most recent call last):
  File ""D:\anaconda3\envs\py310\lib\runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""D:\anaconda3\envs\py310\lib\runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""D:\anaconda3\envs\py310\Scripts\torchrun.exe\__main__.py"", line 7, in <module>
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_instructions.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-28_09:51:06
  host      : DESKTOP-123456
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 13132)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",loleg,"I'm getting this as well, on Linux. Would be nice to have a `USE_NCCL=0` or such environmental variable. One workaround is to swap it out and replace with GLOO. See example in my branch here: https://github.com/loleg/codellama/blob/without-nccl/llama/generation.py#L68 (not working yet, throwing `ChildFailedError` after processing for a while)

See also #40 "
50,Can't run any inference,"I'm trying to use the exemple inference on windows 10 with python 10, like that:

`(py310) d:\git\codellama>torchrun --nproc_per_node 1 example_instructions.py --ckpt_dir CodeLlama-7b-Instruct/ --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model --max_seq_len 512 --max_batch_size 4`

But it seems trying to connect to something in docker that I'm not using. That may be related to NCCL even if I only have one gpu... I don't understand.
This is the output I get:

```
NOTE: Redirects are currently not supported in Windows or MacOs.
[W ..\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - LÆadresse demandÚe nÆest pas valide dans son contexte.).
[W ..\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - LÆadresse demandÚe nÆest pas valide dans son contexte.).
[W ..\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - LÆadresse demandÚe nÆest pas valide dans son contexte.).
[W ..\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - LÆadresse demandÚe nÆest pas valide dans son contexte.).
Traceback (most recent call last):
  File ""d:\git\codellama\example_instructions.py"", line 68, in <module>
    fire.Fire(main)
  File ""D:\anaconda3\envs\py310\lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""D:\anaconda3\envs\py310\lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""D:\anaconda3\envs\py310\lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""d:\git\codellama\example_instructions.py"", line 20, in main
    generator = Llama.build(
  File ""d:\git\codellama\llama\generation.py"", line 68, in build
    torch.distributed.init_process_group(""nccl"")
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\distributed_c10d.py"", line 907, in init_process_group
    default_pg = _new_process_group_helper(
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\distributed_c10d.py"", line 1013, in _new_process_group_helper
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 13132) of binary: D:\anaconda3\envs\py310\python.exe
Traceback (most recent call last):
  File ""D:\anaconda3\envs\py310\lib\runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""D:\anaconda3\envs\py310\lib\runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""D:\anaconda3\envs\py310\Scripts\torchrun.exe\__main__.py"", line 7, in <module>
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_instructions.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-28_09:51:06
  host      : DESKTOP-123456
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 13132)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",Entretoize,"That works, thank you !"
49,Update README.md,minor fix,jspisak,thanks for the fix!
48,How can i generate embeddings from the model for a new source code dataset ?,,jgehring,"Hi @ilyesrezgui, our models are all causal language models and hence don't readily provide a means to compute embeddings from sentences or documents. Obtaining such embeddings from causal LMs is an area of active research; see [this paper](https://arxiv.org/abs/2202.08904) as a starting point, for example."
47,Add code to chat continuously with simple command.,"```sh
% make chat
torchrun --nproc_per_node 1 chat.py \
    --ckpt_dir CodeLlama-7b-Instruct/ \
    --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \
    --max_seq_len 512 --max_batch_size 4
NOTE: Redirects are currently not supported in Windows or MacOs.
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Loaded in 31.36 seconds
Enter a role (default is 'user'): 
Enter a prompt: What is the result of 2+2?
User: What is the result of 2+2?

> Assistant:  The result of 2+2 is 4.

==================================

Enter a role (default is 'user'):      
Enter a prompt: Where is the capital city of Canada?              
User: Where is the capital city of Canada?

> Assistant:  The capital city of Canada is Ottawa.

==================================

Enter a role (default is 'user'): 

```
I thought It will be much easy to simply ask and test models :-)",godpeny,https://github.com/facebookresearch/llama/pull/728#issuecomment-1703807505
46,use 'md5' instead of 'md5sum' if Applie Silicon,For Apple Silicon mac users' convenience.,facebook-github-bot,"Hi @godpeny! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%2346). Thanks!"
46,use 'md5' instead of 'md5sum' if Applie Silicon,For Apple Silicon mac users' convenience.,godpeny,"https://github.com/facebookresearch/llama/pull/727
same issue resolved here :-)"
46,use 'md5' instead of 'md5sum' if Applie Silicon,For Apple Silicon mac users' convenience.,mpu,"Hi,

Beware that the fix linked on the llama repo is actually incorrect, it's not going to check that the files downloaded have the correct checksum. Additionally, the proposed fix may break Linux users.

The simplest way to get the download script running is to install `md5sum` on your mac; both homebrew and macports have a md5sha1sum package you can use:
- `brew install md5sha1sum`
- or `sudo port install md5sha1sum`
"
45,Update tokenizer.py,"With this approach, we can cache the IDs of special tokens in a dictionary called special_token_ids. During initialization, these IDs are looked up and stored for quick access. The get_special_token_id method allows you to retrieve the ID of a special token from the cache using its token string. This can help avoid redundant dictionary lookups when encoding or decoding special tokens",gautierdag,"Hi @mzamini92,
Thanks for your PR! However this functionality of caching `special_token_ids` can be managed by end users and so we will not include it in the official implementation."
44,"When running `bash download.sh` in my windows laptop, this happens","```command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do```",kalyniy,"You can try to install wsl (windows subsystem for linux) linux box and clone repository and run download.sh in there, and then just copy paste the files over. That's what I did yesterday, but I can't run torchrun command afterwards. I wasted my entire day yesterday. I installed ubuntu 22 and I never ran torchrun command successfully. "
44,"When running `bash download.sh` in my windows laptop, this happens","```command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do```",capitanalegria,"The easiest way Ive been able to download all lama models in windows 10 is make sure you have wget and md5sums working on your windows machine, meaning the .exes are in the correct folders, md5sums.exe in the Windows folder and wget.exe in System32, and then double clicking on the download.bash ...make sure to download everything from secure places."
44,"When running `bash download.sh` in my windows laptop, this happens","```command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do```",shengbozeng,"Hi,do you run the shell script on the Windows OS? The script is based on Linux,but you are not perplex for this if you have git installed.

You can open the project folder by resources manager, select the folder and right click it,you can find ""git bash here"" item in menu, click it and you will get git terminal console.

And now try to run the download.sh in this terminal again.

"
44,"When running `bash download.sh` in my windows laptop, this happens","```command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do```",loleg,"I'm surprised the OP is even able to run `download.sh` without WSL.
Another good option is something like [Git for Windows](https://gitforwindows.org/).
This probably needs a README update, rather than code change."
44,"When running `bash download.sh` in my windows laptop, this happens","```command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do```",notTanveer,"> Hi,do you run the shell script on the Windows OS? The script is based on Linux,but you are not perplex for this if you have git installed.
> 
> You can open the project folder by resources manager, select the folder and right click it,you can find ""git bash here"" item in menu, click it and you will get git terminal console.
> 
> And now try to run the download.sh in this terminal again.

Thanks a lot, it worked. But now it shows that the wget and md5sums not found how do i fix this error. You are right I am running the script on Windows. If i need to change my system, guide me on how to do it. OR just tell me the process how you have done it or refer some yt. Thanks again.🙌"
44,"When running `bash download.sh` in my windows laptop, this happens","```command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do```",loleg,"[Chocolatey](https://chocolatey.org/) is a good option for installing various utilities on Windows.

But as you will also need various Data Science components here, I suggest you follow the [PyTorch guide](https://pytorch.org/get-started/locally/#windows-anaconda) to get started on Windows with [Anaconda](https://anaconda.org/)."
44,"When running `bash download.sh` in my windows laptop, this happens","```command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do```",colorincode,"I used conda (or mini conda it that's your vibe). I used anaconda/anaconda navigator. 
 
1) maybe not required but helped: Had to do a fresh Ubuntu install to avoid WSL interference. 
2) from anaconda interface click *cme.exe install* or just navigate to directory of your anaconda install, you'll need to use terminal/bash from there.
3) You *must* run through conda activate and conda init for conda to run bash, without modifying path vars. This enabled me to call bash scripts from the anaconda CMD prompt. "
44,"When running `bash download.sh` in my windows laptop, this happens","```command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do```",shengbozeng,"> > Hi,do you run the shell script on the Windows OS? The script is based on Linux,but you are not perplex for this if you have git installed.
> 
> > 
> 
> > You can open the project folder by resources manager, select the folder and right click it,you can find ""git bash here"" item in menu, click it and you will get git terminal console.
> 
> > 
> 
> > And now try to run the download.sh in this terminal again.
> 
> 
> 
> Thanks a lot, it worked. But now it shows that the wget and md5sums not found how do i fix this error. You are right I am running the script on Windows. If i need to change my system, guide me on how to do it. OR just tell me the process how you have done it or refer some yt. Thanks again.🙌

Wget and md5sums are commands in Linux, and they should be installed in git terminal.

If you can't found them,I think that must be you did not select some options when you installed git.

Sometimes, reinstalling Git for Windows with additional options checked (like “Use Unix tools from the Windows Command Prompt”) might include more utilities.

It's maybe fix the issue.
"
44,"When running `bash download.sh` in my windows laptop, this happens","```command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do```",mhamra,"I used the download.sh script in WSL2, and after that, I moved the files to another location. The command `explorer.exe .` in the wsl2 directory opens the Windows Explorer application and allows copy/paste operations on files. I've moved all the downloaded folders inside the `codellama` directory in Windows, where I've cloned the repository to run the examples.

Is that ok? I have problems running the examples in Windows 10 (see this issue #55)"
44,"When running `bash download.sh` in my windows laptop, this happens","```command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do```",notTanveer,"> > > Hi,do you run the shell script on the Windows OS? The script is based on Linux,but you are not perplex for this if you have git installed.
> > 
> > 
> > > 
> > 
> > 
> > > You can open the project folder by resources manager, select the folder and right click it,you can find ""git bash here"" item in menu, click it and you will get git terminal console.
> > 
> > 
> > > 
> > 
> > 
> > > And now try to run the download.sh in this terminal again.
> > 
> > 
> > Thanks a lot, it worked. But now it shows that the wget and md5sums not found how do i fix this error. You are right I am running the script on Windows. If i need to change my system, guide me on how to do it. OR just tell me the process how you have done it or refer some yt. Thanks again.🙌
> 
> Wget and md5sums are commands in Linux, and they should be installed in git terminal.
> 
> If you can't found them,I think that must be you did not select some options when you installed git.
> 
> Sometimes, reinstalling Git for Windows with additional options checked (like “Use Unix tools from the Windows Command Prompt”) might include more utilities.
> 
> It's maybe fix the issue.

I tried that process but it doesnt seem to be working. If you guys can tell me the step by step process on how you have done it will help me a lot.(Windows user)
"
44,"When running `bash download.sh` in my windows laptop, this happens","```command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do```",shengbozeng,"> > > > Hi,do you run the shell script on the Windows OS? The script is based on Linux,but you are not perplex for this if you have git installed.
> > > 
> > > 
> > > > 
> > > 
> > > 
> > > > You can open the project folder by resources manager, select the folder and right click it,you can find ""git bash here"" item in menu, click it and you will get git terminal console.
> > > 
> > > 
> > > > 
> > > 
> > > 
> > > > And now try to run the download.sh in this terminal again.
> > > 
> > > 
> > > Thanks a lot, it worked. But now it shows that the wget and md5sums not found how do i fix this error. You are right I am running the script on Windows. If i need to change my system, guide me on how to do it. OR just tell me the process how you have done it or refer some yt. Thanks again.🙌
> > 
> > 
> > Wget and md5sums are commands in Linux, and they should be installed in git terminal.
> > If you can't found them,I think that must be you did not select some options when you installed git.
> > Sometimes, reinstalling Git for Windows with additional options checked (like “Use Unix tools from the Windows Command Prompt”) might include more utilities.
> > It's maybe fix the issue.
> 
> I tried that process but it doesnt seem to be working. If you guys can tell me the step by step process on how you have done it will help me a lot.(Windows user)

step1: please visit https://git-scm.com/download/win , and download correct version for windows;
step2: run the setup file of git, and follow tips;
step3:
![image](https://github.com/facebookresearch/codellama/assets/53858417/8174b83c-6c2f-4732-800b-d72a46e10399)

step4: click 'next' button and setup;
step5: done."
44,"When running `bash download.sh` in my windows laptop, this happens","```command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do```",notTanveer,"Help me with this, from where do i install wget and md5sum
"
44,"When running `bash download.sh` in my windows laptop, this happens","```command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do```",monstorbak,"I struggled with this as well. I successfully ran download.sh on a Windows 11 laptop using git for Windows, then I could not replicate the process on another PC. The fix for me was to reinstall git for Windows on the second PC for the **64-bit version of git** as opposed to the 32-bit version that was previously running (and failing)."
43,issue with downloading certain files from the Llama 2 ,"Hello all,
I am currently facing an issue with downloading certain files from the Llama 2 repository. I ran the provided download.sh script to fetch the necessary files but encountered errors that prevented the successful download of the following files:

params.json
tokenizer.model
checklist.chk
I have followed the documentation and also made sure that the URL provided in the email was correctly entered. The script runs without issues for other files but fails specifically for these. I have sufficient disk space and my internet connection is stable.

Could you please assist me in resolving this issue? Is there an alternate way to download these specific files or should I perform some additional troubleshooting steps?

Thank you for your time and assistance.",samuelselvan,Could you please try requesting one more time (the urls would have expired by now) and try downloading and share the logs?
43,issue with downloading certain files from the Llama 2 ,"Hello all,
I am currently facing an issue with downloading certain files from the Llama 2 repository. I ran the provided download.sh script to fetch the necessary files but encountered errors that prevented the successful download of the following files:

params.json
tokenizer.model
checklist.chk
I have followed the documentation and also made sure that the URL provided in the email was correctly entered. The script runs without issues for other files but fails specifically for these. I have sufficient disk space and my internet connection is stable.

Could you please assist me in resolving this issue? Is there an alternate way to download these specific files or should I perform some additional troubleshooting steps?

Thank you for your time and assistance.",cmosguy,I've reported this also in https://github.com/facebookresearch/codellama/issues/113.  @Renfeld - did you ever figure out what the root cause to this was or how to fix this?
42,NOTE: Redirects are currently not supported in Windows or MacOs.,"Hi guys,

I am trying **CodeLlama-13b-Python** model in local MacOS 13.4.1 (c) M2, I can make sure I install all packages they need in `requirement.txt`. 

I want to make sure is it a setting up issue or we have to run this model on Linux, just want to make sure ",spmhickstein,"[https://github.com/facebookresearch/codellama/issues/32](url) same issue, I haven't tested yet, but sounds like a reasonable solution for Windows right now"
42,NOTE: Redirects are currently not supported in Windows or MacOs.,"Hi guys,

I am trying **CodeLlama-13b-Python** model in local MacOS 13.4.1 (c) M2, I can make sure I install all packages they need in `requirement.txt`. 

I want to make sure is it a setting up issue or we have to run this model on Linux, just want to make sure ",JadeCopet,Duplicate of #32. 
41,Update generation.py,"Isn't it better to first sort the probabilities and the indices together, then compute the cumulative sum of the probabilities? The mask is then created by setting all the elements in the cumulative sum that are greater than or equal to p to 1. The sorted probabilities are then set to 0 where the mask is 1. Finally, the indices where the mask is 0 are selected, and the index of the maximum value in this set is returned.",facebook-github-bot,"Hi @mzamini92! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%2341). Thanks!"
41,Update generation.py,"Isn't it better to first sort the probabilities and the indices together, then compute the cumulative sum of the probabilities? The mask is then created by setting all the elements in the cumulative sum that are greater than or equal to p to 1. The sorted probabilities are then set to 0 where the mask is 1. Finally, the indices where the mask is 0 are selected, and the index of the maximum value in this set is returned.",facebook-github-bot,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!
41,Update generation.py,"Isn't it better to first sort the probabilities and the indices together, then compute the cumulative sum of the probabilities? The mask is then created by setting all the elements in the cumulative sum that are greater than or equal to p to 1. The sorted probabilities are then set to 0 where the mask is 1. Finally, the indices where the mask is 0 are selected, and the index of the maximum value in this set is returned.",GaganHonor,well said sir 
41,Update generation.py,"Isn't it better to first sort the probabilities and the indices together, then compute the cumulative sum of the probabilities? The mask is then created by setting all the elements in the cumulative sum that are greater than or equal to p to 1. The sorted probabilities are then set to 0 where the mask is 1. Finally, the indices where the mask is 0 are selected, and the index of the maximum value in this set is returned.",gautierdag,"Hi @mzamini92 

Thanks for you PR, however your changes don't give me the same results as the reference implementation. I invite you to try it with a random set of tensors and compare your solution to the reference implementation. 

Since the original library implementation works we will not merge this PR. 

Feel free to re-open this issue if you have further questions on this."
40,Unable to run it under Windows 10,"I followed the instructions, and I was unable to run it under Windows 10 due to `nccl` ",GaganHonor,"or i guess yiu should use VPC , no need to run on your system just go to AWS and use it :)
still this may help 👇
```bash
import os
import torch
os.environ['PL_TORCH_DISTRIBUTED_BACKEND'] = 'gloo'
os.environ['NCCL_DEBUG'] = 'INFO'
torch.distributed.init_process_group(backend=""gloo"")
```"
40,Unable to run it under Windows 10,"I followed the instructions, and I was unable to run it under Windows 10 due to `nccl` ",eranif,"I am not sure that I understand your comment about AWS...

I am actually trying to make it work to see whether I can integrate it into CodeLite IDE... (As a plugin that generates code based on comment in the editor)

Also, I already tried that solution, still does not work

Eran"
40,Unable to run it under Windows 10,"I followed the instructions, and I was unable to run it under Windows 10 due to `nccl` ",GaganHonor,"Apologies for any confusion caused by my previous comment mentioning [AWS](https://www.google.com/search?q=AWS). I misunderstood your intention to integrate [Code Llama](https://www.google.com/search?q=Code%20Llama) into [CodeLite IDE](https://www.google.com/search?q=CodeLite%20IDE). I appreciate your clarification.

Regarding your goal of integrating [Code Llama](https://www.google.com/search?q=Code%20Llama) as a plugin in [CodeLite IDE](https://www.google.com/search?q=CodeLite%20IDE) to generate code based on comments in the editor, it sounds like an interesting project. However, if you have already tried the suggested solution and it did not work, let's explore alternative approaches to address the issue.

Here are a few suggestions:

Consult the [CodeLite](https://www.google.com/search?q=CodeLite) community: Reach out to the [CodeLite](https://www.google.com/search?q=CodeLite) community or forums for assistance in integrating [Code Llama](https://www.google.com/search?q=Code%20Llama) as a plugin. They may have specific insights, examples, or documentation on how to achieve this integration successfully. You can ask for guidance on the best practices or any known issues related to plugin development in [CodeLite IDE](https://www.google.com/search?q=CodeLite%20IDE).

Check for compatibility: Ensure that [Code Llama](https://www.google.com/search?q=Code%20Llama) is compatible with the version of [CodeLite IDE](https://www.google.com/search?q=CodeLite%20IDE) you are using. Verify if there are any specific requirements or considerations for integrating plugins in [CodeLite IDE](https://www.google.com/search?q=CodeLite%20IDE) and ensure that [Code Llama](https://www.google.com/search?q=Code%20Llama) meets those requirements.

Review the plugin development documentation: Look for documentation or tutorials related to developing plugins for [CodeLite IDE](https://www.google.com/search?q=CodeLite%20IDE). Check if there are any guidelines or examples that can help you understand the process better. Pay attention to any specific steps or configurations required for integrating external tools like [Code Llama](https://www.google.com/search?q=Code%20Llama).

Engage with the [CodeLite](https://www.google.com/search?q=CodeLite) community: Consider engaging with the [CodeLite](https://www.google.com/search?q=CodeLite) community directly. You can share your specific integration goal and ask for assistance or guidance from experienced users or the project maintainers. They might be able to provide valuable insights or suggestions based on their expertise and experience.

Remember to provide detailed information about any error messages or specific issues you encounter during the integration process. This will assist others in understanding the problem better and offering more targeted solutions.

I hope these suggestions help you in your journey to integrate [Code Llama](https://www.google.com/search?q=Code%20Llama) into [CodeLite IDE](https://www.google.com/search?q=CodeLite%20IDE). If you have any further questions or need additional assistance, please feel free to ask. Good luck with your project!



source : gitlab community 

For more help try here buddy ✅
https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/


And share in more detail if possible ❇️

and in last i found this that may be helpful for you ,  https://huggingface.co/blog/codellama#conversational-instructions , great post available. Thanks for reading "
40,Unable to run it under Windows 10,"I followed the instructions, and I was unable to run it under Windows 10 due to `nccl` ",eranif,"Thanks for the tips.
I have pretty much the idea in my head and I think its achievable. Regarding [CodeLite IDE](https://codelite.org): I don't need to consult CodeLite's docs, since I wrote CodeLite and I also wrote the docs ;) 

Eran
"
40,Unable to run it under Windows 10,"I followed the instructions, and I was unable to run it under Windows 10 due to `nccl` ",GaganHonor,"Sir @eranif 🙏 I apologize for the inconvenience while I have not that much knowledge and taken your time , and I am very happy to know that I got a chance to chat with creator of #CodeLITE 🌟

Gagan"
39,# Added Images instruction in README.me 🖼️,"Added 2 Images for more clear understanding ( I myself Faced some issues so added ) .
Added a `media` directory for saving project instruction files or images.

Please let me know If any modifications needed or I should improve it more and also add clear easy to understand instructions for other steps for saving time :) 

Thanks in advance to Project team for reading this 🙏
#docs-improvement ❇️
Thanks again Dear mam/sir ",facebook-github-bot,"Hi @GaganHonor! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%2339). Thanks!"
39,# Added Images instruction in README.me 🖼️,"Added 2 Images for more clear understanding ( I myself Faced some issues so added ) .
Added a `media` directory for saving project instruction files or images.

Please let me know If any modifications needed or I should improve it more and also add clear easy to understand instructions for other steps for saving time :) 

Thanks in advance to Project team for reading this 🙏
#docs-improvement ❇️
Thanks again Dear mam/sir ",facebook-github-bot,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!
39,# Added Images instruction in README.me 🖼️,"Added 2 Images for more clear understanding ( I myself Faced some issues so added ) .
Added a `media` directory for saving project instruction files or images.

Please let me know If any modifications needed or I should improve it more and also add clear easy to understand instructions for other steps for saving time :) 

Thanks in advance to Project team for reading this 🙏
#docs-improvement ❇️
Thanks again Dear mam/sir ",JadeCopet,Sorry this goes beyond the scope of instructions we want to have in the readme. 
38,Greedy decoding of CodeLlama,"Hi, thanks for the great work! From the interface it seems there is not an option like `do_sample=False` to enable deterministic greedy decoding. I am curious if there will be support or how to add that by ourselves. Thanks!
",ganler,"https://github.com/facebookresearch/codellama/blob/main/llama/generation.py#L160

Setting temperature to zero works here. "
37,Getting Issue in installation :(,"Error :


```bash
 root@Indra:/home/gagan/projecta/codellama-gagan-singh# bash download.sh
url:https://download2.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoidHg5ejl1ZHJqN2NkMHN1anNoa3dlaWx3IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTMxMETCCCCCCCCCCCCCCCCCCC&Download-Request-ID=8211ABCDDEMO

Enter the list of models to download without spaces (7b,13b,34b,7b-Python,13b-Python,34b-Python,7b-Instruct,13b-Instruct,34b-Instruct), or press Enter for all:
Downloading LICENSE and Acceptable Usage Policy
wget: missing URL
Usage: wget [OPTION]... [URL]...

Try `wget --help' for more options.
wget: missing URL

```

Whats the issue ? in  download.sh i pasted the link as 

```py
read -p ""url:https://download2.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTMxMDcwNjl9fX1dfQ__&Signature=Bw-dDvS72Gak1eb0GqeJKU5iD887fN022O6uBGfpvlF9eiMPfx7jDNu1kSre2gbEwHi%7EW15B8Ns8-%7E2fuJqa9t9QEhthKA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=821102959DEMOO"" PRESIGNED_URL
echo """"
ALL_MODELS=""7b,13b,34b,7b-Python,13b-Python,34b-Python,7b-Instruct,13b-Instruct,34b-Instruct""
read -p ""Enter the list of models to download without spaces ($ALL_MODELS), or press Enter for all: "" MODEL_SIZE
TARGET_FOLDER="".""             # where all files should end up
mkdir -p ${TARGET_FOLDER}
```

I have changed token to random string , Please help",GaganHonor,"Ok issue was very minor , The issue was i was pasting the link in download.sh 💀

So The easy step is
****
 fork the repo 
git clone repo by the link example 
```code
git clone https://github.com/GaganHonor/codellama-gagan-singh
```
```bash
cd codellama-gagan-singh
```

then type 

```
bash download.sh

``` 

and on first prompt enter url and on 2nd promt choose your model , Each size upto 2.5 Gb


 ****
"
36,Create devcontainer.json,"{
  ""image"": ""mcr.microsoft.com/devcontainers/universal:2"",
  ""features"": {
  }
}",facebook-github-bot,"Hi @Dmellies! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%2336). Thanks!"
36,Create devcontainer.json,"{
  ""image"": ""mcr.microsoft.com/devcontainers/universal:2"",
  ""features"": {
  }
}",syhw,"Hi, sorry but we do not accept new features of the sort, otherwise everybody would add their own environment and the repository would quickly become messy. We want to keep this as simple an example to get started with the Code Llama models as possible. We advise you to fork the repo if you want to develop further on it."
32,NOTE: Redirects are currently not supported in Windows or MacOs.,"I can't run the examples in Windows machine, currently blocked in this attempts to redirect.

We should see information about requirements in the documentation.",GaganHonor,"You can use WSL feature Version 2 and install Debian or Ubuntu then Update and Upgrade your all packages .

Open [PowerShell]  as an administrator.
Run the following command:
 ```code
         wsl --install
          This command will install the latest WSL to install [Debian]

           wsl --set-version Debian `2`
```

I guess some issues on win 11 so follow this if above not works : 



Open powershell as an administrator.
Run the following command to enable the required features:
```code
dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart
   dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart

```
After running these commands, restart your computer.


Once your computer restarts, open Powershell as an administrator again.


Run the following command to set [WSL 2]  as the default version:

```code
wsl --set-default-version 2
```
Finally, try installing the Linux distribution again by running the following command:
```code
wsl --install
```
I hope it helps ❤️

"
31,missing key,"when the link  https://download.llamameta.net/ is opened in the browser I get the below error.


![image](https://github.com/facebookresearch/codellama/assets/34311842/fc04c415-ab20-46d9-8c95-0929b5c96eaf)
",Kallamamran,You shouldn't open it in the browser. Read the instructions again!
30,What's the machine requirements for each model?,"I want to know what's the minimum requirement memory/CPU/GPU for each model to run relatively fast. I ran in my M1

```
torchrun --nproc_per_node 1 example_completion.py \
    --ckpt_dir CodeLlama-7b/ \
    --tokenizer_path CodeLlama-7b/tokenizer.model \
    --max_seq_len 128 --max_batch_size 4
NOTE: Redirects are currently not supported in Windows or MacOs.
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Loaded in 182.59 seconds
```

and it's taking more than 5 minutes.",GaganHonor,"Well ,the 7B model can be served on a single GPU, while the 34B model returns the best results and allows for better coding assistance, but the smaller 7B and 13B models are faster and more suitable for tasks that require low latency, like real-time code completion.  for a 7B parameter model, you need about 14GB of RAM to run it in float16 precision. and 🧑‍💻It’s important to note that running CodeLlama on an M1 Mac may not be optimal as it is not officially supported. The M1 chip is based on ARM architecture, which is different from the x86 architecture used by most GPUs and CPUs. This can result in compatibility issues and slower performance when running software that is not optimized for ARM 🙏 Thanks a lot for reading"
30,What's the machine requirements for each model?,"I want to know what's the minimum requirement memory/CPU/GPU for each model to run relatively fast. I ran in my M1

```
torchrun --nproc_per_node 1 example_completion.py \
    --ckpt_dir CodeLlama-7b/ \
    --tokenizer_path CodeLlama-7b/tokenizer.model \
    --max_seq_len 128 --max_batch_size 4
NOTE: Redirects are currently not supported in Windows or MacOs.
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Loaded in 182.59 seconds
```

and it's taking more than 5 minutes.",Hangsiin,"I could run 7B  model on google Colab environment with T4 GPU (Free GPU). 
7B model is pretty light to use.
But it takes many system RAM when loading so colab pro access is required for more RAM.
It took times about 3~4 minutes for loading.
But Inference was quiet fast!
I was surprised to find that I could run LLM with a single T4 GPU."
30,What's the machine requirements for each model?,"I want to know what's the minimum requirement memory/CPU/GPU for each model to run relatively fast. I ran in my M1

```
torchrun --nproc_per_node 1 example_completion.py \
    --ckpt_dir CodeLlama-7b/ \
    --tokenizer_path CodeLlama-7b/tokenizer.model \
    --max_seq_len 128 --max_batch_size 4
NOTE: Redirects are currently not supported in Windows or MacOs.
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Loaded in 182.59 seconds
```

and it's taking more than 5 minutes.",GaganHonor,"Yes , I tested 7B model on AWS EC2 Instance t2.micro , t3a.micro , and it worked quite well there are 
many more free GPU also there please check it out , and for Trying out the model i find https://huggingface.co/blog/codellama#conversational-instructions very useful 🙏"
30,What's the machine requirements for each model?,"I want to know what's the minimum requirement memory/CPU/GPU for each model to run relatively fast. I ran in my M1

```
torchrun --nproc_per_node 1 example_completion.py \
    --ckpt_dir CodeLlama-7b/ \
    --tokenizer_path CodeLlama-7b/tokenizer.model \
    --max_seq_len 128 --max_batch_size 4
NOTE: Redirects are currently not supported in Windows or MacOs.
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Loaded in 182.59 seconds
```

and it's taking more than 5 minutes.",MattOates,"> It’s important to note that running CodeLlama on an M1 Mac may not be optimal as it is not officially supported. The M1 chip is based on ARM architecture, which is different from the x86 architecture used by most GPUs and CPUs. This can result in compatibility issues and slower performance when running software that is not optimized for ARM

The model is just data, with llama.cpp the models run at realtime speeds with Metal acceleration on M1/2. Thanks to unified memory of the platform if you have 32GB of RAM that's all available to the GPU. Its a dream architecture for running these models, why would you put anyone off? My laptop on battery power can run 13b llama no trouble.

"
29,Give an explicit example of the instruction prompt structure in the readme,"Currently the readme is pointing newcomers to generation.py, where they have to deduce the correct prompt structure for the instruction model from this code:

```
B_INST, E_INST = ""[INST]"", ""[/INST]""
B_SYS, E_SYS = ""<<SYS>>\n"", ""\n<</SYS>>\n\n""

[...]

for dialog in dialogs:
    unsafe_requests.append(
        any([tag in msg[""content""] for tag in SPECIAL_TAGS for msg in dialog])
    )
    if dialog[0][""role""] == ""system"":
        dialog = [
            {
                ""role"": dialog[1][""role""],
                ""content"": B_SYS
                + dialog[0][""content""]
                + E_SYS
                + dialog[1][""content""],
            }
        ] + dialog[2:]
    assert all([msg[""role""] == ""user"" for msg in dialog[::2]]) and all(
        [msg[""role""] == ""assistant"" for msg in dialog[1::2]]
    ), (
        ""model only supports 'system', 'user' and 'assistant' roles, ""
        ""starting with 'system', then 'user' and alternating (u/a/u/a/u...)""
    )
    dialog_tokens: List[int] = sum(
        [
            self.tokenizer.encode(
                f""{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} "",
                bos=True,
                eos=True,
            )
            for prompt, answer in zip(
                dialog[::2],
                dialog[1::2],
            )
        ],
        [],
    )
    assert (
        dialog[-1][""role""] == ""user""
    ), f""Last message must be from user, got {dialog[-1]['role']}""
    dialog_tokens += self.tokenizer.encode(
        f""{B_INST} {(dialog[-1]['content']).strip()} {E_INST}"",
        bos=True,
        eos=False,
    )
    prompt_tokens.append(dialog_tokens)
```

This seems unnecessarily obscure. Is there a specific reason to not just give an example?",alphastrata,https://huggingface.co/blog/codellama#conversational-instructions
29,Give an explicit example of the instruction prompt structure in the readme,"Currently the readme is pointing newcomers to generation.py, where they have to deduce the correct prompt structure for the instruction model from this code:

```
B_INST, E_INST = ""[INST]"", ""[/INST]""
B_SYS, E_SYS = ""<<SYS>>\n"", ""\n<</SYS>>\n\n""

[...]

for dialog in dialogs:
    unsafe_requests.append(
        any([tag in msg[""content""] for tag in SPECIAL_TAGS for msg in dialog])
    )
    if dialog[0][""role""] == ""system"":
        dialog = [
            {
                ""role"": dialog[1][""role""],
                ""content"": B_SYS
                + dialog[0][""content""]
                + E_SYS
                + dialog[1][""content""],
            }
        ] + dialog[2:]
    assert all([msg[""role""] == ""user"" for msg in dialog[::2]]) and all(
        [msg[""role""] == ""assistant"" for msg in dialog[1::2]]
    ), (
        ""model only supports 'system', 'user' and 'assistant' roles, ""
        ""starting with 'system', then 'user' and alternating (u/a/u/a/u...)""
    )
    dialog_tokens: List[int] = sum(
        [
            self.tokenizer.encode(
                f""{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} "",
                bos=True,
                eos=True,
            )
            for prompt, answer in zip(
                dialog[::2],
                dialog[1::2],
            )
        ],
        [],
    )
    assert (
        dialog[-1][""role""] == ""user""
    ), f""Last message must be from user, got {dialog[-1]['role']}""
    dialog_tokens += self.tokenizer.encode(
        f""{B_INST} {(dialog[-1]['content']).strip()} {E_INST}"",
        bos=True,
        eos=False,
    )
    prompt_tokens.append(dialog_tokens)
```

This seems unnecessarily obscure. Is there a specific reason to not just give an example?",marco-ve,"I know the structure. I was pointing out that the readme is not linking to it for no apparent reason.
"
29,Give an explicit example of the instruction prompt structure in the readme,"Currently the readme is pointing newcomers to generation.py, where they have to deduce the correct prompt structure for the instruction model from this code:

```
B_INST, E_INST = ""[INST]"", ""[/INST]""
B_SYS, E_SYS = ""<<SYS>>\n"", ""\n<</SYS>>\n\n""

[...]

for dialog in dialogs:
    unsafe_requests.append(
        any([tag in msg[""content""] for tag in SPECIAL_TAGS for msg in dialog])
    )
    if dialog[0][""role""] == ""system"":
        dialog = [
            {
                ""role"": dialog[1][""role""],
                ""content"": B_SYS
                + dialog[0][""content""]
                + E_SYS
                + dialog[1][""content""],
            }
        ] + dialog[2:]
    assert all([msg[""role""] == ""user"" for msg in dialog[::2]]) and all(
        [msg[""role""] == ""assistant"" for msg in dialog[1::2]]
    ), (
        ""model only supports 'system', 'user' and 'assistant' roles, ""
        ""starting with 'system', then 'user' and alternating (u/a/u/a/u...)""
    )
    dialog_tokens: List[int] = sum(
        [
            self.tokenizer.encode(
                f""{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} "",
                bos=True,
                eos=True,
            )
            for prompt, answer in zip(
                dialog[::2],
                dialog[1::2],
            )
        ],
        [],
    )
    assert (
        dialog[-1][""role""] == ""user""
    ), f""Last message must be from user, got {dialog[-1]['role']}""
    dialog_tokens += self.tokenizer.encode(
        f""{B_INST} {(dialog[-1]['content']).strip()} {E_INST}"",
        bos=True,
        eos=False,
    )
    prompt_tokens.append(dialog_tokens)
```

This seems unnecessarily obscure. Is there a specific reason to not just give an example?",alphastrata,"> I know the structure. I was pointing out that the readme is not linking to it for no apparent reason.

'fe
I've noticed a growing trend in the mega corps _only_ keeping their HF doccos up to date lately, perhaps it's by design?
"
29,Give an explicit example of the instruction prompt structure in the readme,"Currently the readme is pointing newcomers to generation.py, where they have to deduce the correct prompt structure for the instruction model from this code:

```
B_INST, E_INST = ""[INST]"", ""[/INST]""
B_SYS, E_SYS = ""<<SYS>>\n"", ""\n<</SYS>>\n\n""

[...]

for dialog in dialogs:
    unsafe_requests.append(
        any([tag in msg[""content""] for tag in SPECIAL_TAGS for msg in dialog])
    )
    if dialog[0][""role""] == ""system"":
        dialog = [
            {
                ""role"": dialog[1][""role""],
                ""content"": B_SYS
                + dialog[0][""content""]
                + E_SYS
                + dialog[1][""content""],
            }
        ] + dialog[2:]
    assert all([msg[""role""] == ""user"" for msg in dialog[::2]]) and all(
        [msg[""role""] == ""assistant"" for msg in dialog[1::2]]
    ), (
        ""model only supports 'system', 'user' and 'assistant' roles, ""
        ""starting with 'system', then 'user' and alternating (u/a/u/a/u...)""
    )
    dialog_tokens: List[int] = sum(
        [
            self.tokenizer.encode(
                f""{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} "",
                bos=True,
                eos=True,
            )
            for prompt, answer in zip(
                dialog[::2],
                dialog[1::2],
            )
        ],
        [],
    )
    assert (
        dialog[-1][""role""] == ""user""
    ), f""Last message must be from user, got {dialog[-1]['role']}""
    dialog_tokens += self.tokenizer.encode(
        f""{B_INST} {(dialog[-1]['content']).strip()} {E_INST}"",
        bos=True,
        eos=False,
    )
    prompt_tokens.append(dialog_tokens)
```

This seems unnecessarily obscure. Is there a specific reason to not just give an example?",zjysteven,"I second that. Although I do understand that providing examples is of course at low priority for such big project, pointing to a clear, intuitive example or prompt template could be super helpful (e.g., I like how https://github.com/Stability-AI/StableLM/tree/main#quickstart is doing this)."
28,Issue with downloading models,"Hi all,

I am actually struggling with downloading models. When I paste the link when prompted and after tapping 'enter' to download all models I receive the following:

download.sh: 13: [[: not found
Downloading LICENSE and Acceptable Usage Policy
download.sh: 18: Bad substitution

Tried twice with different links, after removing the cloned repo from HDD. ",jldefigueiredo,same issue here!
28,Issue with downloading models,"Hi all,

I am actually struggling with downloading models. When I paste the link when prompted and after tapping 'enter' to download all models I receive the following:

download.sh: 13: [[: not found
Downloading LICENSE and Acceptable Usage Policy
download.sh: 18: Bad substitution

Tried twice with different links, after removing the cloned repo from HDD. ",mkennedym80,"I'm having the same issue. Same errors. I tried editing the shell script to just use the download link they gave me and still isn't working.

correction, the redirect link they sent via email that you're supposed to paste into the prompt, i tried just assigning that to a variable and skipping entering the value and it's still problematic."
28,Issue with downloading models,"Hi all,

I am actually struggling with downloading models. When I paste the link when prompted and after tapping 'enter' to download all models I receive the following:

download.sh: 13: [[: not found
Downloading LICENSE and Acceptable Usage Policy
download.sh: 18: Bad substitution

Tried twice with different links, after removing the cloned repo from HDD. ",mkennedym80,"ok this is dumb, as i was using sh download.sh

but use '$ bash download.sh' and it'll download."
28,Issue with downloading models,"Hi all,

I am actually struggling with downloading models. When I paste the link when prompted and after tapping 'enter' to download all models I receive the following:

download.sh: 13: [[: not found
Downloading LICENSE and Acceptable Usage Policy
download.sh: 18: Bad substitution

Tried twice with different links, after removing the cloned repo from HDD. ",jldefigueiredo,"It's true. Worked for me too. Thanks. Regards.

El vie, 25 de ago de 2023, 12:40, Michael Ryan ***@***.***>
escribió:

> ok this is dumb, as i was using sh download.sh
>
> but use '$ bash download.sh' and it'll download.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/facebookresearch/codellama/issues/28#issuecomment-1693558685>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AP7JNMJDUODIS6KENXH57Z3XXDBPDANCNFSM6AAAAAA36NRZQQ>
> .
> You are receiving this because you commented.Message ID:
> ***@***.***>
>
"
28,Issue with downloading models,"Hi all,

I am actually struggling with downloading models. When I paste the link when prompted and after tapping 'enter' to download all models I receive the following:

download.sh: 13: [[: not found
Downloading LICENSE and Acceptable Usage Policy
download.sh: 18: Bad substitution

Tried twice with different links, after removing the cloned repo from HDD. ",Kallamamran,"I use bash download.sh and it doesn't work for me :(
![image](https://github.com/facebookresearch/codellama/assets/25354644/01070df7-0c37-4cd6-ac37-c5f2ff73f707)
"
28,Issue with downloading models,"Hi all,

I am actually struggling with downloading models. When I paste the link when prompted and after tapping 'enter' to download all models I receive the following:

download.sh: 13: [[: not found
Downloading LICENSE and Acceptable Usage Policy
download.sh: 18: Bad substitution

Tried twice with different links, after removing the cloned repo from HDD. ",jldefigueiredo,"I didnt tried in windows. I used debian

El vie, 25 de ago de 2023, 18:33, Kallamamran ***@***.***>
escribió:

> I use bash download.sh and it doesn't work for me :(
> [image: image]
> <https://user-images.githubusercontent.com/25354644/263396474-01070df7-0c37-4cd6-ac37-c5f2ff73f707.png>
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/facebookresearch/codellama/issues/28#issuecomment-1693958600>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AP7JNMM3FCHVRSZTVSSRNO3XXEKY3ANCNFSM6AAAAAA36NRZQQ>
> .
> You are receiving this because you commented.Message ID:
> ***@***.***>
>
"
28,Issue with downloading models,"Hi all,

I am actually struggling with downloading models. When I paste the link when prompted and after tapping 'enter' to download all models I receive the following:

download.sh: 13: [[: not found
Downloading LICENSE and Acceptable Usage Policy
download.sh: 18: Bad substitution

Tried twice with different links, after removing the cloned repo from HDD. ",Kallamamran,Well... It sure doesn't work downloading on my Windows machine 😥
28,Issue with downloading models,"Hi all,

I am actually struggling with downloading models. When I paste the link when prompted and after tapping 'enter' to download all models I receive the following:

download.sh: 13: [[: not found
Downloading LICENSE and Acceptable Usage Policy
download.sh: 18: Bad substitution

Tried twice with different links, after removing the cloned repo from HDD. ",jldefigueiredo,"Email said that url works only for 24hs or a certain amount of downloads

El vie, 25 de ago de 2023, 18:45, Kallamamran ***@***.***>
escribió:

> Well... It sure doesn't work downloading on my Windows machine 😥
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/facebookresearch/codellama/issues/28#issuecomment-1693968309>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AP7JNMKY63XABESDRMB6YZLXXEMJLANCNFSM6AAAAAA36NRZQQ>
> .
> You are receiving this because you commented.Message ID:
> ***@***.***>
>
"
28,Issue with downloading models,"Hi all,

I am actually struggling with downloading models. When I paste the link when prompted and after tapping 'enter' to download all models I receive the following:

download.sh: 13: [[: not found
Downloading LICENSE and Acceptable Usage Policy
download.sh: 18: Bad substitution

Tried twice with different links, after removing the cloned repo from HDD. ",Kallamamran,Solution for me: Downloading the models from Huggingface - TheBloke in OobaboogaWebUI instead
28,Issue with downloading models,"Hi all,

I am actually struggling with downloading models. When I paste the link when prompted and after tapping 'enter' to download all models I receive the following:

download.sh: 13: [[: not found
Downloading LICENSE and Acceptable Usage Policy
download.sh: 18: Bad substitution

Tried twice with different links, after removing the cloned repo from HDD. ",GaganHonor,"The error messages download.sh: 13: [[: not found and download.sh: 18: Bad substitution suggest that there might be an issue with the shell interpreter being used to run the download.sh script. The [[: not found error message usually occurs when a script that uses Bash-specific syntax is run with a different shell interpreter, such as Dash1. The Bad substitution error message can also occur when a script that uses Bash-specific syntax is run with a different shell interpreter2. To fix this issue, you can try running the ``download.sh`` script with the Bash interpreter by using the command bash download.sh instead of ``sh download.sh``


For more help :   [ Guide by huggingface 🤗 ](https://huggingface.co/blog/codellama#conversational-instructions)

Cheers for @Kallamamran @mkennedym80 They already found 

![ScalerCreateImpactGIF](https://github.com/facebookresearch/codellama/assets/141204441/d6000faf-7c13-43c2-97ca-03d027316e9c)

"
28,Issue with downloading models,"Hi all,

I am actually struggling with downloading models. When I paste the link when prompted and after tapping 'enter' to download all models I receive the following:

download.sh: 13: [[: not found
Downloading LICENSE and Acceptable Usage Policy
download.sh: 18: Bad substitution

Tried twice with different links, after removing the cloned repo from HDD. ",bikekowal,"> I'm having the same issue. Same errors. I tried editing the shell script to just use the download link they gave me and still isn't working.
> 
> correction, the redirect link they sent via email that you're supposed to paste into the prompt, i tried just assigning that to a variable and skipping entering the value and it's still problematic.

Works well! Thanks!"
28,Issue with downloading models,"Hi all,

I am actually struggling with downloading models. When I paste the link when prompted and after tapping 'enter' to download all models I receive the following:

download.sh: 13: [[: not found
Downloading LICENSE and Acceptable Usage Policy
download.sh: 18: Bad substitution

Tried twice with different links, after removing the cloned repo from HDD. ",mike-cipherpunk,"> Solution for me: Downloading the models from Huggingface - TheBloke in OobaboogaWebUI instead

glad you figured it out"
27,Run 13B or 34B in a single GPU,"How to load 13B  or 34B in a single A100, I notice the model parallel size larger than 1 need more gpus",GaganHonor,"[It is possible to run the 13B model on a single A100 GPU, which has sufficient VRAM](https://github.com/facebookresearch/llama/issues/186)[1](https://github.com/facebookresearch/llama/issues/186). However, it can be challenging to figure out how to get it working. [One user reported being able to run the 30B model on an A100 GPU using a specific setup](https://github.com/facebookresearch/llama/issues/186)[1](https://github.com/facebookresearch/llama/issues/186). [Another user reported being able to run the LLaMA-65B model on a single A100 80GB with 8-bit quantization](https://github.com/facebookresearch/llama/issues/101)[2](https://github.com/facebookresearch/llama/issues/101). You may want to refer to these examples and try similar approaches to see if they work for you. 

![NickYoungQuestionMarkGIF](https://github.com/facebookresearch/codellama/assets/141204441/c0ffc064-6e62-487e-a2c3-7a63b824dd80)
"
27,Run 13B or 34B in a single GPU,"How to load 13B  or 34B in a single A100, I notice the model parallel size larger than 1 need more gpus",manoj21192,"> How to load 13B or 34B in a single A100, I notice the model parallel size larger than 1 need more gpus

Hey, did you figure out how to load 13B model on single GPU in windows? I am also struggling with same issue."
26,Model pads response with newlines up to max_length,"I tried several of the models through Huggingface, and the response is always padded with newlines up to the number of tokens specified by the max_length argument in model.generate(). 

I also assign pad_token_id=tokenizer.eos_token_id, so I'm not sure why the model is generating these newline characters.",borzunov,"I observe the same thing with [codellama/CodeLlama-34b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf) on Hugging Face Hub. Quite often, the model starts to generate `\n` indefinitely instead of generating `</s>` and stopping.

This is using the standard generation params (`temperature=0.2, top_p=0.95`) with the prompt format and the example prompt suggested in this repository:

```
<s>[INST] <<SYS>>
A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
<</SYS>>

In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last month? [/INST]
```"
26,Model pads response with newlines up to max_length,"I tried several of the models through Huggingface, and the response is always padded with newlines up to the number of tokens specified by the max_length argument in model.generate(). 

I also assign pad_token_id=tokenizer.eos_token_id, so I'm not sure why the model is generating these newline characters.",Regenhardt,"This one made less newlines, but still way too many than needed:

`You're an intelligent, concise coding assistant. Wrap code in ``` for readability. Don't repeat yourself. Use best practice and good coding standards.`

(Using temp 0.8)"
26,Model pads response with newlines up to max_length,"I tried several of the models through Huggingface, and the response is always padded with newlines up to the number of tokens specified by the max_length argument in model.generate(). 

I also assign pad_token_id=tokenizer.eos_token_id, so I'm not sure why the model is generating these newline characters.",liruiw,same problem here. I am basically using llama recipe's quickstart for training and inference. Inference using the same prompt in this repo will work just fine.
26,Model pads response with newlines up to max_length,"I tried several of the models through Huggingface, and the response is always padded with newlines up to the number of tokens specified by the max_length argument in model.generate(). 

I also assign pad_token_id=tokenizer.eos_token_id, so I'm not sure why the model is generating these newline characters.",ArthurZucker,Could you make sure you are using the latest release / main version of transformers? 
26,Model pads response with newlines up to max_length,"I tried several of the models through Huggingface, and the response is always padded with newlines up to the number of tokens specified by the max_length argument in model.generate(). 

I also assign pad_token_id=tokenizer.eos_token_id, so I'm not sure why the model is generating these newline characters.",Regenhardt,I'm actually using LlamaSharp (https://github.com/SciSharp/LLamaSharp) with the ggml model downloaded from TheBloke
26,Model pads response with newlines up to max_length,"I tried several of the models through Huggingface, and the response is always padded with newlines up to the number of tokens specified by the max_length argument in model.generate(). 

I also assign pad_token_id=tokenizer.eos_token_id, so I'm not sure why the model is generating these newline characters.",zaventh,This issue still occurs with revision `4cb1403c377bb630ab92ec56272a6686c2bff315`  of `codellama/CodeLlama-13b-Instruct-hf` running on TGI. 
26,Model pads response with newlines up to max_length,"I tried several of the models through Huggingface, and the response is always padded with newlines up to the number of tokens specified by the max_length argument in model.generate(). 

I also assign pad_token_id=tokenizer.eos_token_id, so I'm not sure why the model is generating these newline characters.",liruiw,"Actually this minimal example works fine on my computer now. A few references [1](https://huggingface.co/blog/codellama#transformers), [2](https://huggingface.co/spaces/codellama/codellama-playground), [3](https://huggingface.co/spaces/codellama/codellama-13b-chat/blob/main/model.py#L25).
```
from transformers import AutoTokenizer, LlamaForCausalLM
import transformers
import torch

model_id = ""codellama/CodeLlama-7b-Instruct-hf""
tokenizer = AutoTokenizer.from_pretrained(model_id)

pipeline = transformers.pipeline(
    ""text-generation"",
    model=""codellama/CodeLlama-7b-hf"",
    torch_dtype=torch.float16,
    device_map=""auto"",
)

sequences = pipeline(
   'Write the code for quicksort.',
    do_sample=True,
    temperature=0.1,
    top_p=0.9,
    num_return_sequences=1,
    top_k=50,
    max_length=1024,
    eos_token_id=tokenizer.eos_token_id 
)
for seq in sequences:
    print(f""Result: {seq['generated_text']}"")
```"
26,Model pads response with newlines up to max_length,"I tried several of the models through Huggingface, and the response is always padded with newlines up to the number of tokens specified by the max_length argument in model.generate(). 

I also assign pad_token_id=tokenizer.eos_token_id, so I'm not sure why the model is generating these newline characters.",marco-ve,"After upgrading to the latest version of transformers (4.32.1) and huggingface_hub (0.16.4) with pip this gives me

`ValueError: Tokenizer class CodeLlamaTokenizer does not exist or is not currently imported.
`
"
26,Model pads response with newlines up to max_length,"I tried several of the models through Huggingface, and the response is always padded with newlines up to the number of tokens specified by the max_length argument in model.generate(). 

I also assign pad_token_id=tokenizer.eos_token_id, so I'm not sure why the model is generating these newline characters.",aliswel-mt,"Instead of act as padding, I faced all the output is new line
Not all the input act like this, but many of them just output a bunch of new line
ps. I am using latest version of transformers (4.32.1)"
26,Model pads response with newlines up to max_length,"I tried several of the models through Huggingface, and the response is always padded with newlines up to the number of tokens specified by the max_length argument in model.generate(). 

I also assign pad_token_id=tokenizer.eos_token_id, so I'm not sure why the model is generating these newline characters.",ArthurZucker,cc @marco-ve you should install main using `pip install git+https://github.com/huggingface/transformers`
26,Model pads response with newlines up to max_length,"I tried several of the models through Huggingface, and the response is always padded with newlines up to the number of tokens specified by the max_length argument in model.generate(). 

I also assign pad_token_id=tokenizer.eos_token_id, so I'm not sure why the model is generating these newline characters.",strokesegment,Has anyone solved the problem of generating a large number of line breaks?
26,Model pads response with newlines up to max_length,"I tried several of the models through Huggingface, and the response is always padded with newlines up to the number of tokens specified by the max_length argument in model.generate(). 

I also assign pad_token_id=tokenizer.eos_token_id, so I'm not sure why the model is generating these newline characters.",ArthurZucker,You are most probably not using `main`. 
26,Model pads response with newlines up to max_length,"I tried several of the models through Huggingface, and the response is always padded with newlines up to the number of tokens specified by the max_length argument in model.generate(). 

I also assign pad_token_id=tokenizer.eos_token_id, so I'm not sure why the model is generating these newline characters.",sootlasten,"@ArthurZucker is correct. The model repeating a token ad infinitum is the results of the `rope_theta` param not being read in correctly from the model params. This requires transformers version >= 4.33.0 (see also this thread: https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v1-GPTQ/discussions/2).

Closing this issue."
25,torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ,"https://colab.research.google.com/drive/1rXJyXXO4m-nP4XDoLV7h4_Iea3jYHDZY#scrollTo=xjpOpjPDAVQX
In google colab, When I execute fine-tuning it throws the error",sanardicat,"same issue. I can run the infilling file, but not the prompt example. "
25,torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ,"https://colab.research.google.com/drive/1rXJyXXO4m-nP4XDoLV7h4_Iea3jYHDZY#scrollTo=xjpOpjPDAVQX
In google colab, When I execute fine-tuning it throws the error",codingjlu,"Same here, I was watching the CPU RAM and that peaked to max, so that's probably why the program terminated."
25,torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ,"https://colab.research.google.com/drive/1rXJyXXO4m-nP4XDoLV7h4_Iea3jYHDZY#scrollTo=xjpOpjPDAVQX
In google colab, When I execute fine-tuning it throws the error",Levyhhh,same issue. 
25,torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ,"https://colab.research.google.com/drive/1rXJyXXO4m-nP4XDoLV7h4_Iea3jYHDZY#scrollTo=xjpOpjPDAVQX
In google colab, When I execute fine-tuning it throws the error",faabian,"Yes, this appears to be an out-of-memory error. You can try lowering the batch size and sequence length until it fits. Please feel free to re-open if the problem turns out not to be an out-of-memory error."
24,how to finetune it locally,,faabian,"Please understand that we do not provide fine-tuning code in this repository. But you may want to take a look at this repository which applies to Code Llama just as it applies to Llama 2 and uses the HuggingFace transformers library: https://github.com/facebookresearch/llama-recipes/
Generally, all local finetuning methods developed in the last months for Llama also work with Code Llama, the main difference being the different value of `rope_theta` (see https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py for details)."
23,Fix script names in README,,facebook-github-bot,"Hi @zachschillaci27! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%2323). Thanks!"
22,SSL error when downloading,"Hi, 

The download script throws the following error:

```
Resolving download2.llamameta.net (download2.llamameta.net)... ::ffff:130.226.237.92, 130.226.237.92
Connecting to download2.llamameta.net (download2.llamameta.net)|::ffff:130.226.237.92|:443... connected.
OpenSSL: error:0A000152:SSL routines::unsafe legacy renegotiation disabled
Unable to establish SSL connection.
Checking checksums
md5sum: checklist.chk: no properly formatted MD5 checksum lines found
```


If I set the `Options = UnsafeLegacyRenegotiation` in SSL conf then it throws the error mentioned in #8 . Thanks for the help!",az-pz,The download links expire after 24 hours. You may need to request the link again.
22,SSL error when downloading,"Hi, 

The download script throws the following error:

```
Resolving download2.llamameta.net (download2.llamameta.net)... ::ffff:130.226.237.92, 130.226.237.92
Connecting to download2.llamameta.net (download2.llamameta.net)|::ffff:130.226.237.92|:443... connected.
OpenSSL: error:0A000152:SSL routines::unsafe legacy renegotiation disabled
Unable to establish SSL connection.
Checking checksums
md5sum: checklist.chk: no properly formatted MD5 checksum lines found
```


If I set the `Options = UnsafeLegacyRenegotiation` in SSL conf then it throws the error mentioned in #8 . Thanks for the help!",adiyoss,"Hi @ankitkariryaa , 
@az-pz is correct the download links expire after 24 hours. Please request again the link."
21,How to deploy this model?,,Akanshu9990,I want steps to deploy it after downloading
21,How to deploy this model?,,syhw,"You should try 
```
torchrun --nproc_per_node 1 example_completion.py \
    --ckpt_dir CodeLlama-7b/ \
    --tokenizer_path CodeLlama-7b/tokenizer.model \
    --max_seq_len 128 --max_batch_size 4
```"
20,Requesting a Colab file to run code llama,"Hey, 
Thank you very much for such a great work, it would be great if anyone can create a colab file where we can run this model.
In different regions we are not able to download the model weights and run it.
Thanking You in advance. : )",Mraghuvaran,https://colab.research.google.com/drive/1lyEj1SRw0B9I2UUI2HOrtiJ_fjvbXtA2?usp=sharing#scrollTo=eK1C84v0AyrX
20,Requesting a Colab file to run code llama,"Hey, 
Thank you very much for such a great work, it would be great if anyone can create a colab file where we can run this model.
In different regions we are not able to download the model weights and run it.
Thanking You in advance. : )",faabian,"thank you @Mraghuvaran
@Shivansh-Srivastava-techolution you can also take a look at the `README.md` for more examples, or at the implementation by HuggingFace: https://huggingface.co/blog/codellama#how-to-use-code-llama

closing this as solved."
19,Inference on multi-gpu,"Tried to run:
```
torchrun --nproc_per_node 1 codellama/example_instructions.py \
     --ckpt_dir /home/ubuntu/model/ \
     --tokenizer_path /home/ubuntu/model/tokenizer.model \
     --max_seq_len 4512 --max_batch_size 4

```
I have a long prompt (4000 tokens).
I have 4 Nvidia A10G each with 300W and 24GB VRAM. However I see only one GPU being used (on nvidia-smi).
The error I get is:
```
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary:
```
The whole tracelog is:
```
 torchrun --nproc_per_node 1 codellama/example_instructions.py \
>     --ckpt_dir /home/ubuntu/model/ \
>     --tokenizer_path /home/ubuntu/model/tokenizer.model \
>     --max_seq_len 4512 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1

Loaded in 7.07 seconds
Traceback (most recent call last):
  File ""/home/ubuntu/codellama/example_instructions.py"", line 114, in <module>
    fire.Fire(main)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/ubuntu/codellama/example_instructions.py"", line 97, in main
    results = generator.chat_completion(
  File ""/home/ubuntu/codellama/llama/generation.py"", line 335, in chat_completion
    generation_tokens, generation_logprobs = self.generate(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/generation.py"", line 148, in generate
    logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 288, in forward
    h = layer(h, start_pos, freqs_cis, mask)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 240, in forward
    h = x + self.attention.forward(
  File ""/home/ubuntu/codellama/llama/model.py"", line 181, in forward
    scores = F.softmax(scores.float(), dim=-1).type_as(xq)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary: /home/ubuntu/venv/bin/python
Traceback (most recent call last):
  File ""/home/ubuntu/venv/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
codellama/example_instructions.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-25_04:41:10
  host      : ip-172-31-92-135.ec2.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 33763)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",itaowei,"Same issue.
I also want to inference on multi-GPU like using Llama-2 (hf version)"
19,Inference on multi-gpu,"Tried to run:
```
torchrun --nproc_per_node 1 codellama/example_instructions.py \
     --ckpt_dir /home/ubuntu/model/ \
     --tokenizer_path /home/ubuntu/model/tokenizer.model \
     --max_seq_len 4512 --max_batch_size 4

```
I have a long prompt (4000 tokens).
I have 4 Nvidia A10G each with 300W and 24GB VRAM. However I see only one GPU being used (on nvidia-smi).
The error I get is:
```
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary:
```
The whole tracelog is:
```
 torchrun --nproc_per_node 1 codellama/example_instructions.py \
>     --ckpt_dir /home/ubuntu/model/ \
>     --tokenizer_path /home/ubuntu/model/tokenizer.model \
>     --max_seq_len 4512 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1

Loaded in 7.07 seconds
Traceback (most recent call last):
  File ""/home/ubuntu/codellama/example_instructions.py"", line 114, in <module>
    fire.Fire(main)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/ubuntu/codellama/example_instructions.py"", line 97, in main
    results = generator.chat_completion(
  File ""/home/ubuntu/codellama/llama/generation.py"", line 335, in chat_completion
    generation_tokens, generation_logprobs = self.generate(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/generation.py"", line 148, in generate
    logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 288, in forward
    h = layer(h, start_pos, freqs_cis, mask)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 240, in forward
    h = x + self.attention.forward(
  File ""/home/ubuntu/codellama/llama/model.py"", line 181, in forward
    scores = F.softmax(scores.float(), dim=-1).type_as(xq)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary: /home/ubuntu/venv/bin/python
Traceback (most recent call last):
  File ""/home/ubuntu/venv/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
codellama/example_instructions.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-25_04:41:10
  host      : ip-172-31-92-135.ec2.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 33763)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",asolano,"Is the model at `/home/ubuntu/model/` the 7B, 13B or 34B version? You may need to adjust the `--nproc_per_node` parameter to 1, 2 and 4 respectively. 

(It is stated [here](https://github.com/facebookresearch/codellama#inference) in case you missed it.)"
19,Inference on multi-gpu,"Tried to run:
```
torchrun --nproc_per_node 1 codellama/example_instructions.py \
     --ckpt_dir /home/ubuntu/model/ \
     --tokenizer_path /home/ubuntu/model/tokenizer.model \
     --max_seq_len 4512 --max_batch_size 4

```
I have a long prompt (4000 tokens).
I have 4 Nvidia A10G each with 300W and 24GB VRAM. However I see only one GPU being used (on nvidia-smi).
The error I get is:
```
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary:
```
The whole tracelog is:
```
 torchrun --nproc_per_node 1 codellama/example_instructions.py \
>     --ckpt_dir /home/ubuntu/model/ \
>     --tokenizer_path /home/ubuntu/model/tokenizer.model \
>     --max_seq_len 4512 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1

Loaded in 7.07 seconds
Traceback (most recent call last):
  File ""/home/ubuntu/codellama/example_instructions.py"", line 114, in <module>
    fire.Fire(main)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/ubuntu/codellama/example_instructions.py"", line 97, in main
    results = generator.chat_completion(
  File ""/home/ubuntu/codellama/llama/generation.py"", line 335, in chat_completion
    generation_tokens, generation_logprobs = self.generate(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/generation.py"", line 148, in generate
    logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 288, in forward
    h = layer(h, start_pos, freqs_cis, mask)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 240, in forward
    h = x + self.attention.forward(
  File ""/home/ubuntu/codellama/llama/model.py"", line 181, in forward
    scores = F.softmax(scores.float(), dim=-1).type_as(xq)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary: /home/ubuntu/venv/bin/python
Traceback (most recent call last):
  File ""/home/ubuntu/venv/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
codellama/example_instructions.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-25_04:41:10
  host      : ip-172-31-92-135.ec2.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 33763)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",itaowei,"> Is the model at `/home/ubuntu/model/` the 7B, 13B or 34B version? You may need to adjust the `--nproc_per_node` parameter to 1, 2 and 4 respectively.
> 
> (It is stated [here](https://github.com/facebookresearch/codellama#inference) in case you missed it.)

Thanks for your reminder.
Actually I noticed that point but I still have the question: how to run 13B model on 8 or more GPUs? I tried to adjust the `--nproc_per_node` parameter to 8 when running the 13B model but it failed."
19,Inference on multi-gpu,"Tried to run:
```
torchrun --nproc_per_node 1 codellama/example_instructions.py \
     --ckpt_dir /home/ubuntu/model/ \
     --tokenizer_path /home/ubuntu/model/tokenizer.model \
     --max_seq_len 4512 --max_batch_size 4

```
I have a long prompt (4000 tokens).
I have 4 Nvidia A10G each with 300W and 24GB VRAM. However I see only one GPU being used (on nvidia-smi).
The error I get is:
```
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary:
```
The whole tracelog is:
```
 torchrun --nproc_per_node 1 codellama/example_instructions.py \
>     --ckpt_dir /home/ubuntu/model/ \
>     --tokenizer_path /home/ubuntu/model/tokenizer.model \
>     --max_seq_len 4512 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1

Loaded in 7.07 seconds
Traceback (most recent call last):
  File ""/home/ubuntu/codellama/example_instructions.py"", line 114, in <module>
    fire.Fire(main)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/ubuntu/codellama/example_instructions.py"", line 97, in main
    results = generator.chat_completion(
  File ""/home/ubuntu/codellama/llama/generation.py"", line 335, in chat_completion
    generation_tokens, generation_logprobs = self.generate(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/generation.py"", line 148, in generate
    logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 288, in forward
    h = layer(h, start_pos, freqs_cis, mask)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 240, in forward
    h = x + self.attention.forward(
  File ""/home/ubuntu/codellama/llama/model.py"", line 181, in forward
    scores = F.softmax(scores.float(), dim=-1).type_as(xq)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary: /home/ubuntu/venv/bin/python
Traceback (most recent call last):
  File ""/home/ubuntu/venv/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
codellama/example_instructions.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-25_04:41:10
  host      : ip-172-31-92-135.ec2.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 33763)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",asolano,"Good to hear!

IIRC it is not a quick fix to change the model parallel configuration, as the code expects the exact name and number of layers indicated in the model files, but if all you want to do is run inference with the 13B model in a 8 GPU system maybe you could launch 4 processes, each taking 2 GPUs (using something like CUDA_VISIBLE_DEVICES to assign them) and splitting the inputs into 4 chunks of (almost) equal size? The throughput should be similar."
19,Inference on multi-gpu,"Tried to run:
```
torchrun --nproc_per_node 1 codellama/example_instructions.py \
     --ckpt_dir /home/ubuntu/model/ \
     --tokenizer_path /home/ubuntu/model/tokenizer.model \
     --max_seq_len 4512 --max_batch_size 4

```
I have a long prompt (4000 tokens).
I have 4 Nvidia A10G each with 300W and 24GB VRAM. However I see only one GPU being used (on nvidia-smi).
The error I get is:
```
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary:
```
The whole tracelog is:
```
 torchrun --nproc_per_node 1 codellama/example_instructions.py \
>     --ckpt_dir /home/ubuntu/model/ \
>     --tokenizer_path /home/ubuntu/model/tokenizer.model \
>     --max_seq_len 4512 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1

Loaded in 7.07 seconds
Traceback (most recent call last):
  File ""/home/ubuntu/codellama/example_instructions.py"", line 114, in <module>
    fire.Fire(main)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/ubuntu/codellama/example_instructions.py"", line 97, in main
    results = generator.chat_completion(
  File ""/home/ubuntu/codellama/llama/generation.py"", line 335, in chat_completion
    generation_tokens, generation_logprobs = self.generate(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/generation.py"", line 148, in generate
    logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 288, in forward
    h = layer(h, start_pos, freqs_cis, mask)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 240, in forward
    h = x + self.attention.forward(
  File ""/home/ubuntu/codellama/llama/model.py"", line 181, in forward
    scores = F.softmax(scores.float(), dim=-1).type_as(xq)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary: /home/ubuntu/venv/bin/python
Traceback (most recent call last):
  File ""/home/ubuntu/venv/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
codellama/example_instructions.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-25_04:41:10
  host      : ip-172-31-92-135.ec2.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 33763)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",itaowei,"> Good to hear!
> 
> IIRC it is not a quick fix to change the model parallel configuration, as the code expects the exact name and number of layers indicated in the model files, but if all you want to do is run inference with the 13B model in a 8 GPU system maybe you could launch 4 processes, each taking 2 GPUs (using something like CUDA_VISIBLE_DEVICES to assign them) and splitting the inputs into 4 chunks of (almost) equal size? The throughput should be similar.

The problem is one input(prompt) can not be split into many chunks. Your mentioned method cannot extend the GPU memory usage in **one generation** while `-huggingface` version of `Llama-2` can do... When you want to use `codellama-13B` to generate, 8 * NVIDIA 3090(24GB GPU) cannot be fully used in one generation but `Llama-2-13B-hf` and `StarChat-15B` can fully use all GPUs to generate each time."
19,Inference on multi-gpu,"Tried to run:
```
torchrun --nproc_per_node 1 codellama/example_instructions.py \
     --ckpt_dir /home/ubuntu/model/ \
     --tokenizer_path /home/ubuntu/model/tokenizer.model \
     --max_seq_len 4512 --max_batch_size 4

```
I have a long prompt (4000 tokens).
I have 4 Nvidia A10G each with 300W and 24GB VRAM. However I see only one GPU being used (on nvidia-smi).
The error I get is:
```
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary:
```
The whole tracelog is:
```
 torchrun --nproc_per_node 1 codellama/example_instructions.py \
>     --ckpt_dir /home/ubuntu/model/ \
>     --tokenizer_path /home/ubuntu/model/tokenizer.model \
>     --max_seq_len 4512 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1

Loaded in 7.07 seconds
Traceback (most recent call last):
  File ""/home/ubuntu/codellama/example_instructions.py"", line 114, in <module>
    fire.Fire(main)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/ubuntu/codellama/example_instructions.py"", line 97, in main
    results = generator.chat_completion(
  File ""/home/ubuntu/codellama/llama/generation.py"", line 335, in chat_completion
    generation_tokens, generation_logprobs = self.generate(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/generation.py"", line 148, in generate
    logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 288, in forward
    h = layer(h, start_pos, freqs_cis, mask)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 240, in forward
    h = x + self.attention.forward(
  File ""/home/ubuntu/codellama/llama/model.py"", line 181, in forward
    scores = F.softmax(scores.float(), dim=-1).type_as(xq)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary: /home/ubuntu/venv/bin/python
Traceback (most recent call last):
  File ""/home/ubuntu/venv/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
codellama/example_instructions.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-25_04:41:10
  host      : ip-172-31-92-135.ec2.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 33763)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",asolano,"I see. I'm afraid I am not familiar with that kind of setup, but there is already a HuggingFace version of Code Llama, so you may try running that instead and see if it fits your use case:

https://huggingface.co/docs/transformers/main/model_doc/code_llama"
19,Inference on multi-gpu,"Tried to run:
```
torchrun --nproc_per_node 1 codellama/example_instructions.py \
     --ckpt_dir /home/ubuntu/model/ \
     --tokenizer_path /home/ubuntu/model/tokenizer.model \
     --max_seq_len 4512 --max_batch_size 4

```
I have a long prompt (4000 tokens).
I have 4 Nvidia A10G each with 300W and 24GB VRAM. However I see only one GPU being used (on nvidia-smi).
The error I get is:
```
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary:
```
The whole tracelog is:
```
 torchrun --nproc_per_node 1 codellama/example_instructions.py \
>     --ckpt_dir /home/ubuntu/model/ \
>     --tokenizer_path /home/ubuntu/model/tokenizer.model \
>     --max_seq_len 4512 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1

Loaded in 7.07 seconds
Traceback (most recent call last):
  File ""/home/ubuntu/codellama/example_instructions.py"", line 114, in <module>
    fire.Fire(main)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/ubuntu/codellama/example_instructions.py"", line 97, in main
    results = generator.chat_completion(
  File ""/home/ubuntu/codellama/llama/generation.py"", line 335, in chat_completion
    generation_tokens, generation_logprobs = self.generate(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/generation.py"", line 148, in generate
    logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 288, in forward
    h = layer(h, start_pos, freqs_cis, mask)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 240, in forward
    h = x + self.attention.forward(
  File ""/home/ubuntu/codellama/llama/model.py"", line 181, in forward
    scores = F.softmax(scores.float(), dim=-1).type_as(xq)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary: /home/ubuntu/venv/bin/python
Traceback (most recent call last):
  File ""/home/ubuntu/venv/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
codellama/example_instructions.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-25_04:41:10
  host      : ip-172-31-92-135.ec2.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 33763)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",manoj21192,"> Good to hear!
> 
> IIRC it is not a quick fix to change the model parallel configuration, as the code expects the exact name and number of layers indicated in the model files, but if all you want to do is run inference with the 13B model in a 8 GPU system maybe you could launch 4 processes, each taking 2 GPUs (using something like CUDA_VISIBLE_DEVICES to assign them) and splitting the inputs into 4 chunks of (almost) equal size? The throughput should be similar.

Can you please guide me how to run 13B and 34B model on Windows? I have single GPU and hence able to run 7B model whose Model parallel value=1. 13B model requires MP value=2 but I have only 1 GPU on which I want to to inference, what changes should I make in code and in which file so that I can run 13B model?"
19,Inference on multi-gpu,"Tried to run:
```
torchrun --nproc_per_node 1 codellama/example_instructions.py \
     --ckpt_dir /home/ubuntu/model/ \
     --tokenizer_path /home/ubuntu/model/tokenizer.model \
     --max_seq_len 4512 --max_batch_size 4

```
I have a long prompt (4000 tokens).
I have 4 Nvidia A10G each with 300W and 24GB VRAM. However I see only one GPU being used (on nvidia-smi).
The error I get is:
```
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary:
```
The whole tracelog is:
```
 torchrun --nproc_per_node 1 codellama/example_instructions.py \
>     --ckpt_dir /home/ubuntu/model/ \
>     --tokenizer_path /home/ubuntu/model/tokenizer.model \
>     --max_seq_len 4512 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1

Loaded in 7.07 seconds
Traceback (most recent call last):
  File ""/home/ubuntu/codellama/example_instructions.py"", line 114, in <module>
    fire.Fire(main)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/ubuntu/codellama/example_instructions.py"", line 97, in main
    results = generator.chat_completion(
  File ""/home/ubuntu/codellama/llama/generation.py"", line 335, in chat_completion
    generation_tokens, generation_logprobs = self.generate(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/generation.py"", line 148, in generate
    logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 288, in forward
    h = layer(h, start_pos, freqs_cis, mask)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 240, in forward
    h = x + self.attention.forward(
  File ""/home/ubuntu/codellama/llama/model.py"", line 181, in forward
    scores = F.softmax(scores.float(), dim=-1).type_as(xq)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary: /home/ubuntu/venv/bin/python
Traceback (most recent call last):
  File ""/home/ubuntu/venv/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
codellama/example_instructions.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-25_04:41:10
  host      : ip-172-31-92-135.ec2.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 33763)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",CODE-FOR,"The model parallel size (MP) is fixed. Which is:
- 7b: 1
- 13b: 2
- 34b: 4

Sadly when you don't change the llama loading code, you have to set num_gpus(n_procs_per_node) equal to MP size.
If you want to load like codellama-13b on 8 gpus, you should change the **loading code** in llama/generation.py. Here is a rough example for loading codellama-7b on 2 gpus using deepspeed framework:

```python
model = Transformer(model_args)
checkpoint = torch.load(ckpt_dir + '/consolidated.00.pth', map_location=""cpu"")
deepspeed_generator, _, _, _ = deepspeed.initialize(model=model, model_parameters=checkpoint, config={
""fp16"": {
    ""enabled"": True
}})
model = Llama(deepspeed_generator, tokenizer)
```

**PS.** If anyone finds an open source framework, please share it with me, Tks in advance. (╥﹏╥) "
19,Inference on multi-gpu,"Tried to run:
```
torchrun --nproc_per_node 1 codellama/example_instructions.py \
     --ckpt_dir /home/ubuntu/model/ \
     --tokenizer_path /home/ubuntu/model/tokenizer.model \
     --max_seq_len 4512 --max_batch_size 4

```
I have a long prompt (4000 tokens).
I have 4 Nvidia A10G each with 300W and 24GB VRAM. However I see only one GPU being used (on nvidia-smi).
The error I get is:
```
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary:
```
The whole tracelog is:
```
 torchrun --nproc_per_node 1 codellama/example_instructions.py \
>     --ckpt_dir /home/ubuntu/model/ \
>     --tokenizer_path /home/ubuntu/model/tokenizer.model \
>     --max_seq_len 4512 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1

Loaded in 7.07 seconds
Traceback (most recent call last):
  File ""/home/ubuntu/codellama/example_instructions.py"", line 114, in <module>
    fire.Fire(main)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/ubuntu/codellama/example_instructions.py"", line 97, in main
    results = generator.chat_completion(
  File ""/home/ubuntu/codellama/llama/generation.py"", line 335, in chat_completion
    generation_tokens, generation_logprobs = self.generate(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/generation.py"", line 148, in generate
    logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 288, in forward
    h = layer(h, start_pos, freqs_cis, mask)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 240, in forward
    h = x + self.attention.forward(
  File ""/home/ubuntu/codellama/llama/model.py"", line 181, in forward
    scores = F.softmax(scores.float(), dim=-1).type_as(xq)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary: /home/ubuntu/venv/bin/python
Traceback (most recent call last):
  File ""/home/ubuntu/venv/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
codellama/example_instructions.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-25_04:41:10
  host      : ip-172-31-92-135.ec2.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 33763)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```",LinkaG,"> The model parallel size (MP) is fixed. Which is:
> 
> * 7b: 1
> * 13b: 2
> * 34b: 4
> 
> Sadly when you don't change the llama loading code, you have to set num_gpus(n_procs_per_node) equal to MP size. If you want to load like codellama-13b on 8 gpus, you should change the **loading code** in llama/generation.py. Here is a rough example for loading codellama-7b on 2 gpus using deepspeed framework:
> 
> ```python
> model = Transformer(model_args)
> checkpoint = torch.load(ckpt_dir + '/consolidated.00.pth', map_location=""cpu"")
> deepspeed_generator, _, _, _ = deepspeed.initialize(model=model, model_parameters=checkpoint, config={
> ""fp16"": {
>     ""enabled"": True
> }})
> model = Llama(deepspeed_generator, tokenizer)
> ```
> 
> **PS.** If anyone finds an open source framework, please share it with me, Tks in advance. (╥﹏╥)

Could you please give an exapmle of full code in llama/generation.py. I try to change the code, but i have the same error with memory allocation. I want to use codellama-7b on 2 gpus. "
18,Support of Mac m1,,facebook-github-bot,"Hi @davideuler! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%2318). Thanks!"
18,Support of Mac m1,,facebook-github-bot,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!
18,Support of Mac m1,,zachschillaci27,Thanks! I just ran this on my M1 Pro without a problem.
18,Support of Mac m1,,zachschillaci27,"> It is not working on M2 ?

I didn't mean for that to imply it won't work on M2, just specifying my hardware. Unfortunately I don't have an M2 to test it on, but naively I would expect it to work."
18,Support of Mac m1,,killian-mannarelli,Working on M2 
18,Support of Mac m1,,edisonslamp,"> Thanks! I just ran this on my M1 Pro without a problem.

Hello! Can't figure out how to run the model on M1. Is there any manuals for this? When I follow Meta's instruction here I face the CUDA problem."
18,Support of Mac m1,,zachschillaci27,"> 



> > Thanks! I just ran this on my M1 Pro without a problem.
> 
> Hello! Can't figure out how to run the model on M1. Is there any manuals for this? When I follow Meta's instruction here I face the CUDA problem.

Until this PR is merged, you'll need to run off of this feature branch. You can follow the steps below:

1. Add the new remote repository:
```
git remote add mps git@github.com:davideuler/codellama.git      
```

2. Fetch the changes in the new repository:
```
git fetch mps
```

3. Checkout this branch from the new remote:
```
git checkout mps/main
```

Now you should be all set to run!
"
18,Support of Mac m1,,lostmygithubaccount,does anyone have an example running on the 13B or larger? I can only load in the 7B and get hanging or various errors trying to load in the larger versions 
18,Support of Mac m1,,Domenico-Esposito,"It doesn't work for me on Mac M1. I have the following error.

I have the same error for all available examples: example_completion.py, example_infilling.py, and example_instructions.py.

```
NOTE: Redirects are currently not supported in Windows or MacOs.
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 0 (pid: 45430) of binary: /Users/dms/.pyenv/versions/3.10.5/bin/python3.10
Traceback (most recent call last):
  File ""/Users/dms/.pyenv/versions/3.10.5/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/Users/dms/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/Users/dms/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/Users/dms/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/Users/dms/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/Users/dms/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
example_instructions.py FAILED
------------------------------------------------------
```"
18,Support of Mac m1,,Rehanchy,"> It doesn't work for me on Mac M1. I have the following error.
> 
> I have the same error for all available examples: example_completion.py, example_infilling.py, and example_instructions.py.
> 
> ```
> NOTE: Redirects are currently not supported in Windows or MacOs.
> > initializing model parallel with size 1
> > initializing ddp with size 1
> > initializing pipeline with size 1
> ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 0 (pid: 45430) of binary: /Users/dms/.pyenv/versions/3.10.5/bin/python3.10
> Traceback (most recent call last):
>   File ""/Users/dms/.pyenv/versions/3.10.5/bin/torchrun"", line 8, in <module>
>     sys.exit(main())
>   File ""/Users/dms/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
>     return f(*args, **kwargs)
>   File ""/Users/dms/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/distributed/run.py"", line 794, in main
>     run(args)
>   File ""/Users/dms/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/distributed/run.py"", line 785, in run
>     elastic_launch(
>   File ""/Users/dms/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
>     return launch_agent(self._config, self._entrypoint, list(args))
>   File ""/Users/dms/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
>     raise ChildFailedError(
> torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
> ======================================================
> example_instructions.py FAILED
> ------------------------------------------------------
> ```

Doesn't work for me on Mac M2, same error as this one, have I got any setups wrong?"
18,Support of Mac m1,,davideuler,"It works on my mac studio with M1 ultra, Python 3.10.10. I checked the dependencies. I'm not sure if the dependencies will cause different result.

```
pip freeze | grep -e torch -e transformer -e sentencepiece -e fairscale
ctransformers @ https://github.com/jllllll/ctransformers-cuBLAS-wheels/releases/download/AVX2/ctransformers-0.2.22+cu117-py3-none-any.whl#sha256=ac34fd73bf1c00bda40510c3bda689db420cfa37a2341e625fe6fd26355248b2
fairscale==0.4.13
pytorch-lightning==1.6.5
sentencepiece==0.1.99
torch==2.0.1
torchaudio==2.0.2
torcheval==0.0.6
torchmetrics==0.11.1
torchtnt==0.2.0
torchvision==0.16.0.dev20230816
transformers @ git+https://github.com/huggingface/transformers@baf1daa58eb2960248fd9f7c3af0ed245b8ce4af
transformers-stream-generator==0.0.4
```"
18,Support of Mac m1,,Domenico-Esposito,"> It works on my mac studio with M1 ultra, Python 3.10.10. I checked the dependencies. I'm not sure if the dependencies will cause different result.
> 
> ```
> pip freeze | grep -e torch -e transformer -e sentencepiece -e fairscale
> ctransformers @ https://github.com/jllllll/ctransformers-cuBLAS-wheels/releases/download/AVX2/ctransformers-0.2.22+cu117-py3-none-any.whl#sha256=ac34fd73bf1c00bda40510c3bda689db420cfa37a2341e625fe6fd26355248b2
> fairscale==0.4.13
> pytorch-lightning==1.6.5
> sentencepiece==0.1.99
> torch==2.0.1
> torchaudio==2.0.2
> torcheval==0.0.6
> torchmetrics==0.11.1
> torchtnt==0.2.0
> torchvision==0.16.0.dev20230816
> transformers @ git+https://github.com/huggingface/transformers@baf1daa58eb2960248fd9f7c3af0ed245b8ce4af
> transformers-stream-generator==0.0.4
> ```

Nothing, even using the same versions, I have the same error.
PS: The version ""torchvision==0.16.0.dev20230816"" doesn't seem to exist. I used version 0.15.2. Do you think the error is related to this?"
17,Still no wget resume option for model download?,Simply adding --continue would save time for interrupted downloads.,amitj-sol,PR already there https://github.com/facebookresearch/codellama/pull/15
16,"Sorry, the download is not available in your region",,MrRace,Download bias!!!!
16,"Sorry, the download is not available in your region",,oukohou,"> Download bias!!!!

dude, you got any link for this bias? thanks"
16,"Sorry, the download is not available in your region",,GaganHonor,"Oh Don`t worry just use AWS ec2 free to use gpu instance of diffrent region , or just reply here i can help you step by step , still you can see this [Demo](https://huggingface.co/blog/codellama#conversational-instructions) 
![SurpriseShockGIF](https://github.com/facebookresearch/codellama/assets/141204441/ac0fc4bc-f435-42ba-9221-6ba126b29445)
"
16,"Sorry, the download is not available in your region",,wusuhuang,"same question. how to do that?
"
16,"Sorry, the download is not available in your region",,GaganHonor,"> same question. how to do that?

Go visit AWS sagemaker"
16,"Sorry, the download is not available in your region",,ColorStar71,Download bias!
16,"Sorry, the download is not available in your region",,Rajjaa,"I can't access LLama 2 from Morocco. I am getting ""Sorry, the download is not available in your region"""
16,"Sorry, the download is not available in your region",,tula-liuxin,It's ruined my fantasy of so called humanity.
16,"Sorry, the download is not available in your region",,sunfeng90,How to solve this?
15,Resume download of partially-downloaded files.,"I observed that the download restarts from the beginning if it gets interrupted.
This PR makes it easier to download the model weights on a poor internet connection.",loleg,It would be good if the md5 mismatch error also then suggested running the script again to continue the downloda.
14,How to trigger the model?,"I got the following - how do I trigger the model GUI? 

(codegenllama) C:\>pip install -e codellama
Obtaining file:///C:/codellama
  Preparing metadata (setup.py) ... done
Requirement already satisfied: torch in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from codellama==0.0.1) (2.0.1+cu117)
Collecting fairscale (from codellama==0.0.1)
  Downloading fairscale-0.4.13.tar.gz (266 kB)
     ---------------------------------------- 266.3/266.3 kB 5.4 MB/s eta 0:00:00
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
Collecting fire (from codellama==0.0.1)
  Downloading fire-0.5.0.tar.gz (88 kB)
     ---------------------------------------- 88.3/88.3 kB 5.2 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Collecting sentencepiece (from codellama==0.0.1)
  Downloading sentencepiece-0.1.99-cp310-cp310-win_amd64.whl (977 kB)
     ---------------------------------------- 977.5/977.5 kB 10.3 MB/s eta 0:00:00
Requirement already satisfied: numpy>=1.22.0 in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from fairscale->codellama==0.0.1) (1.24.1)
Requirement already satisfied: filelock in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from torch->codellama==0.0.1) (3.9.0)
Requirement already satisfied: typing-extensions in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from torch->codellama==0.0.1) (4.4.0)
Requirement already satisfied: sympy in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from torch->codellama==0.0.1) (1.11.1)
Requirement already satisfied: networkx in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from torch->codellama==0.0.1) (3.0)
Requirement already satisfied: jinja2 in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from torch->codellama==0.0.1) (3.1.2)
Collecting six (from fire->codellama==0.0.1)
  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)
Collecting termcolor (from fire->codellama==0.0.1)
  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)
Requirement already satisfied: MarkupSafe>=2.0 in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from jinja2->torch->codellama==0.0.1) (2.1.2)
Requirement already satisfied: mpmath>=0.19 in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from sympy->torch->codellama==0.0.1) (1.2.1)
Building wheels for collected packages: fairscale, fire
  Building wheel for fairscale (pyproject.toml) ... done
  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332117 sha256=0945c01902555985db3ce29135a6b2eef4448b83305b47dbb614485d15188a24
  Stored in directory: c:\users\home\appdata\local\pip\cache\wheels\78\a4\c0\fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3
  Building wheel for fire (setup.py) ... done
  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116947 sha256=d2a6f24d8bb7ed4b42b780a7fcdbae383514bf5ef87b37c07ab76410ca582514
  Stored in directory: c:\users\home\appdata\local\pip\cache\wheels\90\d4\f7\9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95
Successfully built fairscale fire
Installing collected packages: sentencepiece, termcolor, six, fire, fairscale, codellama
  Running setup.py develop for codellama
Successfully installed codellama-0.0.1 fairscale-0.4.13 fire-0.5.0 sentencepiece-0.1.99 six-1.16.0 termcolor-2.3.0",faabian,"There is no GUI. Please refer to the [README](https://github.com/facebookresearch/codellama/blob/main/README.md) for instructions and examples for usage.
You can also play with the model on Hugging Face's playground: https://huggingface.co/spaces/codellama/codellama-playground"
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",lostmygithubaccount,+1 just went down this rabbit hole for a bit -- closest thing I found to helping here: https://github.com/facebookresearch/llama/commit/9a5670bef6d5a57faa3c408935c5fded020b94eb
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",davideuler,I've sent a PR for running CodeLlama on mac:  https://github.com/facebookresearch/codellama/pull/18 
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",sdfgsdfgd,"David, does this work on M2 macbooks ? If so, I'll patch it. 


EDIT:
I just applied that PR patch, since mine is M2 - I went with lostmygithubaccount's reference. 
Also patched it so the WORLD_SIZE count matched the mp count. 

Finally made it work with Code Llama 34B model !!!! 
As soon as it began running, everything froze and my laptop crashed. I heard some weird noises from my dear computer. 
I'm not coming back here again, GPT4 is good for everything 

lol"
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",mauermbq,"> +1 just went down this rabbit hole for a bit -- closest thing I found to helping here: [facebookresearch/llama@9a5670b](https://github.com/facebookresearch/llama/commit/9a5670bef6d5a57faa3c408935c5fded020b94eb)

yep, this brought a step further: ther is still another problem:
RuntimeErrorRuntimeError: : ProcessGroupGloo::allgather: invalid tensor type at index 0 (expected TensorOptions(dtype=c10::Half, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)), got TensorOptions(dtype=c10::Half, device=mps:0, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))ProcessGroupGloo::allgather: invalid tensor type at index 0 (expected TensorOptions(dtype=c10::Half, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)), got TensorOptions(dtype=c10::Half, device=mps:0, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))

ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 55764) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10"
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",davideuler,"> David, does this work on M2 macbooks ? If so, I'll patch it.
> 
> EDIT: I just applied that PR patch, since mine is M2 - I went with lostmygithubaccount's reference. Also patched it so the WORLD_SIZE count matched the mp count.
> 
> Finally made it work with Code Llama 34B model !!!! As soon as it began running, everything froze and my laptop crashed. I heard some weird noises from my dear computer. I'm not coming back here again, GPT4 is good for everything
> 
> lol

I have no M2 on my hand. I tested it on my Mac M1 Ultra, and it works. Not sure if it works on m2. So far as I know it should be compatible.
And I haven't test the PR on cuda, it will be a great job if anyone could help to test the PR on cuda."
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",lostmygithubaccount,"the PR does work on M2, at least the 7b model. I was having trouble w/ the 13b and 34b with the mp count and world_size setting, not sure what I was doing wrong "
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",brianirish,"Can confirm the fix from @davideuler works on my M2 Macbook Air, running the 7b-Instruct model."
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",foolyoghurt,"Verified that the solution provided by @davideuler is effective on my M1 MacBook Pro using the 7b model. However, the performance is notably sluggish. Is it possible to run it using GPU acceleration? It runs so fast with GPU acceleration by llama.cpp"
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",liqiang28,"> > +1 just went down this rabbit hole for a bit -- closest thing I found to helping here: [facebookresearch/llama@9a5670b](https://github.com/facebookresearch/llama/commit/9a5670bef6d5a57faa3c408935c5fded020b94eb)
> 
> yep, this brought a step further: ther is still another problem: RuntimeErrorRuntimeError: : ProcessGroupGloo::allgather: invalid tensor type at index 0 (expected TensorOptions(dtype=c10::Half, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)), got TensorOptions(dtype=c10::Half, device=mps:0, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))ProcessGroupGloo::allgather: invalid tensor type at index 0 (expected TensorOptions(dtype=c10::Half, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)), got TensorOptions(dtype=c10::Half, device=mps:0, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))
> 
> ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 55764) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

I had the same issue. can anybody provide any help?"
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",lostmygithubaccount,"
> I had the same issue. can anybody provide any help?

did you try the PR at https://github.com/facebookresearch/codellama/pull/18? it should work for 7b at least "
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",sdfgsdfgd,"while 34b is useless with reasoning, 7b generates almost relevant code. I could probably write a 10 liner py script that generates snippets with almost same success. Would have been cool to get 34b running though. 7b is extremely useless, why wont 34b run on mac"
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",binoculars,34b freezes on my m1 mac
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",manoj21192,"> 34b freezes on my m1 mac

Can you please guide me how to run 13B and 34B model on Windows? I have single GPU and hence able to run 7B model whose Model parallel value=1. 13B model requires MP value=2 but I have only 1 GPU on which I want to to inference, what changes should I make in code and in which file so that I can run 13B model?"
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",liqiang28,"> > I had the same issue. can anybody provide any help?
> 
> did you try the PR at #18? it should work for 7b at least

I tried the PR at #18 but I used 13b-instruct,  should I change the model to 7b ?"
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",lostmygithubaccount,"@liqiang28 7b should work with that PR, I haven't been able to get any larger models to work "
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",DavidLuong98,"> Verified that the solution provided by @davideuler is effective on my M1 MacBook Pro using the 7b model. However, the performance is notably sluggish. Is it possible to run it using GPU acceleration? It runs so fast with GPU acceleration by llama.cpp

@foolyoghurt Out of curiosity, what's your token per second ? I'm experiencing the sluggish performance as well."
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",liqiang28,"> @liqiang28 7b should work with that PR, I haven't been able to get any larger models to work

Yes, it can work after I changed the model to 7B, thanks a lot"
11,Run code llama on mac?,"Hi,

on mac I got the following error:
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

Guess this is because of the missing CUDA. Is there an option to run it with CPU?",robinsonmhj,"> > +1 just went down this rabbit hole for a bit -- closest thing I found to helping here: [facebookresearch/llama@9a5670b](https://github.com/facebookresearch/llama/commit/9a5670bef6d5a57faa3c408935c5fded020b94eb)
> 
> yep, this brought a step further: ther is still another problem: RuntimeErrorRuntimeError: : ProcessGroupGloo::allgather: invalid tensor type at index 0 (expected TensorOptions(dtype=c10::Half, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)), got TensorOptions(dtype=c10::Half, device=mps:0, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))ProcessGroupGloo::allgather: invalid tensor type at index 0 (expected TensorOptions(dtype=c10::Half, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)), got TensorOptions(dtype=c10::Half, device=mps:0, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))
> 
> ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 55764) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10

I have similar issue above, any fix?"
10,Could not parse check file 'checklist.chk',"Getting this on MacOS when attempting to download

```
HTTP request sent, awaiting response... 200 OK
Length: 6489 (6.3K) [text/html]
Saving to: ‘./CodeLlama-13b-Instruct/checklist.chk’

./CodeLlama-13b-Instruct/checkl 100%[=====================================================>]   6.34K  --.-KB/s    in 0s

2023-08-24 13:51:33 (35.0 MB/s) - ‘./CodeLlama-13b-Instruct/checklist.chk’ saved [6489/6489]

Checking checksums
Could not parse check file 'checklist.chk' (2)
```",CharlieGreenman,"passed in wrong url, i am closing"
9,Any plans to put this on Replicate?,"Would love to be able to use this on [Replicate](https://replicate.com/facebookresearch) – are there any plans for Facebook Research to upload this there?

Thank you!",GaganHonor,"Already available on AWS EC2 and HuggingFace 

[AWS](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/)
[HF🤗](https://huggingface.co/blog/codellama#conversational-instructions)

And , Yes, CodeLlama is available on Replicate. [You can run CodeLlama models with an API on Replicate](https://www.bing.com/search?form=NTPCHB&q=Bing+AI&showconv=1#)[1](https://replicate.com/replicate/codellama-13b). [For example, you can run the CodeLlama-13b model, which is a 13b parameter Llama model tuned for code completion](https://replicate.com/replicate/codellama-13b)[1](https://replicate.com/replicate/codellama-13b). [You can also run the CodeLlama-7b-Instruct model, which is a 7b parameter Llama model tuned for chatting about code](https://replicate.com/replicate/codellama-7b-instruct)

Thanks for Reading my answer @carolinedlu mam , I am honored. 🌟"
9,Any plans to put this on Replicate?,"Would love to be able to use this on [Replicate](https://replicate.com/facebookresearch) – are there any plans for Facebook Research to upload this there?

Thank you!",baptisteroziere,"Thanks @GaganHonor for answering. I am closing this issue now, please feel free to reopen if needed."
9,Any plans to put this on Replicate?,"Would love to be able to use this on [Replicate](https://replicate.com/facebookresearch) – are there any plans for Facebook Research to upload this there?

Thank you!",GaganHonor,Thanks for your kind words @brozi .
8,Unable to locally verify the issuer's authority,"```
Enter the list of models to download without spaces (7b,13b,34b,7b-Python,13b-Python,34b-Python,7b-Instruct,13b-Instruct,34b-Instruct), or press Enter for all: 7b
Downloading LICENSE and Acceptable Usage Policy
--2023-08-24 17:02:44--  {codellama url}
Download-Request-ID=254298234165953
Resolving download2.llamameta.net (download2.llamameta.net)... 54.230.31.124, 54.230.31.3, 54.230.31.14, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|54.230.31.124|:443... connected.
ERROR: cannot verify download2.llamameta.net's certificate, issued by ‘emailAddress=support@fortinet.com,CN=FGT3KDT418800895,OU=Certificate Authority,O=Fortinet,L=Sunnyvale,ST=California,C=US’:
  Unable to locally verify the issuer's authority.
To connect to download2.llamameta.net insecurely, use `--no-check-certificate'.

```
I've requested the link 3 times now and the download.sh script still throws this error when trying to download the model. 
Thought it was a configuration issue on my side, but I tried again with normal llama-2 and it worked.

Confusingly, the download.sh script for llama-2 only works for **some** models. 70B-chat worked, 7B and 13B did not.

I also tried editing the download.sh script to include the --no-check-certificate option on all wget commands, but that then gives me the following error:
```
  Unable to locally verify the issuer's authority.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-24 16:45:15 ERROR 403: Forbidden.
```",az-pz,The download links expire after 24 hours. You may need to request the link again.
8,Unable to locally verify the issuer's authority,"```
Enter the list of models to download without spaces (7b,13b,34b,7b-Python,13b-Python,34b-Python,7b-Instruct,13b-Instruct,34b-Instruct), or press Enter for all: 7b
Downloading LICENSE and Acceptable Usage Policy
--2023-08-24 17:02:44--  {codellama url}
Download-Request-ID=254298234165953
Resolving download2.llamameta.net (download2.llamameta.net)... 54.230.31.124, 54.230.31.3, 54.230.31.14, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|54.230.31.124|:443... connected.
ERROR: cannot verify download2.llamameta.net's certificate, issued by ‘emailAddress=support@fortinet.com,CN=FGT3KDT418800895,OU=Certificate Authority,O=Fortinet,L=Sunnyvale,ST=California,C=US’:
  Unable to locally verify the issuer's authority.
To connect to download2.llamameta.net insecurely, use `--no-check-certificate'.

```
I've requested the link 3 times now and the download.sh script still throws this error when trying to download the model. 
Thought it was a configuration issue on my side, but I tried again with normal llama-2 and it worked.

Confusingly, the download.sh script for llama-2 only works for **some** models. 70B-chat worked, 7B and 13B did not.

I also tried editing the download.sh script to include the --no-check-certificate option on all wget commands, but that then gives me the following error:
```
  Unable to locally verify the issuer's authority.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-24 16:45:15 ERROR 403: Forbidden.
```",adiyoss,"Hi @spikedoanz, 
@az-pz is correct the download links expire after 24 hours. Please request again the link."
7,Where is the 70B Model?,Just a question: Are you planning to release the 70B model in the future or what is the plan?,syhw,No plans to release a 70B Code Llama model as of now.
7,Where is the 70B Model?,Just a question: Are you planning to release the 70B model in the future or what is the plan?,sirus20x6,"I think we'd appreciate it if it exists. Hardware is getting better. Inference is getting optimized. people are buying new gpus and ram just for LLMs. Hell I bought an Epyc system with 512gb of ram for this stuff. So far I'm still shackled to chatgpt4, and as much as I like it, there are some things I would like to keep in house. Also if I start building apps with the api it would get really expensive really quick. I feel like llama2 and codellama are close to greatness, but there is only so far finetuning can bring us."
7,Where is the 70B Model?,Just a question: Are you planning to release the 70B model in the future or what is the plan?,ignoramous,Announced today: https://analyticsindiamag.com/meta-releases-code-llama-70b/
7,Where is the 70B Model?,Just a question: Are you planning to release the 70B model in the future or what is the plan?,mayurtikundi12,Announced today: https://bizmorphic.com/blogs/meta-ais-massive-70b-parameter-codex-crushes-gpt-and-googles-gemini-models
6,Size of the model info is missing,It would be helpful to mention the size of disk space the pre-trained models will take so as to determine if this can be easily done on laptop or not.,Hangsiin,I downloaded 7B-Python version and it was 12.55GB.
6,Size of the model info is missing,It would be helpful to mention the size of disk space the pre-trained models will take so as to determine if this can be easily done on laptop or not.,Rupesh-rkgit,13B is 24GB approx
6,Size of the model info is missing,It would be helpful to mention the size of disk space the pre-trained models will take so as to determine if this can be easily done on laptop or not.,jspisak,I can put up a PR and add a table to the readme. I agree this is a handy thing to have..
6,Size of the model info is missing,It would be helpful to mention the size of disk space the pre-trained models will take so as to determine if this can be easily done on laptop or not.,jspisak,"https://github.com/facebookresearch/codellama/pull/103

Closing this issue as I'll merge it to the readme this morning. Thanks!"
5,access through HuggingFace?,"Thanks so much for your work and for opening up access to Code Llama.

We received access for Llama2 previously and access the models via HF.  Is it possible to do the same with Code Llama?
(We just requested access for Code Llama via your submission form).",dennis-gonzales,+1
5,access through HuggingFace?,"Thanks so much for your work and for opening up access to Code Llama.

We received access for Llama2 previously and access the models via HF.  Is it possible to do the same with Code Llama?
(We just requested access for Code Llama via your submission form).",timxx,Maybe https://huggingface.co/codellama is the official HF?
5,access through HuggingFace?,"Thanks so much for your work and for opening up access to Code Llama.

We received access for Llama2 previously and access the models via HF.  Is it possible to do the same with Code Llama?
(We just requested access for Code Llama via your submission form).",GaganHonor,https://huggingface.co/blog/codellama#conversational-instructions lol 💀
5,access through HuggingFace?,"Thanks so much for your work and for opening up access to Code Llama.

We received access for Llama2 previously and access the models via HF.  Is it possible to do the same with Code Llama?
(We just requested access for Code Llama via your submission form).",ryancurrah,What's benefit of using huggingface vs just downloading it?
5,access through HuggingFace?,"Thanks so much for your work and for opening up access to Code Llama.

We received access for Llama2 previously and access the models via HF.  Is it possible to do the same with Code Llama?
(We just requested access for Code Llama via your submission form).",GaganHonor,"> What's benefit of using huggingface vs just downloading it?

Same as using Bing Chat vs Chat GPT4 by open Ai"
5,access through HuggingFace?,"Thanks so much for your work and for opening up access to Code Llama.

We received access for Llama2 previously and access the models via HF.  Is it possible to do the same with Code Llama?
(We just requested access for Code Llama via your submission form).",varunk29,"Hugging face version seems to run very slow, is it only me ?"
5,access through HuggingFace?,"Thanks so much for your work and for opening up access to Code Llama.

We received access for Llama2 previously and access the models via HF.  Is it possible to do the same with Code Llama?
(We just requested access for Code Llama via your submission form).",timxx,"> Hugging face version seems to run very slow, is it only me ?

I have the same problem with 7b instruct hf model."
5,access through HuggingFace?,"Thanks so much for your work and for opening up access to Code Llama.

We received access for Llama2 previously and access the models via HF.  Is it possible to do the same with Code Llama?
(We just requested access for Code Llama via your submission form).",syhw,"Here are some links:
[Models on the Hub](https://huggingface.co/codellama)
[Demo (code completion, streaming server)](https://huggingface.co/spaces/codellama/codellama-playground)
[Demo (instruction fine-tuned, self-contained & clonable)](https://huggingface.co/spaces/codellama/codellama-13b-chat)"
4,Scheme missing,"Followed the instructions as per the blog and got my email download code.

Ater running download.sh, download link and choosing the models, I get a Scheme missing error and the script gets stuck on Checking checksums",gabrielbacha,It seems when copying the email link there are 2 hidden characters before and after the link
3,Readme updates to reflect file names,"I noticed that `example_code_completion.py` doesn't exist in this repo, but `example_completion.py` does, same with `example_code_infilling.py`",facebook-github-bot,"Hi @DavidZirinsky! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Fcodellama%20%233). Thanks!"
3,Readme updates to reflect file names,"I noticed that `example_code_completion.py` doesn't exist in this repo, but `example_completion.py` does, same with `example_code_infilling.py`",facebook-github-bot,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!
2,Does it support FIM (fill-in-the-middle)?,"This is useful for code completion tasks. Starcoder, for instance, is trained with FIM — how does Code LLama compare?",StevenJohnston,"https://about.fb.com/news/2023/08/code-llama-ai-for-coding/

> The 7B and 13B base and instruct models have also been trained with fill-in-the-middle (FIM) capability"
2,Does it support FIM (fill-in-the-middle)?,"This is useful for code completion tasks. Starcoder, for instance, is trained with FIM — how does Code LLama compare?",zachschillaci27,See the README for more info: https://github.com/facebookresearch/codellama#code-infilling
